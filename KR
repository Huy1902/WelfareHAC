{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b485500",
   "metadata": {
    "id": "36GFFPEquVlH",
    "papermill": {
     "duration": 0.007774,
     "end_time": "2025-07-17T10:07:30.017056",
     "exception": false,
     "start_time": "2025-07-17T10:07:30.009282",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "994edd43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T10:07:30.031308Z",
     "iopub.status.busy": "2025-07-17T10:07:30.031037Z",
     "iopub.status.idle": "2025-07-17T10:07:33.049178Z",
     "shell.execute_reply": "2025-07-17T10:07:33.048327Z"
    },
    "papermill": {
     "duration": 3.027528,
     "end_time": "2025-07-17T10:07:33.051250",
     "exception": false,
     "start_time": "2025-07-17T10:07:30.023722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\r\n"
     ]
    }
   ],
   "source": [
    "!wandb login --relogin c2aabf528c3a17ca15b2306fdef1f0f0d24798bf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b10b572",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T10:07:33.066878Z",
     "iopub.status.busy": "2025-07-17T10:07:33.066199Z",
     "iopub.status.idle": "2025-07-17T10:07:40.370218Z",
     "shell.execute_reply": "2025-07-17T10:07:40.369364Z"
    },
    "id": "lJ3KHm7bdEpB",
    "outputId": "e85fb76f-ac93-4310-ed56-b532d2b2cbc9",
    "papermill": {
     "duration": 7.312433,
     "end_time": "2025-07-17T10:07:40.371439",
     "exception": false,
     "start_time": "2025-07-17T10:07:33.059006",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/kr-hac/user_info.npy\n",
      "/kaggle/input/kr-hac/item_info.npy\n",
      "/kaggle/input/kr-hac/all.csv\n",
      "/kaggle/input/kr-hac/train.csv\n",
      "/kaggle/input/kr-hac/test.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.utils.data import DataLoader\n",
    "from time import time\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77aaec09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T10:07:40.385793Z",
     "iopub.status.busy": "2025-07-17T10:07:40.385410Z",
     "iopub.status.idle": "2025-07-17T10:07:40.486445Z",
     "shell.execute_reply": "2025-07-17T10:07:40.485622Z"
    },
    "id": "batCUlrPrPpR",
    "papermill": {
     "duration": 0.109476,
     "end_time": "2025-07-17T10:07:40.487788",
     "exception": false,
     "start_time": "2025-07-17T10:07:40.378312",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title Hyperparameter\n",
    "\n",
    "path_to_data = \"/kaggle/input/kr-hac\"\n",
    "path_to_output = \"/kaggle/working/kr-hac/\"\n",
    "\n",
    "\n",
    "cuda = 0\n",
    "if cuda >= 0 and torch.cuda.is_available():\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(cuda)\n",
    "    torch.cuda.set_device(cuda)\n",
    "    device = f\"cuda:{cuda}\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "576ff407",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T10:07:40.501642Z",
     "iopub.status.busy": "2025-07-17T10:07:40.501425Z",
     "iopub.status.idle": "2025-07-17T10:07:40.759446Z",
     "shell.execute_reply": "2025-07-17T10:07:40.758533Z"
    },
    "papermill": {
     "duration": 0.266472,
     "end_time": "2025-07-17T10:07:40.761045",
     "exception": false,
     "start_time": "2025-07-17T10:07:40.494573",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p /kaggle/working/kr-hac/agent\n",
    "!mkdir -p /kaggle/working/kr-hac/env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47f608ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T10:07:40.775355Z",
     "iopub.status.busy": "2025-07-17T10:07:40.775112Z",
     "iopub.status.idle": "2025-07-17T10:07:42.261362Z",
     "shell.execute_reply": "2025-07-17T10:07:42.260732Z"
    },
    "papermill": {
     "duration": 1.494861,
     "end_time": "2025-07-17T10:07:42.262567",
     "exception": false,
     "start_time": "2025-07-17T10:07:40.767706",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>slate_of_items</th>\n",
       "      <th>user_clicks</th>\n",
       "      <th>user_click_history</th>\n",
       "      <th>sequence_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[1, 2, 3, 4, 4, 5, 6, 7, 8, 9]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 1]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[10, 11, 12, 13, 14, 15, 16, 17, 18, 19]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[7, 9]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[20, 21, 22, 23, 24, 25, 26, 27, 28, 29]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>[7, 9]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[30, 31, 32, 33, 34, 35, 32, 36, 37, 38]</td>\n",
       "      <td>[0, 0, 0, 0, 1, 1, 0, 0, 0, 0]</td>\n",
       "      <td>[7, 9, 27]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[39, 40, 41, 42, 43, 44, 45, 46, 47, 48]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 1, 0, 1]</td>\n",
       "      <td>[7, 9, 27, 34, 35]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                            slate_of_items  \\\n",
       "0        0            [1, 2, 3, 4, 4, 5, 6, 7, 8, 9]   \n",
       "1        0  [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]   \n",
       "2        0  [20, 21, 22, 23, 24, 25, 26, 27, 28, 29]   \n",
       "3        0  [30, 31, 32, 33, 34, 35, 32, 36, 37, 38]   \n",
       "4        0  [39, 40, 41, 42, 43, 44, 45, 46, 47, 48]   \n",
       "\n",
       "                      user_clicks  user_click_history  sequence_id  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 1, 0, 1]                  []            0  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]              [7, 9]            1  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]              [7, 9]            2  \n",
       "3  [0, 0, 0, 0, 1, 1, 0, 0, 0, 0]          [7, 9, 27]            3  \n",
       "4  [0, 0, 0, 1, 0, 0, 0, 1, 0, 1]  [7, 9, 27, 34, 35]            4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>slate_of_items</th>\n",
       "      <th>user_clicks</th>\n",
       "      <th>user_click_history</th>\n",
       "      <th>sequence_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[514, 514, 515, 516, 517, 518, 519, 520, 521, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[7, 9, 27, 34, 35, 42, 46, 48, 51, 52, 58, 63,...</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[523, 524, 525, 525, 526, 527, 526, 528, 529, ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[7, 9, 27, 34, 35, 42, 46, 48, 51, 52, 58, 63,...</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[531, 532, 533, 534, 535, 536, 537, 538, 539, ...</td>\n",
       "      <td>[0, 0, 0, 1, 1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[7, 9, 27, 34, 35, 42, 46, 48, 51, 52, 58, 63,...</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[540, 541, 542, 543, 544, 17, 545, 546, 547, 548]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>[7, 9, 27, 34, 35, 42, 46, 48, 51, 52, 58, 63,...</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[549, 550, 551, 552, 553, 554, 554, 555, 556, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]</td>\n",
       "      <td>[7, 9, 27, 34, 35, 42, 46, 48, 51, 52, 58, 63,...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                     slate_of_items  \\\n",
       "0        0  [514, 514, 515, 516, 517, 518, 519, 520, 521, ...   \n",
       "1        0  [523, 524, 525, 525, 526, 527, 526, 528, 529, ...   \n",
       "2        0  [531, 532, 533, 534, 535, 536, 537, 538, 539, ...   \n",
       "3        0  [540, 541, 542, 543, 544, 17, 545, 546, 547, 548]   \n",
       "4        0  [549, 550, 551, 552, 553, 554, 554, 555, 556, ...   \n",
       "\n",
       "                      user_clicks  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "1  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "2  [0, 0, 0, 1, 1, 0, 0, 0, 0, 0]   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]   \n",
       "\n",
       "                                  user_click_history  sequence_id  \n",
       "0  [7, 9, 27, 34, 35, 42, 46, 48, 51, 52, 58, 63,...           57  \n",
       "1  [7, 9, 27, 34, 35, 42, 46, 48, 51, 52, 58, 63,...           58  \n",
       "2  [7, 9, 27, 34, 35, 42, 46, 48, 51, 52, 58, 63,...           59  \n",
       "3  [7, 9, 27, 34, 35, 42, 46, 48, 51, 52, 58, 63,...           60  \n",
       "4  [7, 9, 27, 34, 35, 42, 46, 48, 51, 52, 58, 63,...           61  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(11643, 32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(986, 32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title Load data for ml1m\n",
    "item_info = np.load(os.path.join(path_to_data, \"item_info.npy\"))\n",
    "user_info = np.load(os.path.join(path_to_data, \"user_info.npy\"))\n",
    "train = pd.read_csv(os.path.join(path_to_data, \"train.csv\"), sep=\"@\")\n",
    "test = pd.read_csv(os.path.join(path_to_data, \"test.csv\"), sep=\"@\")\n",
    "display(train.head())\n",
    "display(test.head())\n",
    "display(item_info.shape)\n",
    "display(user_info.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c194bc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T10:07:42.278682Z",
     "iopub.status.busy": "2025-07-17T10:07:42.278451Z",
     "iopub.status.idle": "2025-07-17T10:07:42.308422Z",
     "shell.execute_reply": "2025-07-17T10:07:42.307819Z"
    },
    "papermill": {
     "duration": 0.038499,
     "end_time": "2025-07-17T10:07:42.309532",
     "exception": false,
     "start_time": "2025-07-17T10:07:42.271033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>sequence_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>72757.000000</td>\n",
       "      <td>72757.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>489.220144</td>\n",
       "      <td>56.828745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>288.486319</td>\n",
       "      <td>50.477912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>233.000000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>489.000000</td>\n",
       "      <td>43.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>744.000000</td>\n",
       "      <td>80.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>985.000000</td>\n",
       "      <td>335.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            user_id   sequence_id\n",
       "count  72757.000000  72757.000000\n",
       "mean     489.220144     56.828745\n",
       "std      288.486319     50.477912\n",
       "min        0.000000      0.000000\n",
       "25%      233.000000     19.000000\n",
       "50%      489.000000     43.000000\n",
       "75%      744.000000     80.000000\n",
       "max      985.000000    335.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>sequence_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>23775.000000</td>\n",
       "      <td>23775.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>489.122944</td>\n",
       "      <td>134.036383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>288.520211</td>\n",
       "      <td>77.999722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>233.000000</td>\n",
       "      <td>77.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>489.000000</td>\n",
       "      <td>118.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>744.000000</td>\n",
       "      <td>176.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>985.000000</td>\n",
       "      <td>446.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            user_id   sequence_id\n",
       "count  23775.000000  23775.000000\n",
       "mean     489.122944    134.036383\n",
       "std      288.520211     77.999722\n",
       "min        0.000000      4.000000\n",
       "25%      233.000000     77.000000\n",
       "50%      489.000000    118.000000\n",
       "75%      744.000000    176.000000\n",
       "max      985.000000    446.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train.describe())\n",
    "display(test.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5324db00",
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2025-07-17T10:07:42.326184Z",
     "iopub.status.busy": "2025-07-17T10:07:42.325866Z",
     "iopub.status.idle": "2025-07-17T10:07:42.342146Z",
     "shell.execute_reply": "2025-07-17T10:07:42.341277Z"
    },
    "id": "6uAs4kp4whrk",
    "papermill": {
     "duration": 0.026811,
     "end_time": "2025-07-17T10:07:42.343455",
     "exception": false,
     "start_time": "2025-07-17T10:07:42.316644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title Support function\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def padding_and_clip(sequence, max_len, padding_direction = 'left'):\n",
    "    if len(sequence) < max_len:\n",
    "        sequence = [0] * (max_len - len(sequence)) + sequence if padding_direction == 'left' else sequence + [0] * (max_len - len(sequence))\n",
    "    sequence = sequence[-max_len:] if padding_direction == 'left' else sequence[:max_len]\n",
    "    # print(f\"sequence{sequence}\")\n",
    "    return sequence\n",
    "\n",
    "def get_regularization(*modules):\n",
    "  \"\"\"\n",
    "  Customized L2 regularization\n",
    "  \"\"\"\n",
    "  reg = 0\n",
    "  for m in modules:\n",
    "    for p in m.parameters():\n",
    "      reg = torch.mean(p * p) + reg\n",
    "  return reg\n",
    "\n",
    "def wrap_batch(batch, device):\n",
    "  \"\"\"\n",
    "  Build feed_dict from batch data and move data to device\n",
    "  \"\"\"\n",
    "  for k,val in batch.items():\n",
    "    if type(val).__module__ == np.__name__:\n",
    "        batch[k] = torch.from_numpy(val)\n",
    "    elif torch.is_tensor(val):\n",
    "        batch[k] = val\n",
    "    elif type(val) is list:\n",
    "        batch[k] = torch.tensor(val)\n",
    "    else:\n",
    "        continue\n",
    "    if batch[k].type() == \"torch.DoubleTensor\":\n",
    "        batch[k] = batch[k].float()\n",
    "    batch[k] = batch[k].to(device)\n",
    "  return batch\n",
    "\n",
    "def sample_categorical_action(action_prob, candidate_ids, slate_size,\n",
    "                              with_replacement=True, batch_wise=False,\n",
    "                              return_idx=False):\n",
    "  '''\n",
    "  @input:\n",
    "  - action_prob: (B, L)\n",
    "  - candidate_ids: (B, L) or (1, L)\n",
    "  - slate_size: K\n",
    "  - with_replacement: sample with replacement\n",
    "  - batch_wise: do batch wise candidate selection\n",
    "  '''\n",
    "  if with_replacement:\n",
    "    # (K, B)\n",
    "    indices = Categorical(action_prob).sample(sample_shape = (slate_size,))\n",
    "    # (B, K)\n",
    "    indices = torch.transpose(indices, 0, 1)\n",
    "  else:\n",
    "    indices = torch.cat([torch.multinomial(prob, slate_size, replacement=False).view(1, -1) \\\n",
    "                         for prob in action_prob], dim = 0)\n",
    "  action = torch.gather(candidate_ids, 1, indices) if batch_wise else candidate_ids[indices]\n",
    "  if return_idx:\n",
    "    return action.detach(), indices.detach()\n",
    "  else:\n",
    "    return action.detach()\n",
    "\n",
    "\n",
    "##################\n",
    "#   Learning     #\n",
    "##################\n",
    "\n",
    "class LinearScheduler(object):\n",
    "  def __init__(self, schedule_timesteps, final_p, initial_p=1.0):\n",
    "    self.schedule_timesteps = schedule_timesteps\n",
    "    self.final_p = final_p\n",
    "    self.initial_p = initial_p\n",
    "\n",
    "  def value(self, t):\n",
    "    '''\n",
    "    see Schedule.value\n",
    "    '''\n",
    "    fraction = min(float(t) / self.schedule_timesteps, 1.0)\n",
    "    return self.initial_p + fraction * (self.final_p - self.initial_p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "545efa3b",
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2025-07-17T10:07:42.367776Z",
     "iopub.status.busy": "2025-07-17T10:07:42.367470Z",
     "iopub.status.idle": "2025-07-17T10:07:42.381234Z",
     "shell.execute_reply": "2025-07-17T10:07:42.380492Z"
    },
    "id": "I_-339N7XCzA",
    "papermill": {
     "duration": 0.025091,
     "end_time": "2025-07-17T10:07:42.382293",
     "exception": false,
     "start_time": "2025-07-17T10:07:42.357202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title Plot Function\n",
    "\n",
    "def smooth(values, window = 3):\n",
    "  left = window // 2\n",
    "  new_values = [np.mean(values[max(0,idx-left):min(idx-left+window,len(values))]) for idx in range(len(values))]\n",
    "  return new_values\n",
    "\n",
    "\n",
    "def get_rl_training_info(log_path, training_losses = ['actor_loss', 'critic_loss']):\n",
    "  episode = []\n",
    "  average_total_reward, reward_variance, max_total_reward, min_total_reward, average_n_step, max_n_step, min_n_step \\\n",
    "          = [], [], [], [], [], [], []\n",
    "  training_loss_records = {k: [] for k in training_losses}\n",
    "  with open(log_path, 'r') as infile:\n",
    "    for line in tqdm(infile):\n",
    "      split = line.split('@')\n",
    "      # episode\n",
    "      episode.append(eval(split[0].split(':')[1]))\n",
    "      # episode report\n",
    "      episode_report = eval(split[1].strip()[len(\"episode report:\"):])\n",
    "      average_total_reward.append(episode_report['average_total_reward'])\n",
    "      reward_variance.append(episode_report['reward_variance'])\n",
    "      max_total_reward.append(episode_report['max_total_reward'])\n",
    "      min_total_reward.append(episode_report['min_total_reward'])\n",
    "      average_n_step.append(episode_report['average_n_step'])\n",
    "      max_n_step.append(episode_report['max_n_step'])\n",
    "      min_n_step.append(episode_report['min_n_step'])\n",
    "      # loss report\n",
    "      if training_losses:\n",
    "          loss_report = eval(split[2].strip()[len(\"step loss:\"):])\n",
    "          for k in training_losses:\n",
    "              training_loss_records[k].append(loss_report[k])\n",
    "  info = {\n",
    "      \"episode\": episode,\n",
    "      \"average_total_reward\": average_total_reward,\n",
    "      \"reward_variance\": reward_variance,\n",
    "      \"max_total_reward\": max_total_reward,\n",
    "      \"min_total_reward\": min_total_reward,\n",
    "      \"average_depth_per_episode\": average_n_step,\n",
    "      \"max_depth_per_episode\": max_n_step,\n",
    "      \"min_depth_per_episode\": min_n_step\n",
    "  }\n",
    "  if training_losses:\n",
    "      for k in training_losses:\n",
    "        info[k] = training_loss_records[k]\n",
    "  return info\n",
    "\n",
    "def plot_multiple_line(legend_names, list_of_stats, x_name, ncol = 2, row_height = 4, save_path=\"/kaggle/working/fig/rl.png\"):\n",
    "  '''\n",
    "  @input:\n",
    "  - legend_names: [legend]\n",
    "  - list_of_stats: [{field_name: [values]}]\n",
    "  - x_name: x-axis field_name\n",
    "  - ncol: number of subplots in each row\n",
    "  '''\n",
    "  plt.rcParams.update({'font.size': 14})\n",
    "  assert ncol > 0\n",
    "  features = list(list_of_stats[0].keys())\n",
    "  features.remove(x_name)\n",
    "  N = len(features)\n",
    "  fig_height = 12 // ncol if len(features) == 1 else row_height*((N-1)//ncol+1)\n",
    "  plt.figure(figsize = (16, fig_height))\n",
    "  for i,field in enumerate(features):\n",
    "      plt.subplot((N-1)//ncol+1,ncol,i+1)\n",
    "      minY,maxY = float('inf'),float('-inf')\n",
    "      for j,L in enumerate(legend_names):\n",
    "          X = list_of_stats[j][x_name]\n",
    "          value_list = list_of_stats[j][field]\n",
    "          minY,maxY = min(minY,min(value_list)),max(maxY,max(value_list))\n",
    "          plt.plot(X[:len(value_list)], value_list, label = L)\n",
    "      plt.ylabel(field)\n",
    "      plt.xlabel(x_name)\n",
    "      scale = 1e-4 + maxY - minY\n",
    "      plt.ylim(minY - scale * 0.05, maxY + scale * 0.05)\n",
    "      plt.legend()\n",
    "  plt.savefig(save_path)\n",
    "  plt.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75bc49bb",
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2025-07-17T10:07:42.402841Z",
     "iopub.status.busy": "2025-07-17T10:07:42.402478Z",
     "iopub.status.idle": "2025-07-17T10:07:42.406802Z",
     "shell.execute_reply": "2025-07-17T10:07:42.405829Z"
    },
    "id": "HVUXZjicg3kg",
    "papermill": {
     "duration": 0.018623,
     "end_time": "2025-07-17T10:07:42.407912",
     "exception": false,
     "start_time": "2025-07-17T10:07:42.389289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title Socrer function\n",
    "def dot_scorer(action_emb, item_emb, item_dim):\n",
    "  '''\n",
    "  score = item_emb * weight\n",
    "\n",
    "  @input:\n",
    "  - action_emb: (B, i_dim)\n",
    "  - item_emb: (B, L, i_dim) or (1, L, i_dim)\n",
    "  @output:\n",
    "  - score: (B, L)\n",
    "  '''\n",
    "  output = torch.sum(action_emb.view(-1, 1, item_dim) * item_emb, dim=-1)\n",
    "\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48e815df",
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2025-07-17T10:07:42.422689Z",
     "iopub.status.busy": "2025-07-17T10:07:42.422290Z",
     "iopub.status.idle": "2025-07-17T10:07:42.427535Z",
     "shell.execute_reply": "2025-07-17T10:07:42.426852Z"
    },
    "id": "t-f8lshQUlDz",
    "papermill": {
     "duration": 0.013807,
     "end_time": "2025-07-17T10:07:42.428680",
     "exception": false,
     "start_time": "2025-07-17T10:07:42.414873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title Dense Neural Network\n",
    "\n",
    "class DNN(nn.Module):\n",
    "  def __init__(self, in_dim, hidden_dims, out_dim=1, dropout_rate= 0.,\n",
    "               do_batch_norm=True):\n",
    "    super(DNN, self).__init__()\n",
    "    self.in_dim = in_dim\n",
    "    layers = []\n",
    "\n",
    "    for hidden_dim in hidden_dims:\n",
    "      linear_layer = nn.Linear(in_dim, hidden_dim)\n",
    "\n",
    "      layers.append(linear_layer)\n",
    "      in_dim = hidden_dim\n",
    "      layers.append(nn.ReLU())\n",
    "      if dropout_rate > 0:\n",
    "        layers.append(nn.Dropout(dropout_rate))\n",
    "      if do_batch_norm:\n",
    "        layers.append(nn.LayerNorm(hidden_dim))\n",
    "\n",
    "    # Prediction layer\n",
    "    last_layer = nn.Linear(in_dim, out_dim)\n",
    "    layers.append(last_layer)\n",
    "\n",
    "    self.layers = nn.Sequential(*layers)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x.view(-1, self.in_dim)\n",
    "    logit = self.layers(x)\n",
    "    return logit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c1fd3d7",
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2025-07-17T10:07:42.443272Z",
     "iopub.status.busy": "2025-07-17T10:07:42.443100Z",
     "iopub.status.idle": "2025-07-17T10:07:42.447475Z",
     "shell.execute_reply": "2025-07-17T10:07:42.446985Z"
    },
    "id": "EcNXI4e7tyGR",
    "papermill": {
     "duration": 0.012702,
     "end_time": "2025-07-17T10:07:42.448461",
     "exception": false,
     "start_time": "2025-07-17T10:07:42.435759",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title # Data Reader class\n",
    "\n",
    "class BaseDataReader(Dataset):\n",
    "  def __init__(self, params):\n",
    "    self.phase = 'train'\n",
    "    self.n_worker = params['n_worker']\n",
    "    self._read_data(params)\n",
    "\n",
    "  def _read_data(self, params):\n",
    "    self.data = dict()\n",
    "    self.data['train'] = params['train']\n",
    "    self.data['val'] = params['val']\n",
    "\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    pass\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data[self.phase])\n",
    "\n",
    "  def get_statistics(self):\n",
    "    return {'length': len(self)}\n",
    "\n",
    "  def set_phase(self, phase):\n",
    "    assert phase in ['train', 'val', 'test']\n",
    "    self.phase = phase\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09e37db5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T10:07:42.462881Z",
     "iopub.status.busy": "2025-07-17T10:07:42.462676Z",
     "iopub.status.idle": "2025-07-17T10:07:42.471285Z",
     "shell.execute_reply": "2025-07-17T10:07:42.470579Z"
    },
    "papermill": {
     "duration": 0.017156,
     "end_time": "2025-07-17T10:07:42.472390",
     "exception": false,
     "start_time": "2025-07-17T10:07:42.455234",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class KRDataReader(BaseDataReader):\n",
    "        \n",
    "    def log(self):\n",
    "        super().log()\n",
    "        \n",
    "    def __init__(self, params):\n",
    "        '''\n",
    "        - from BaseReader:\n",
    "            - phase\n",
    "            - data: will add Position column\n",
    "        '''\n",
    "        print(\"init kr reader\")\n",
    "        self.max_seq_len = params['max_seq_len']\n",
    "        super().__init__(params)\n",
    "        \n",
    "    def _read_data(self, params):\n",
    "        # read data_file\n",
    "        super()._read_data(params)\n",
    "        print(\"Load item meta data\")\n",
    "        self.item_meta = params['item_meta']\n",
    "        self.user_meta = params['user_meta']\n",
    "        self.item_vec_size = len(self.item_meta[0])\n",
    "        self.user_vec_size = len(self.user_meta[0])\n",
    "        self.portrait_len = len(self.user_meta[0])\n",
    "    \n",
    "    ###########################\n",
    "    #        Iterator         #\n",
    "    ###########################\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        train batch after collate:\n",
    "        {\n",
    "        'resp': 2, \n",
    "        'user_UserID': (B,) \n",
    "        'user_XXX': (B,feature_size)\n",
    "        'item_ItemID': (B,)\n",
    "        'item_XXX': (B,feature_size)\n",
    "        'negi_ItemID': (B,n_neg) \n",
    "        'negi_XXX': (B,n_neg,feature_size) \n",
    "        }\n",
    "        '''\n",
    "        user_ID, slate_of_items, user_clicks, user_click_history, sequence_id = self.data[self.phase].iloc[idx]\n",
    "        user_profile = self.user_meta[user_ID]\n",
    "\n",
    "\n",
    "        exposure = eval(slate_of_items)\n",
    "        # exposure = list(map(lambda x: self.item_vocab[x], exposure))\n",
    "        # exposure = padding_and_clip(exposure, 10) # test\n",
    "        history = eval(user_click_history)\n",
    "        # history = list(map(lambda x: self.item_vocab[x], history))\n",
    "        hist_length = len(history)\n",
    "        history = padding_and_clip(history, self.max_seq_len)\n",
    "        feedback = eval(user_clicks)\n",
    "        record = {\n",
    "            'timestamp': int(1),\n",
    "            # 'timestamp': int(timestamp),\n",
    "            'exposure': np.array(exposure).astype(int), \n",
    "            'exposure_features': self.get_item_list_meta(exposure).astype(float),\n",
    "            'feedback': np.array(feedback).astype(float),\n",
    "            'history': np.array(history).astype(int),\n",
    "            'history_features': self.get_item_list_meta(history).astype(float),\n",
    "            'history_length': int(min(hist_length, self.max_seq_len)),\n",
    "            'user_profile': np.array(user_profile)\n",
    "            # 'user_profile': np.array(user_ID).astype(int)\n",
    "        }\n",
    "        return record\n",
    "        \n",
    "    def get_item_list_meta(self, iid_list, from_idx = False):\n",
    "        '''\n",
    "        @input:\n",
    "        - iid_list: item id list\n",
    "        @output:\n",
    "        - meta_data: {field_name: (B,feature_size)}\n",
    "        '''\n",
    "        features = []\n",
    "        for iid in iid_list:\n",
    "            if iid == 0:\n",
    "                features.append([0]*self.item_vec_size)\n",
    "            else:\n",
    "                features.append(self.item_meta[iid-1])\n",
    "        return np.array(features)\n",
    "    \n",
    "    def get_statistics(self):\n",
    "        '''\n",
    "        - n_user\n",
    "        - n_item\n",
    "        - s_parsity\n",
    "        - from BaseReader:\n",
    "            - length\n",
    "            - fields\n",
    "        '''\n",
    "        stats = super().get_statistics()\n",
    "        stats[\"n_item\"] = len(self.item_meta)\n",
    "        stats[\"item_vec_size\"] = self.item_vec_size\n",
    "        stats[\"user_portrait_len\"] = self.portrait_len\n",
    "        stats[\"max_seq_len\"] = self.max_seq_len\n",
    "        stats[\"n_feedback\"] = 2\n",
    "        return stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c56cdfa",
   "metadata": {
    "id": "BZu5Gr-atrEE",
    "papermill": {
     "duration": 0.006485,
     "end_time": "2025-07-17T10:07:42.485640",
     "exception": false,
     "start_time": "2025-07-17T10:07:42.479155",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "855a1502",
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2025-07-17T10:07:42.500211Z",
     "iopub.status.busy": "2025-07-17T10:07:42.499992Z",
     "iopub.status.idle": "2025-07-17T10:07:42.508179Z",
     "shell.execute_reply": "2025-07-17T10:07:42.507635Z"
    },
    "id": "aE05j2QDr6lh",
    "papermill": {
     "duration": 0.016655,
     "end_time": "2025-07-17T10:07:42.509220",
     "exception": false,
     "start_time": "2025-07-17T10:07:42.492565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title Base Model\n",
    "\n",
    "class BaseModel(nn.Module):\n",
    "  def __init__(self, reader, params):\n",
    "    super().__init__()\n",
    "    self.display_name = \"BaseModel\"\n",
    "    self.reader = reader\n",
    "    self.model_path = params['model_path']\n",
    "    self.loss_type = params['loss_type']\n",
    "    self.l2_coef = params['l2_coef']\n",
    "    self.device = params['device']\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "    self._define_params(reader, params)\n",
    "\n",
    "  def get_regularization(self, *modules):\n",
    "    return get_regularization(*modules)\n",
    "\n",
    "  def do_forward_and_loss(self, feed_dict: dict) -> dict:\n",
    "    '''\n",
    "    Used during training to compute predictions and the loss.\n",
    "    '''\n",
    "    out_dict = self.get_forward(feed_dict)\n",
    "    out_dict['loss'] = self.get_loss(feed_dict, out_dict)\n",
    "    return out_dict\n",
    "\n",
    "  def forward(self, feed_dict: dict, return_prob=True) -> dict:\n",
    "    '''\n",
    "      Used during evaluation/prediction to generate predictions and probabilities\n",
    "    '''\n",
    "    out_dict = self.get_forward(feed_dict)\n",
    "    if return_prob:\n",
    "      out_dict['probs'] = self.sigmoid(out_dict['preds'])\n",
    "    return out_dict\n",
    "\n",
    "  def wrap_batch (self, batch):\n",
    "    '''\n",
    "    Build feed_dict from batch data and move data to self.device\n",
    "    '''\n",
    "    for k, val in batch.items():\n",
    "      if type(val).__module__ == np.__name__:\n",
    "        batch[k] = torch.from_numpy(val)\n",
    "      elif torch.is_tensor(val):\n",
    "        batch[k] = val\n",
    "      elif type(val) is list:\n",
    "        batch[k] = torch.tensor(val)\n",
    "      else:\n",
    "        continue # No compatiable type\n",
    "      if batch[k].type() == 'torch.DoubleTensor':\n",
    "        batch[k] = batch[k].type(torch.FloatTensor)\n",
    "      batch[k] = batch[k].to(self.device)\n",
    "    return batch\n",
    "\n",
    "  def save_checkpoint(self):\n",
    "    torch.save({\n",
    "        \"model_state_dict\": self.state_dict(),\n",
    "        \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
    "    }, self.model_path + \".checkpoint\")\n",
    "\n",
    "  def load_checkpoint(self, model_path, with_optimizer=True):\n",
    "    checkpoint = torch.load(model_path + \".checkpoint\",\n",
    "                            map_location=self.device)\n",
    "    self.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    if with_optimizer:\n",
    "      self.optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    self.model_path = model_path\n",
    "\n",
    "  def _define_params(self, reader, params):\n",
    "    pass\n",
    "\n",
    "  def get_forward(self, feed_dict: dict) -> dict:\n",
    "    pass\n",
    "\n",
    "  def get_loss(self, feed_dict: dict, out_dict: dict) -> dict:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33405027",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T10:07:42.523725Z",
     "iopub.status.busy": "2025-07-17T10:07:42.523498Z",
     "iopub.status.idle": "2025-07-17T10:07:42.532861Z",
     "shell.execute_reply": "2025-07-17T10:07:42.532169Z"
    },
    "papermill": {
     "duration": 0.017956,
     "end_time": "2025-07-17T10:07:42.534018",
     "exception": false,
     "start_time": "2025-07-17T10:07:42.516062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class KRUserResponse(BaseModel):\n",
    "    \n",
    "    def log(self):\n",
    "        super().log()\n",
    "        print(\"\\tencoding_dim = \" + str(self.feature_dim))\n",
    "        print(\"\\titem_input_dim = \" + str(self.feature_dim))\n",
    "        print(\"\\tuser_input_dim = \" + str(self.feature_dim))\n",
    "        \n",
    "    def __init__(self, reader, params):\n",
    "        super().__init__(reader, params)\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss(reduction = 'none')\n",
    "        self.loss = []\n",
    "\n",
    "    def _define_params(self, reader, params):\n",
    "        stats = reader.get_statistics()\n",
    "        print(stats)\n",
    "        self.potrait_len = stats['user_portrait_len']\n",
    "        self.item_dim = stats['item_vec_size']\n",
    "        self.feature_dim = params['feature_dim']\n",
    "        self.hidden_dim = params['hidden_dims']\n",
    "        self.attn_n_head = params['attn_n_head']\n",
    "        self.dropout_rate = params['dropout_rate']\n",
    "        self.uEmb = nn.Embedding.from_pretrained(torch.FloatTensor(self.reader.user_meta), freeze=False)\n",
    "        self.iEmb = nn.Embedding.from_pretrained(torch.FloatTensor(self.reader.item_meta), freeze=False)\n",
    "    \n",
    "        # fuse information\n",
    "        self.concat_layer = nn.Linear(self.feature_dim * 2, self.feature_dim)\n",
    "    \n",
    "        # portrait embedding\n",
    "        self.portrait_encoding_layer = DNN(self.potrait_len, self.hidden_dim,\n",
    "                                            self.feature_dim, self.dropout_rate,\n",
    "                                            do_batch_norm= False)\n",
    "        # item embedding\n",
    "        self.item_emb_layer = nn.Linear(self.item_dim, self.feature_dim)\n",
    "    \n",
    "        # user history encoder\n",
    "        self.seq_self_attn_layer = nn.MultiheadAttention(self.feature_dim, self.attn_n_head, batch_first= True)\n",
    "        self.seq_user_attn_layer = nn.MultiheadAttention(self.feature_dim, self.attn_n_head, batch_first= True)\n",
    "    \n",
    "        self.loss = []\n",
    "\n",
    "    def get_forward(self, feed_dict: dict):\n",
    "        user_emb = self.portrait_encoding_layer(feed_dict['user_profile']).view(-1,1,self.feature_dim)\n",
    "\n",
    "\n",
    "        history_item_emb = self.item_emb_layer(feed_dict['history_features'])\n",
    "\n",
    "\n",
    "\n",
    "        # sequence self attention, encoded sequence is (B,H,f_dim)\n",
    "        seq_encoding, attn_weight = self.seq_self_attn_layer(history_item_emb, history_item_emb, history_item_emb)\n",
    "        # cross attention, encoded history is (B,1,f_dim)\n",
    "        user_interest, attn_weight = self.seq_user_attn_layer(user_emb, seq_encoding, seq_encoding)\n",
    "        # rec item embedding (B,L,f_dim)\n",
    "        user_interest = torch.concat([user_interest, user_emb], axis=-1) # waiting \n",
    "        user_interest = self.concat_layer(user_interest)                            # waiting \n",
    "        exposure_item_emb = self.item_emb_layer(feed_dict['exposure_features'])\n",
    "\n",
    "        score = torch.sum(exposure_item_emb * user_interest, dim = -1)\n",
    "        # regularization terms\n",
    "        reg = self.get_regularization(self.uEmb, self.iEmb, self.portrait_encoding_layer, self.item_emb_layer, \n",
    "                                      self.seq_user_attn_layer, self.seq_self_attn_layer)\n",
    "        return {'preds': score, 'reg': reg}\n",
    "    \n",
    "    def get_loss(self, feed_dict: dict, out_dict: dict):\n",
    "        \"\"\"\n",
    "        @input:\n",
    "        - feed_dict: {...}\n",
    "        - out_dict: {\"preds\":, \"reg\":}\n",
    "        \n",
    "        Loss terms implemented:\n",
    "        - BCE\n",
    "        \"\"\"\n",
    "        \n",
    "        preds, reg = out_dict[\"preds\"].view(-1), out_dict[\"reg\"] # (B,L), scalar\n",
    "        target = feed_dict['feedback'].view(-1).to(torch.float) # (B,L)\n",
    "        loss = torch.mean(self.bce_loss(self.sigmoid(preds), target))\n",
    "        loss = loss + self.l2_coef * reg\n",
    "        self.loss.append(loss.item())\n",
    "        return loss\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fde72f",
   "metadata": {
    "papermill": {
     "duration": 0.006556,
     "end_time": "2025-07-17T10:07:42.547238",
     "exception": false,
     "start_time": "2025-07-17T10:07:42.540682",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9263e710",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T10:07:42.561311Z",
     "iopub.status.busy": "2025-07-17T10:07:42.561134Z",
     "iopub.status.idle": "2025-07-17T10:07:42.569655Z",
     "shell.execute_reply": "2025-07-17T10:07:42.568978Z"
    },
    "papermill": {
     "duration": 0.016795,
     "end_time": "2025-07-17T10:07:42.570698",
     "exception": false,
     "start_time": "2025-07-17T10:07:42.553903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = dict()\n",
    "params['train'] = train\n",
    "params['val'] = test\n",
    "params['item_meta'] = item_info\n",
    "params['user_meta'] = user_info\n",
    "params['n_worker'] = 4\n",
    "params['max_seq_len'] = 50\n",
    "\n",
    "params['loss_type'] = 'bce'\n",
    "params['device'] = device\n",
    "params['l2_coef'] = 0.001\n",
    "params['lr'] = 0.0003\n",
    "params['feature_dim'] = 16\n",
    "params['hidden_dims'] = [256]\n",
    "params['attn_n_head'] = 2\n",
    "params['batch_size'] = 128\n",
    "params['seed'] = 13\n",
    "params['epoch'] = 2\n",
    "params['dropout_rate'] = 0.2\n",
    "params['model_path'] = os.path.join(path_to_output, \n",
    "                          f\"env/kr_env_lr{params['lr']}_reg{params['l2_coef']}.model\")\n",
    "set_random_seed(params['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3713e46b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T10:07:42.585059Z",
     "iopub.status.busy": "2025-07-17T10:07:42.584861Z",
     "iopub.status.idle": "2025-07-17T10:08:24.325960Z",
     "shell.execute_reply": "2025-07-17T10:08:24.324878Z"
    },
    "id": "pUiyHg58pQk3",
    "outputId": "de7d8a66-a100-4cfe-b476-963ac617cda0",
    "papermill": {
     "duration": 41.749629,
     "end_time": "2025-07-17T10:08:24.327192",
     "exception": false,
     "start_time": "2025-07-17T10:07:42.577563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init kr reader\n",
      "Load item meta data\n",
      "{'length': 72757, 'n_item': 11643, 'item_vec_size': 32, 'user_portrait_len': 32, 'max_seq_len': 50, 'n_feedback': 2}\n",
      "epoch 0 is training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "72832it [00:14, 4867.18it/s]                           \n",
      "23808it [00:04, 5577.04it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 validating; auc: 0.6804\n",
      "epoch 1 is training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "72832it [00:13, 5312.44it/s]                           \n",
      "23808it [00:04, 5496.82it/s]                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 validating; auc: 0.6839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Train user response\n",
    "reader = KRDataReader(params)\n",
    "model = KRUserResponse(reader, params).to(device)\n",
    "\n",
    "\n",
    "# reader = RL4RSDataReader(params)\n",
    "# model = RL4RSUserResponse(reader, params).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "model.optimizer = optimizer\n",
    "\n",
    "\n",
    "epo = 0\n",
    "while epo < params['epoch']:\n",
    "  print(f\"epoch {epo} is training\")\n",
    "  epo += 1\n",
    "\n",
    "  model.train()\n",
    "  reader.set_phase(\"train\")\n",
    "  train_loader = DataLoader(reader, params['batch_size'], shuffle = True, pin_memory = True,\n",
    "                            num_workers= params['n_worker'])\n",
    "\n",
    "  t1 = time()\n",
    "  pbar = tqdm(total=len(train_loader.dataset))\n",
    "  step_loss = []\n",
    "  for i, batch_data in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    wrapped_batch = wrap_batch(batch_data, device)\n",
    "\n",
    "    out_dict = model.do_forward_and_loss(wrapped_batch)\n",
    "    loss = out_dict['loss']\n",
    "    loss.backward()\n",
    "    step_loss.append(loss.item())\n",
    "    optimizer.step()\n",
    "    pbar.update(params['batch_size'])\n",
    "    # print(model.loss)\n",
    "    # if (i + 1) % 10 == 0:\n",
    "      # print(f\"Iteration {i + 1}, loss {np.mean(step_loss[-100:])}\")\n",
    "  pbar.close()\n",
    "    # print(\"Epoch {}; time {:.4f}\".format(epo, time() - t1))\n",
    "\n",
    "  # validation\n",
    "  t2 = time()\n",
    "  reader.set_phase(\"val\")\n",
    "  val_loader = DataLoader(reader, params['batch_size'], shuffle = False, pin_memory = False,\n",
    "                          num_workers= params['n_worker'])\n",
    "  valid_probs, valid_true =  [], []\n",
    "  pbar = tqdm(total = len(val_loader.dataset))\n",
    "  with torch.no_grad():\n",
    "    for i, batch_data in enumerate(val_loader):\n",
    "      wrapped_batch = wrap_batch(batch_data, device)\n",
    "      out_dict = model.forward(wrapped_batch)\n",
    "      valid_probs.append(out_dict['probs'].cpu().numpy())\n",
    "      valid_true.append(batch_data['feedback'].cpu().numpy())\n",
    "      pbar.update(params['batch_size'])\n",
    "  pbar.close()\n",
    "  auc = roc_auc_score(np.concatenate(valid_true), np.concatenate(valid_probs))\n",
    "  print(f\"epoch {epo} validating\" + \"; auc: {:.4f}\".format(np.mean(auc)))\n",
    "  model.save_checkpoint()\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7768a0dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T10:08:24.407601Z",
     "iopub.status.busy": "2025-07-17T10:08:24.406779Z",
     "iopub.status.idle": "2025-07-17T10:08:24.411205Z",
     "shell.execute_reply": "2025-07-17T10:08:24.410655Z"
    },
    "papermill": {
     "duration": 0.066075,
     "end_time": "2025-07-17T10:08:24.412324",
     "exception": false,
     "start_time": "2025-07-17T10:08:24.346249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !mkdir -p /kaggle/working/kr-hac/env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1907ea8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T10:08:24.450840Z",
     "iopub.status.busy": "2025-07-17T10:08:24.450629Z",
     "iopub.status.idle": "2025-07-17T10:08:24.723198Z",
     "shell.execute_reply": "2025-07-17T10:08:24.722422Z"
    },
    "id": "eqgPbcAsID16",
    "outputId": "ca1dea56-cb24-4412-b8bb-133c693198cd",
    "papermill": {
     "duration": 0.292424,
     "end_time": "2025-07-17T10:08:24.724464",
     "exception": false,
     "start_time": "2025-07-17T10:08:24.432040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABl4AAAIjCAYAAABiRGYbAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAACk90lEQVR4nOzdZ3hU1eL24Wdm0hstlRAIEHonSAgQitLUo6CIKCCCigrhAGJBjopiQ1ERKwhSRQQFKQpSBOm9d0IghZYCIQQSUmfeD/7NezggEkjYyeR3X1c+zJq19zx7Lhaew5O1t8lms9kEAAAAAAAAAACA22Y2OgAAAAAAAAAAAIC9oHgBAAAAAAAAAAAoJBQvAAAAAAAAAAAAhYTiBQAAAAAAAAAAoJBQvAAAAAAAAAAAABQSihcAAAAAAAAAAIBCQvECAAAAAAAAAABQSCheAAAAAAAAAAAACgnFCwAAAAAAAAAAQCGheAEAAACAIjZ9+nSZTCbFxsYaHQUAAABAEaN4AQAAAFDs/FVU7Nixw+goN/TWW2/JZDLl/7i5ualu3bp6/fXXlZaWViifMXv2bI0fP75QzgUAAACg6DkYHQAAAAAASroJEybIw8NDly9f1ooVK/Tee+9p9erV2rhxo0wm022de/bs2Tpw4ICGDRtWOGEBAAAAFCmKFwAAAAC4TY888oi8vb0lSc8//7y6d++un3/+WVu2bFF4eLjB6QAAAADcSdxqDAAAAECJtXv3bt17773y8vKSh4eH7rnnHm3ZsuWqOTk5ORo9erRq1KghFxcXVahQQa1bt9bKlSvz5yQkJKh///6qVKmSnJ2dFRAQoK5du97yM1nuvvtuSVJMTMwN53399deqV6+enJ2dVbFiRUVGRio1NTX//Xbt2mnJkiWKi4vLv51ZcHDwLWUCAAAAcGew4wUAAABAiXTw4EFFRETIy8tLr7zyihwdHfXNN9+oXbt2Wrt2rcLCwiT9+RyWMWPG6JlnnlHz5s2VlpamHTt2aNeuXerYsaMkqXv37jp48KD+/e9/Kzg4WElJSVq5cqXi4+Nvqeg4fvy4JKlChQp/O+ett97S6NGj1aFDBw0cOFBHjx7VhAkTtH37dm3cuFGOjo567bXXdPHiRZ06dUqffvqpJMnDw6PAeQAAAADcORQvAAAAAEqk119/XTk5OdqwYYOqVasmSerbt69q1aqlV155RWvXrpUkLVmyRPfdd58mTZp03fOkpqZq06ZN+uijj/TSSy/lj48cOfKms6SkpEhS/jNevv76a/n5+SkiIuK685OTkzVmzBh16tRJv/32m8zmP29GULt2bQ0ePFizZs1S//791bFjRwUGBurChQvq06fPTecBAAAAYBxuNQYAAACgxMnLy9OKFSvUrVu3/NJFkgICAtSrVy9t2LBBaWlpkqSyZcvq4MGDOnbs2HXP5erqKicnJ61Zs0YXLly4pTy1atWSj4+Pqlatqueee04hISFasmSJ3Nzcrjv/999/V3Z2toYNG5ZfukjSgAED5OXlpSVLltxSDgAAAADGo3gBAAAAUOIkJycrIyNDtWrVuua9OnXqyGq16uTJk5Kkt99+W6mpqapZs6YaNGigl19+Wfv27cuf7+zsrA8//FC//fab/Pz81KZNG40dO1YJCQk3nWf+/PlauXKl1qxZo+joaB04cEChoaF/Oz8uLk6Srsnv5OSkatWq5b8PAAAAoOSheAEAAABg19q0aaPjx49r6tSpql+/vr799ls1bdpU3377bf6cYcOGKSoqSmPGjJGLi4veeOMN1alTR7t3777pz+jQoYPatm2r6tWrF9WlAAAAACgBKF4AAAAAlDg+Pj5yc3PT0aNHr3nvyJEjMpvNCgoKyh8rX768+vfvrx9++EEnT55Uw4YN9dZbb111XPXq1fXiiy9qxYoVOnDggLKzs/XJJ58USf4qVapI0jX5s7OzFRMTk/++JJlMpiLJAAAAAKBoULwAAAAAKHEsFos6deqkRYsWKTY2Nn88MTFRs2fPVuvWreXl5SVJOn/+/FXHenh4KCQkRFlZWZKkjIwMZWZmXjWnevXq8vT0zJ9T2Dp06CAnJyd9/vnnstls+eNTpkzRxYsXdf/99+ePubu76+LFi0WSAwAAAEDhczA6AAAAAAD8nalTp2rZsmXXjA8dOlTvvvuuVq5cqdatW2vQoEFycHDQN998o6ysLI0dOzZ/bt26ddWuXTuFhoaqfPny2rFjh+bNm6fBgwdLkqKionTPPffo0UcfVd26deXg4KAFCxYoMTFRjz32WJFcl4+Pj0aOHKnRo0erS5cuevDBB3X06FF9/fXXuuuuu9SnT5/8uaGhoZo7d66GDx+uu+66Sx4eHnrggQeKJBcAAACA20fxAgAAAKDYmjBhwnXH+/Xrp3r16mn9+vUaOXKkxowZI6vVqrCwMM2aNUthYWH5c4cMGaLFixdrxYoVysrKUpUqVfTuu+/q5ZdfliQFBQXp8ccf16pVq/Tdd9/JwcFBtWvX1o8//qju3bsX2bW99dZb8vHx0ZdffqkXXnhB5cuX17PPPqv3339fjo6O+fMGDRqkPXv2aNq0afr0009VpUoVihcAAACgGDPZ/ntfOwAAAAAAAAAAAG4Zz3gBAAAAAAAAAAAoJBQvAAAAAAAAAAAAhYTiBQAAAAAAAAAAoJBQvAAAAAAAAAAAABQSihcAAAAAAAAAAIBCQvECAAAAAAAAAABQSByMDlAcWa1WnTlzRp6enjKZTEbHAQAAAAAAAAAABrLZbLp06ZIqVqwos/nGe1ooXq7jzJkzCgoKMjoGAAAAAAAAAAAoRk6ePKlKlSrdcA7Fy3V4enpK+vML9PLyMjhN8ZKTk6MVK1aoU6dOcnR0NDoOgELC2gbsF+sbsF+sb8B+sb4B+8X6BkqutLQ0BQUF5fcHN0Lxch1/3V7My8uL4uV/5OTkyM3NTV5eXvzHAbAjrG3AfrG+AfvF+gbsF+sbsF+sb6Dku5nHk9z4RmQAAAAAAAAAAAC4aRQvAAAAAAAAAAAAhYTiBQAAAAAAAAAAoJDwjBcAAAAAAAAAQKmTl5ennJwco2OgmLBYLHJwcLipZ7j8E4oXAAAAAAAAAECpcvnyZZ06dUo2m83oKChG3NzcFBAQICcnp9s6D8ULAAAAAAAAAKDUyMvL06lTp+Tm5iYfH59C2eGAks1msyk7O1vJycmKiYlRjRo1ZDbf+pNaKF4AAAAAAAAAAKVGTk6ObDabfHx85OrqanQcFBOurq5ydHRUXFycsrOz5eLicsvnuvXKBgAAAAAAAACAEoqdLvhft7PL5arzFMpZAAAAAAAAAAAAQPECAAAAAAAAAABQWCheAAAAAAAAAACA1qxZI5PJpNTU1Js+Jjg4WOPHjy+yTCURxQsAAAAAAAAAAMVcv379ZDKZ9Pzzz1/zXmRkpEwmk/r163fngxWCWyl8ijOKFwAAAAAAAAAASoCgoCDNmTNHV65cyR/LzMzU7NmzVblyZQOT4b9RvAAAAAAAAAAASi2bzaaM7FxDfmw2W4GyNm3aVEFBQfr555/zx37++WdVrlxZTZo0uWpuVlaWhgwZIl9fX7m4uKh169bavn37VXOWLl2qmjVrytXVVe3bt1dsbOw1n7lhwwZFRETI1dVVQUFBGjJkiNLT0wuU+3ZduHBBffv2Vbly5eTm5qZ7771Xx44dy38/Li5ODzzwgMqVKyd3d3fVq1dPS5cuzT+2d+/e8vHxkaurq2rUqKFp06YVaV6HIj07AAAAAAAAAADF2JWcPNUdtdyQzz70dme5ORXsn+mfeuopTZs2Tb1795YkTZ06Vf3799eaNWuumvfKK69o/vz5mjFjhqpUqaKxY8eqc+fOio6OVvny5XXy5Ek9/PDDioyM1LPPPqsdO3boxRdfvOocx48fV5cuXfTuu+9q6tSpSk5O1uDBgzV48OAiLy/+W79+/XTs2DEtXrxYXl5eGjFihO677z4dOnRIjo6OioyMVHZ2ttatWyd3d3cdOnRIHh4ekqQ33nhDhw4d0m+//SZvb29FR0dftWOoKFC8AAAAAAAAAABQQvTp00cjR45UXFycJGnjxo2aM2fOVcVLenq6JkyYoOnTp+vee++VJE2ePFkrV67UlClT9PLLL2vChAmqXr26PvnkE0lSrVq1tH//fn344Yf55xkzZox69+6tYcOGSZJq1Kihzz//XG3bttWECRPk4uJS5Nf7V+GyceNGtWzZUpL0/fffKygoSAsXLlSPHj0UHx+v7t27q0GDBpKkatWq5R8fHx+vJk2aqFmzZpKk4ODgIs9M8YKblme16Ys/jisg1+gkAAAAAAAAAFA4XB0tOvR2Z8M+u6B8fHx0//33a/r06bLZbLr//vvl7e191Zzjx48rJydHrVq1yh9zdHRU8+bNdfjwYUnS4cOHFRYWdtVx4eHhV73eu3ev9u3bp++//z5/zGazyWq1KiYmRnXq1Llh1vXr1+cXP5L0zTff5O/UuVmHDx+Wg4PDVVkrVKigWrVq5V/LkCFDNHDgQK1YsUIdOnRQ9+7d1bBhQ0nSwIED1b17d+3atUudOnVSt27d8gucokLxgpv2zq+HNH1TrILcLerUMUcVHB2NjgQAAAAAAAAAt8VkMhX4dl9Ge+qppzR48GBJ0ldffVVkn3P58mU999xzGjJkyDXvVa5c+R+Pb9asmfbs2ZP/2s/PrzDj5XvmmWfUuXNnLVmyRCtWrNCYMWP0ySef6N///rfuvfdexcXFaenSpVq5cqXuueceRUZG6uOPPy6SLJJkLrIzw+481jxI5dwcdTLdpP4zdurilRyjIwEAAAAAAABAqdOlSxdlZ2crJydHnTtfu1unevXqcnJy0saNG/PHcnJytH37dtWtW1eSVKdOHW3btu2q47Zs2XLV66ZNm+rQoUMKCQm55sfJyekfc7q6ul51jKenZ4GvtU6dOsrNzdXWrVvzx86fP6+jR4/mX4skBQUF6fnnn9fPP/+sF198UZMnT85/z8fHR08++aRmzZql8ePHa9KkSQXOURAUL7hptf29NLN/M7k72LTvdJoem7RFG46dMzoWAAAAAAAAAJQqFotFhw8f1qFDh2SxXHu7Mnd3dw0cOFAvv/yyli1bpkOHDmnAgAHKyMjQ008/LUl6/vnndezYMb388ss6evSoZs+erenTp191nhEjRmjTpk0aPHiw9uzZo2PHjmnRokX5u20K2/79+7Vnz578n71796pGjRrq2rWrBgwYoA0bNmjv3r3q06ePAgMD1bVrV0nSsGHDtHz5csXExGjXrl36448/8m+DNmrUKC1atEjR0dE6ePCgfv3113+8RdrtKln7p2C42v6eiqybp8nRrjp8Nk19pmzVWw/UVb9WVY2OBgAAAAAAAAClhpeX1w3f/+CDD2S1WvXEE0/o0qVLatasmZYvX65y5cpJ+vNWYfPnz9cLL7ygL774Qs2bN9f777+vp556Kv8cDRs21Nq1a/Xaa68pIiJCNptN1atXV8+ePYvkmtq0aXPVa4vFotzcXE2bNk1Dhw7Vv/71L2VnZ6tNmzZaunSpHP/vcRh5eXmKjIzUqVOn5OXlpS5duujTTz+VJDk5OWnkyJGKjY2Vq6urIiIiNGfOnCLJ/xeTzWazFeknlEBpaWkqU6aMLl68+I9/eEubnJwcLV26VE1b362v18boh20nZTGbNLFPqDrWLZr78wEoen+t7fvuuy//P1gA7APrG7BfrG/AfrG+AfvF+i4eMjMzFRMTo6pVq8rFxcXoOChGbvRnoyC9Abcawy3x93LR+w81UPemlZRntenZ73boi1XHZLXS4wEAAAAAAAAASi+KF9wyk8mkMQ83UO+wyrLZpE9WRunJadu0+kii2EgFAAAAAAAAACiNKF5wW5wczHrvoQb6sHsDOVnMWn/snJ6avkPPzNihpEuZRscDAAAAAAAAAOCOonhBoeh5V2UtHRqh/q2C5WQxa9WRJHX6dJ1WHEwwOhoAAAAAAAAAAHcMxQsKTYivh958oJ5++Xdr1avopdSMHD03a6e++iNaV7LzjI4HAAAAAAAAAPl4XAL+V2H9maB4QaGr5e+pBYNaqU+LP5/98tHyo2r5wSp9vPyoLmflGh0PAAAAAAAAQClmsVgkSdnZ2QYnQXGTkZEhSXJ0dLyt8zgURhjgfzk5mPVO1/qqX7GMvloTrZMpV/TlH9HafOK8Zj7VXO7O/NEDAAAAAAAAcOc5ODjIzc1NycnJcnR0lNnM/oTSzmazKSMjQ0lJSSpbtmx+OXer+NdvFBmTyaTHmldWj2ZBWn4wQa/O36edcRfUc9Jmje/ZRCG+HkZHBAAAAAAAAFDKmEwmBQQEKCYmRnFxcUbHQTFStmxZ+fv73/Z5KF5Q5Cxmk+5rEKCAMi7qP327DpxOU+fx69QjtJLefKCeXJ1urz0EAAAAAAAAgIJwcnJSjRo1uN0Y8jk6Ot72Tpe/FIvi5auvvtJHH32khIQENWrUSF988YWaN29+3bnt2rXT2rVrrxm/7777tGTJEkl/bgt68803NXnyZKWmpqpVq1aaMGGCatSoUaTXgRtrUrmclg1to5E/79MfR5M1Z/tJxZ5P19R+d8nNqVj8UQQAAAAAAABQSpjNZrm4uBgdA3bI8JvXzZ07V8OHD9ebb76pXbt2qVGjRurcubOSkpKuO//nn3/W2bNn838OHDggi8WiHj165M8ZO3asPv/8c02cOFFbt26Vu7u7OnfurMzMzDt1Wfgb/mVcNK1/c33/TJg8nB205USK+k3broSLmbLZbEbHAwAAAAAAAADgthi+zWDcuHEaMGCA+vfvL0maOHGilixZoqlTp+rVV1+9Zn758uWvej1nzhy5ubnlFy82m03jx4/X66+/rq5du0qSZs6cKT8/Py1cuFCPPfbYNefMyspSVlZW/uu0tDRJUk5OjnJycgrnQu3EX9/H7X4vzauU0dQnm+qpGbu0LSZFLcasUtPKZfXlY43k4+lcGFEBFEBhrW0AxQ/rG7BfrG/AfrG+AfvF+gZKroKsW5PNwG0G2dnZcnNz07x589StW7f88SeffFKpqalatGjRP56jQYMGCg8P16RJkyRJJ06cUPXq1bV79241btw4f17btm3VuHFjffbZZ9ec46233tLo0aOvGZ89e7bc3NwKfmG4aXGXpR+OW5SQIdlkUjknmx6sYlWTCjaZTEanAwAAAAAAAABAysjIUK9evXTx4kV5eXndcK6hO17OnTunvLw8+fn5XTXu5+enI0eO/OPx27Zt04EDBzRlypT8sYSEhPxz/O85/3rvf40cOVLDhw/Pf52WlqagoCB16tTpH7/A0iYnJ0crV65Ux44d5ejoWCjnHCgp7nyGnpq5U/EpVzTjmEVnHHz1wUP15OVaOJ8B4MaKYm0DKB5Y34D9Yn0D9ov1Ddgv1jdQcv11p6ybYfitxm7HlClT1KBBAzVv3vy2zuPs7Cxn52tvb+Xo6MhfgH+jsL+bEP8y+m1oG327PkZf/RGtlYeTtD3ugkbeW1s976pcaJ8D4Mb4ew+wX6xvwH6xvgH7xfoG7BfrGyh5CrJmzUWY4x95e3vLYrEoMTHxqvHExET5+/vf8Nj09HTNmTNHTz/99FXjfx13K+eEsdydHTS0Qw39+Hy4Qnw9lJqRoxHz9+vb9SeMjgYAAAAAAAAAwE0xtHhxcnJSaGioVq1alT9mtVq1atUqhYeH3/DYn376SVlZWerTp89V41WrVpW/v/9V50xLS9PWrVv/8ZwoHhoHldWyoREa3D5EkvTuksN6+5dDyszJMzgZAAAAAAAAAAA3ZmjxIknDhw/X5MmTNWPGDB0+fFgDBw5Uenq6+vfvL0nq27evRo4cec1xU6ZMUbdu3VShQoWrxk0mk4YNG6Z3331Xixcv1v79+9W3b19VrFhR3bp1uxOXhELgYDHrxU419XLnWpKkqRtj1PajP/T7ocR/OBIAAAAAAAAAAOMY/oyXnj17Kjk5WaNGjVJCQoIaN26sZcuWyc/PT5IUHx8vs/nqfujo0aPasGGDVqxYcd1zvvLKK0pPT9ezzz6r1NRUtW7dWsuWLZOLi0uRXw8Kj8lkUmT7EFX38dDoXw7q7MVMDfhuh17oUFPPta0mZweL0REBAAAAAAAAALiK4cWLJA0ePFiDBw++7ntr1qy5ZqxWrVqy2Wx/ez6TyaS3335bb7/9dmFFhIG61PdX+9o+Gv3LIc3eGq9xK6M0f9cpvXZfHXWqx3N7AAAAAAAAAADFh+G3GgNuhrODRe91q69xjzaSr6ez4s5n6NnvdmrssiM3LOEAAAAAAAAAALiTKF5QYphMJj3ctJL+eKmdnmtbTZL09Zrjeu67nTp1IcPgdAAAAAAAAAAAULygBHJ3dtDIe+vovYfqy2I2acWhRHUYt1Zfrj6mrNw8o+MBAAAAAAAAAEoxiheUWL3DqmjpkAiFVS2vzByrPl4RpS7j1+tE8mWjowEAAAAAAAAASimKF5Rotfw9NefZFvrsscby9XRWzLl0PT55i2LOpRsdDQAAAAAAAABQClG8oMQzmUzq2jhQvw2NUE0/DyWmZenxSVt0LPGS0dEAAAAAAAAAAKUMxQvsRgUPZ33/TAuF+HooIS1Tncav09PTt+vA6YtGRwMAAAAAAAAAlBIUL7ArPp7Omj0gTO1q+chmk1YdSdK/vtiggbN2sgMGAAAAAAAAAFDkKF5gd3w9XTS9f3OtfrGtujWuKJNJ+u1AgjqPX6f/LNivi1dyjI4IAAAAAAAAALBTFC+wW9V8PDT+sSZaNrSNOtfzk9Umzd4ar47j1mrZgQSj4wEAAAAAAAAA7BDFC+xeLX9PffNEM819toWqebsr6VKWnp+1U99vjTM6GgAAAAAAAADAzlC8oNQIq1ZBS4dGqF/LYEnS6wsPUL4AAAAAAAAAAAoVxQtKFRdHi958oK56h1WWzSa9tuCAhs7ZraS0TKOjAQAAAAAAAADsAMULSh2TyaR3u9XXy51ryWSSFu05o7s/WatJ644rO9dqdDwAAAAAAAAAQAlG8YJSyWQyKbJ9iBYOaqVGQWV1OStX7y89ons/W6f1x5KNjgcAAAAAAAAAKKEoXlCqNQoqqwUDW2rsIw1Vwd1Jx5PT9cSUbXr+u506mZJhdDwAAAAAAAAAQAlD8YJSz2w26dFmQVr9Ujv1bxUsi9mkZQcTdM+4tRqz9LCOJlwyOiIAAAAAAAAAoISgeAH+TxlXR735QD0tHRKhltUrKDvXqm/WnVDn8ev04o97dSE92+iIAAAAAAAAAIBijuIF+B+1/D31/TNhmvREqDrU8ZXJJM3fdUodxq3VxLXHdfhsmtERAQAAAAAAAADFlIPRAYDiyGQyqVM9f3Wq56+dcRf06vx9OpZ0WR/8dkQf/HZE9Sp6qeddQXoktJLcnFhGAAAAAAAAAIA/seMF+AehVcppyZAIvd21nu6u7StHi0kHz6Rp1KKDav/xGs3beUpWq83omAAAAAAAAACAYoBf1QdugpODWX3Dg9U3PFgp6dlauPu0pm6M0akLV/TST3s1e2ucvu4dKv8yLkZHBQAAAAAAAAAYiB0vQAGVd3fSU62r6vfhbTWiS215ODtoV3yqHvhyg1YdTjQ6HgAAAAAAAADAQBQvwC1ycbRoYLvqWjokQjX9PJR8KUtPz9ihyNm7FHsu3eh4AAAAAAAAAAADULwAt6lyBTctimyt59pUk8Vs0pJ9Z9Xu4zXq+tVGbT5+3uh4AAAAAAAAAIA7iOIFKASuThaNvK+OFkW2UkQNbzmYTdp7MlWPT96ip6dv1+GzaUZHBAAAAAAAAADcAQ5GBwDsSf3AMvru6TCdu5ylz1cd0/db47XqSJJWHUlSl3r+eqFjTdXy9zQ6JgAAAAAAAACgiLDjBSgC3h7Oertrfa14oY3+1TBAJpO07GCC7v98vT5dGaWLV3KMjggAAAAAAAAAKAIUL0ARqu7joS97NdWKYW3UoY6fcq02fbbqmMLe/11PTd+uTdHnjI4IAAAAAAAAAChEFC/AHVDDz1OT+4bq88ebqKafhzJzrFp9JEm9vt2q95YcUm6e1eiIAAAAAAAAAIBCwDNegDvEZDLpwUYV9UDDAB06m6ZZW+L1w7Z4TV4fo6OJl/Vxj4by9XQxOiYAAAAAAAAA4Daw4wW4w0wmk+pVLKMxDzfQ172bysXRrHVRyer06Tot3H1aNpvN6IgAAAAAAAAAgFtE8QIY6L4GAVowqJXqBngpNSNHw+bu0YCZO5WUlml0NAAAAAAAAADALaB4AQxWJ8BLiwa30vCONeVoMen3w4nqMG6t5u88xe4XAAAAAAAAAChhKF6AYsDRYtaQe2rol3+3VoPAMkrLzNWLP+3V0zN2KOEiu18AAAAAAAAAoKSgeAGKkdr+XlowqKVe7lxLThazVh9JUsdP1+rHHSfZ/QIAAAAAAAAAJQDFC1DMOFjMimwfoiVDWqtRUFldyszVK/P26clp23Um9YrR8QAAAAAAAAAAN0DxAhRTNfw8Nf/5cL16b205OZi1LipZnT5dpznb4tn9AgAAAAAAAADFFMULUIw5WMx6vm11LR0SoaaVy+pyVq5e/Xm/Xpi7R5k5eUbHAwAAAAAAAAD8D4oXoAQI8fXQT8+31H/uqy2L2aSFe86ox8TNOs2txwAAAAAAAACgWKF4AUoIi9mkZ9tU13dPN1c5N0ftP31RHT5Zq6/XRMtq5dZjAAAAAAAAAFAcULwAJUzL6t5aPLi1QquU05WcPI1ddlTPfrdDx5MvGx0NAAAAAAAAAEo9ihegBAoq76Z5z4frw+4N5GQx6/fDSbrnk7X6z4L9upSZY3Q8AAAAAAAAACi1KF6AEspkMqnnXZX186CW6lDHV5I0e2u8On26TssOnOX2YwAAAAAAAABgAIoXoISrH1hG3z55l2YPCFPl8m46ezFTz8/apU7j12lT9Dmj4wEAAAAAAABAqULxAtiJltW9tXxYG0W2ry5PFwdFJ11Wr2+36sNlR9j9AgAAAAAAAAB3CMULYEdcnSx6uXNtbXz1bj3RoookacKa4+o/fbsS0zINTgcAAAAAAAAA9o/iBbBDXi6OeqdbfY17tJGcHMxaG5Ws+z9fr32nUo2OBgAAAAAAAAB2jeIFsGMPN62kJf9urdr+njp3OVuPTNisUYsO6EzqFaOjAQAAAAAAAIBdongB7FwNP0/NG9hS99T2VXaeVTM3x6ntR39ozG+HlZmTZ3Q8AAAAAAAAALArFC9AKeDh7KBvn2ym2QPC1KJaeeXk2fTN2hO697P12haTYnQ8AAAAAAAAALAbFC9AKWEymdSyurfmPBuuyX2bydfTWTHn0vXoN5v17MwdSriYaXREAAAAAAAAACjxKF6AUqhjXT+tHN5WjzcPktkkrTiUqH99sV6/7T8rm81mdDwAAAAAAAAAKLEoXoBSqoyro8Y83FDLhrVRbX9PnbucrYHf79Jjk7bodOoVo+MBAAAAAAAAQIlE8QKUcjX9PLVgUCsNuTtEro4WbY1JUcdxa/XJiqPKzMkzOh4AAAAAAAAAlCgULwDk6mTR8E61tGxYhEKrlFNGdp6+WB2trl9u1NGES0bHAwAAAAAAAIASg+IFQL4qFdw17/lwTejdVN4ezjqaeEkPfrlBs7bE8ewXAAAAAAAAALgJFC8ArmIymXRvgwAtGxahtjV9lJVr1esLD+j5WTuVmpFtdDwAAAAAAAAAKNYoXgBcl7eHs6b1u0uv319HjhaTlh9MVJfx6/Xdljie/QIAAAAAAAAAf4PiBcDfMptNeiaimhYMaqWq3u5KSMvUGwsPqPWHqzVp3XEKGAAAAAAAAAD4HxQvAP5R/cAyWjokQqMfrKfAsq46dzlb7y89ovYfr9G8nad4/gsAAAAAAAAA/B+KFwA3xdXJoidbBmvNy+009pGGqljGRWcvZuqln/bq5Xn7lJXL7hcAAAAAAAAAoHgBUCCOFrMebRak1S+108uda8lskubtPKUu49dryb6zys2zGh0RAAAAAAAAAAxD8QLglrg4WhTZPkRT+90lH09nxZxLV+TsXbr3s/U6mnDJ6HgAAAAAAAAAYAjDi5evvvpKwcHBcnFxUVhYmLZt23bD+ampqYqMjFRAQICcnZ1Vs2ZNLV26NP/9S5cuadiwYapSpYpcXV3VsmVLbd++vagvAyi12tXy1eoX22rI3SEq5+aoY0mXdd/n69Xn263aHptidDwAAAAAAAAAuKMMLV7mzp2r4cOH680339SuXbvUqFEjde7cWUlJSdedn52drY4dOyo2Nlbz5s3T0aNHNXnyZAUGBubPeeaZZ7Ry5Up999132r9/vzp16qQOHTro9OnTd+qygFLH08VRwzvV0u/D2+ru2r7Ks9q0IfqcekzcrL5Tt2lj9DmjIwIAAAAAAADAHWFo8TJu3DgNGDBA/fv3V926dTVx4kS5ublp6tSp150/depUpaSkaOHChWrVqpWCg4PVtm1bNWrUSJJ05coVzZ8/X2PHjlWbNm0UEhKit956SyEhIZowYcKdvDSgVKrg4ayp/e7S2pfb6fHmQTKZpHVRyeozZau+WHVMWbl5RkcEAAAAAAAAgCLlYNQHZ2dna+fOnRo5cmT+mNlsVocOHbR58+brHrN48WKFh4crMjJSixYtko+Pj3r16qURI0bIYrEoNzdXeXl5cnFxueo4V1dXbdiw4W+zZGVlKSsrK/91WlqaJCknJ0c5OTm3c5l256/vg+8FN1LRy0lvP1BHT7esom/Wx+innaf1ycoozdoap4+7N1CLauWNjoj/wdoG7BfrG7BfrG/AfrG+AfvF+gZKroKsW8OKl3PnzikvL09+fn5Xjfv5+enIkSPXPebEiRNavXq1evfuraVLlyo6OlqDBg1STk6O3nzzTXl6eio8PFzvvPOO6tSpIz8/P/3www/avHmzQkJC/jbLmDFjNHr06GvGV6xYITc3t9u7UDu1cuVKoyOghGjtJDlUN2npSbMS07L05PTteqSqVa38bEZHw3WwtgH7xfoG7BfrG7BfrG/AfrG+gZInIyPjpueabDabIf/6eebMGQUGBmrTpk0KDw/PH3/llVe0du1abd269ZpjatasqczMTMXExMhisUj683ZlH330kc6ePStJOn78uJ566imtW7dOFotFTZs2Vc2aNbVz504dPnz4ulmut+MlKChI586dk5eXV2FedomXk5OjlStXqmPHjnJ0dDQ6DkqQzJw8/WfhQf2yL0GS1CM0UC91rKHy7k4GJ4PE2gbsGesbsF+sb8B+sb4B+8X6BkqutLQ0eXt76+LFi//YGxi248Xb21sWi0WJiYlXjScmJsrf3/+6xwQEBMjR0TG/dJGkOnXqKCEhQdnZ2XJyclL16tW1du1apaenKy0tTQEBAerZs6eqVav2t1mcnZ3l7Ox8zbijoyN/Af4NvhsUlKOjoz5/vKlq+Ufr4xVR+mnnaS07mKghd9dQ35ZV5Oxg+eeToMixtgH7xfoG7BfrG7BfrG/AfrG+gZKnIGvWXIQ5bsjJyUmhoaFatWpV/pjVatWqVauu2gHz31q1aqXo6GhZrdb8saioKAUEBMjJ6erfmnd3d1dAQIAuXLig5cuXq2vXrkVzIQBumslk0uC7a2jOsy1Ur6KXLmXm6r2lh9Xp03XaFH3O6HgAAAAAAAAAcNsMK14kafjw4Zo8ebJmzJihw4cPa+DAgUpPT1f//v0lSX379tXIkSPz5w8cOFApKSkaOnSooqKitGTJEr3//vuKjIzMn7N8+XItW7ZMMTExWrlypdq3b6/atWvnnxOA8VpUq6DFg1tr7CMN5ePprLjzGeozZate/mmvNlLAAAAAAAAAACjBDLvVmCT17NlTycnJGjVqlBISEtS4cWMtW7ZMfn5+kqT4+HiZzf+/GwoKCtLy5cv1wgsvqGHDhgoMDNTQoUM1YsSI/DkXL17UyJEjderUKZUvX17du3fXe++9x9Y9oJixmE16tFmQ7m8QoNG/HNSPO07pp51//jzVqqpe7lxLrk7cfgwAAAAAAABAyWJo8SJJgwcP1uDBg6/73po1a64ZCw8P15YtW/72fI8++qgeffTRwooHoIi5Ozto7CON1K1xoBbuOa0fd5zS1I0x+mXfGQ25O0Q976osJwdDN+cBAAAAAAAAwE3jXzMBFAstQ7w19pFGmty3mSqVc1XypSy9seig7hm3Rgt2n1Ke1WZ0RAAAAAAAAAD4RxQvAIqVjnX9tPrFdnqnaz35eDrrZMoVvTB3r+7/fL2OJ182Oh4AAAAAAAAA3BDFC4Bix8nBrCfCg7X25XZ6pUstebk46EjCJT301UbN3R6v3Dyr0REBAAAAAAAA4LooXgAUW25ODhrULkSrX2qn0CrllJaZqxHz9+vRbzYr6VKm0fEAAAAAAAAA4BoULwCKPW8PZ80eEKbX768jLxcH7YpP1T2frNXoXw7qBLcfAwAAAAAAAFCMULwAKBGcHSx6JqKaFg1urdr+nrqUmatpG2N19ydr1efbrVp+MIFbkAEAAAAAAAAwnIPRAQCgIKp6u2vpkAitO5asWVvitOpIkjZEn9OG6HOq7uOud7rWV4tqFWQ2m4yOCgAAAAAAAKAUongBUOKYzSa1q+WrdrV8dTIlQ7O3xWvOtngdT05Xr2+3qry7k55qFaynWleVmxN/zQEAAAAAAAC4c7jVGIASLai8m0Z0qa01L7VXr7DK8nB2UEp6tj5eEaU2Y9foqz+iFX8+w+iYAAAAAAAAAEoJihcAdqGMm6Pef6iB9ozqqPE9G6tyeTedu5ylj5YfVftP1ug/C/br0Jk0o2MCAAAAAAAAsHMULwDsioPFrG5NAvX78LYa272hwqtVUJ7Vptlb43Xf5+s1cNZOpWZkGx0TAAAAAAAAgJ2ieAFgl5wczHr0riD98GwLzXm2hTrX85OD2aTfDiSo/cdrNHVDjPKsNqNjAgAAAAAAALAzPHUagN1rUa2CWlSroP2nLuqFH/coOumy3v71kH47cFZta/qoa+NABZV3MzomAAAAAAAAADvAjhcApUaDSmW0bGiE3u1WX66OFm2PvaCPV0Tp7k/WaMxvh5WVm2d0RAAAAAAAAAAlHDteAJQqDhaz+rSootYh3vp13xltOn5em46f1zdrT2j14SS92KmWOtb1k8VsMjoqAAAAAAAAgBKIHS8ASqVgb3cNvruGZg9ooUlPhKq8u5OOJV3W87N2qs3YP7QjNsXoiAAAAAAAAABKIIoXAKVep3r+Wv1iW0W2r64yro46nXpFA7/fpXOXs4yOBgAAAAAAAKCEoXgBAEll3Zz0cufa2jzybtXw9VDypSwNm7NHOXlWo6MBAAAAAAAAKEEoXgDgv7g5OeiLXk3k6mjRhuhzenPxQdlsNqNjAQAAAAAAACghKF4A4H/U9vfSZ481lskkzd4arykbYoyOBAAAAAAAAKCEoHgBgOvoVM9fr91XR5L03tLDWnYgweBEAAAAAAAAAEoCihcA+BtPt66q3mGVZbNJQ+fs1tYT542OBAAAAAAAAKCYo3gBgL9hMpk0+sF6uqe2r7Jyreo7dZvWRSUbHQsAAAAAAABAMUbxAgA34GAx66veTfPLl8jvdynmXLrRsQAAAAAAAAAUUxQvAPAPXBwtmvhEqO4KLqdLWbkaOGunsnLzjI4FAAAAAAAAoBiieAGAm+BoMevLXk1Vwd1JRxIu6bPfjxkdCQAAAAAAAEAxRPECADfJz8tF7z1UX5I0ce1xfb81zuBEAAAAAAAAAIobihcAKIAu9QPUO6yyrDbptQUH9MO2eKMjAQAAAAAAAChGKF4AoIDe7VZfke2rS5LeW3JYZy9eMTgRAAAAAAAAgOKC4gUACshkMml4x1pqUrmsLmflavjcvcrJsxodCwAAAAAAAEAxQPECALfAYjbpo0cayt3Jos0nzmvUooOy2WxGxwIAAAAAAABgMIoXALhFIb6eGv9YE5lM0g/b4vX2r4dktVK+AAAAAAAAAKUZxQsA3IaOdf005qEGkqRpG2PVZ8pWJaZlGpwKAAAAAAAAgFEoXgDgNj3WvLI+7dlIbk4WbTp+Xl3Gr9Om6HNGxwIAAAAAAABgAIoXACgEDzWppF//3Vr1A710ISNHz8zcoT0nU42OBQAAAAAAAOAOo3gBgEJSzcdD8we2VEQNb2Vk56nPt1u1+kii0bEAAAAAAAAA3EEULwBQiJwdLJrYJ1QtqpXX5axcPTNjhxbvPWN0LAAAAAAAAAB3CMULABQyd2cHzXwqTN2bVpLVJr0wd4+W7DtrdCwAAAAAAAAAdwDFCwAUAScHsz56pKEeCa2kPKtNQ+bs1oqDCUbHAgAAAAAAAFDEKF4AoIiYzSZ92L2hHmoSqDyrTf/+Ybd2xl0wOhYAAAAAAACAIkTxAgBFyGI26aNHGuqe2r7KyrVq0Pc7lZaZY3QsAAAAAAAAAEWE4gUAipiDxawvejVRVW93JaZlaeyyI0ZHAgAAAAAAAFBEKF4A4A5wc3LQew/VlyTN2hKvH7bFG5wIAAAAAAAAQFGgeAGAO6RldW8NbFddkvSfBfs1bWOMwYkAAAAAAAAAFDaKFwC4g17pXEv9WgbLZpNG/3JIn686ZnQkAAAAAAAAAIWI4gUA7iCTyaQ3H6irV7rUkiSNWxmlxXvPGJwKAAAAAAAAQGGheAGAO8xkMmlQuxANiKgqSXptwX4lX8oyOBUAAAAAAACAwkDxAgAGefXeOmoQWEaXMnP1/tLDRscBAAAAAAAAUAgoXgDAIBazSe90qy+TSVqw+7QmrTtudCQAAAAAAAAAt4niBQAM1DiorF7pXFuS9P7SI5qzLd7gRAAAAAAAAABuB8ULABhsYLvqer5tdUnSyAX7teJggsGJAAAAAAAAANwqihcAKAZGdKmlx5tXls0mvfjTXp1MyTA6EgAAAAAAAIBbQPECAMWAyWTS213rqWnlsrqUmavBs3cpO9dqdCwAAAAAAAAABUTxAgDFhKPFrC96NVUZV0ftPXVRH/x2xOhIAAAAAAAAAAqI4gUAipHAsq76pEcjSdLUjTE87wUAAAAAAAAoYSheAKCY6VDXT8+0ripJeumnvdpzMtXYQAAAAAAAAABuGsULABRDr3SpraaVyyotM1ePfrNZfxxNMjoSAAAAAAAAgJtA8QIAxZCTg1kznw7T3bV9lZ1rVeT3u3Tg9EWjYwEAAAAAAAD4BxQvAFBMeTg7aGKfULUO8VZGdp6en7VTaZk5RscCAAAAAAAAcAMULwBQjDk5mPV1n6YKKu+qUxeuaOTP+2W12oyOBQAAAAAAAOBvULwAQDHn5eKozx5rIovZpCX7zuqtXw7KZqN8AQAAAAAAAIojihcAKAGaVi6nj3s0lMkkzdwcpw+WHaF8AQAAAAAAAIohihcAKCEealJJ73VrIEn6Zu0JDZy1S0mXMg1OBQAAAAAAAOC/UbwAQAnSK6yyRj9YTxazScsOJuixSVt0IT3b6FgAAAAAAAAA/g/FCwCUME+2DNav/26timVcdCI5Xc/N2imrlduOAQAAAAAAAMWB4cXLV199peDgYLm4uCgsLEzbtm274fzU1FRFRkYqICBAzs7OqlmzppYuXZr/fl5ent544w1VrVpVrq6uql69ut555x2ehQDArtQJ8NLMp5vL3cmibTEpWrz3jNGRAAAAAAAAAMjg4mXu3LkaPny43nzzTe3atUuNGjVS586dlZSUdN352dnZ6tixo2JjYzVv3jwdPXpUkydPVmBgYP6cDz/8UBMmTNCXX36pw4cP68MPP9TYsWP1xRdf3KnLAoA7IsTXU4Pah0iSPvjtCM97AQAAAAAAAIoBQ4uXcePGacCAAerfv7/q1q2riRMnys3NTVOnTr3u/KlTpyolJUULFy5Uq1atFBwcrLZt26pRo0b5czZt2qSuXbvq/vvvV3BwsB555BF16tTpH3fSAEBJ9HTrqqpc3k0JaZl67JstlC8AAAAAAACAwRyM+uDs7Gzt3LlTI0eOzB8zm83q0KGDNm/efN1jFi9erPDwcEVGRmrRokXy8fFRr169NGLECFksFklSy5YtNWnSJEVFRalmzZrau3evNmzYoHHjxv1tlqysLGVlZeW/TktLkyTl5OQoJyenMC7Xbvz1ffC9AMWDRdLUvk3Vd9oOnTiXrqE/7Na0J0NlMZsKdB7WNmC/WN+A/WJ9A/aL9Q3YL9Y3UHIVZN0aVrycO3dOeXl58vPzu2rcz89PR44cue4xJ06c0OrVq9W7d28tXbpU0dHRGjRokHJycvTmm29Kkl599VWlpaWpdu3aslgsysvL03vvvafevXv/bZYxY8Zo9OjR14yvWLFCbm5ut3GV9mvlypVGRwDwX/pVlT7eZ9HmEykaNHG5Hqxilalg3Ysk1jZgz1jfgP1ifQP2i/UN2C/WN1DyZGRk3PRcw4qXW2G1WuXr66tJkybJYrEoNDRUp0+f1kcffZRfvPz444/6/vvvNXv2bNWrV0979uzRsGHDVLFiRT355JPXPe/IkSM1fPjw/NdpaWkKCgpSp06d5OXldUeuraTIycnRypUr1bFjRzk6OhodB8B/KVf9jF6ef0Crz5oVXLWqXu1SU6abbF9Y24D9Yn0D9ov1Ddgv1jdgv1jfQMn1152yboZhxYu3t7csFosSExOvGk9MTJS/v/91jwkICJCjo2P+bcUkqU6dOkpISFB2dracnJz08ssv69VXX9Vjjz0mSWrQoIHi4uI0ZsyYvy1enJ2d5ezsfM24o6MjfwH+Db4boPjpcVcVXcm1adSig5q6KU55NumtB+vddPkisbYBe8b6BuwX6xuwX6xvwH6xvoGSpyBr1lyEOW7IyclJoaGhWrVqVf6Y1WrVqlWrFB4eft1jWrVqpejoaFmt1vyxqKgoBQQEyMnJSdKf233M5qsvy2KxXHUMANirvuHBGvNwA5lM0ozNcfrPggOyWm1GxwIAAAAAAABKDcOKF0kaPny4Jk+erBkzZujw4cMaOHCg0tPT1b9/f0lS3759NXLkyPz5AwcOVEpKioYOHaqoqCgtWbJE77//viIjI/PnPPDAA3rvvfe0ZMkSxcbGasGCBRo3bpweeuihO359AGCEx5tX1kePNJLZJP2wLV5D5uxWVm6e0bEAAAAAAACAUsHQZ7z07NlTycnJGjVqlBISEtS4cWMtW7ZMfn5+kqT4+Pirdq8EBQVp+fLleuGFF9SwYUMFBgZq6NChGjFiRP6cL774Qm+88YYGDRqkpKQkVaxYUc8995xGjRp1x68PAIzySGglOTmY9eKPe/TrvrNydbToox6NjI4FAAAAAAAA2D1DixdJGjx4sAYPHnzd99asWXPNWHh4uLZs2fK35/P09NT48eM1fvz4QkoIACXTg40qytPFQU9N366fdp5Sh7p+6lzv+s/QAgAAAAAAAFA4butWY5mZmYWVAwBQBNrX8tWzbapJkl6Yu0fbYlIMTgQAAAAAAADYtwIXL1arVe+8844CAwPl4eGhEydOSJLeeOMNTZkypdADAgBuz/CONdU6xFsZ2XnqP22bjidfNjoSAAAAAAAAYLcKXLy8++67mj59usaOHSsnJ6f88fr16+vbb78t1HAAgNvn7GDRt082U/Pg8krPzlPk97t0IT3b6FgAAAAAAACAXSpw8TJz5kxNmjRJvXv3lsViyR9v1KiRjhw5UqjhAACFw8XRoi96NVEFdycdSbike8at1dqoZKNjAQAAAAAAAHanwMXL6dOnFRIScs241WpVTk5OoYQCABQ+Py8XzXiquWr4eiglPVtPT9+uxXvPGB0LAAAAAAAAsCsFLl7q1q2r9evXXzM+b948NWnSpFBCAQCKRv3AMloyJEJdG1dUrtWm4XP3aB07XwAAAAAAAIBC41DQA0aNGqUnn3xSp0+fltVq1c8//6yjR49q5syZ+vXXX4siIwCgEDk5mPXpo40lSYv2nNHzs3Zq8hMU5wAAAAAAAEBhKPCOl65du+qXX37R77//Lnd3d40aNUqHDx/WL7/8oo4dOxZFRgBAITObTfrokUaKqOGtjOw8Dfhut6LTjE4FAAAAAAAAlHwF3vEiSREREVq5cmVhZwEA3EFODmZN7ttMA2bu0Ppj5/TNYYtaxKaoVQ0/o6MBAAAAAAAAJVaBd7wAAOyHi6NFk/s2U+uQCsq2mjTgu93aFpNidCwAAAAAAACgxCpw8WI2m2WxWP72BwBQsrg4WjShV2PVLmNVRnaeBs7aqcS0TKNjAQAAAAAAACVSgW81tmDBgqte5+TkaPfu3ZoxY4ZGjx5daMEAAHeOi6NFT9eyamp8GR1OuKR/z96tmU83l4sjhToAAAAAAABQEAUuXrp27XrN2COPPKJ69epp7ty5evrppwslGADgznKySJ/1bKiHJmzRttgUDZi5QxP7hMrd+ZYeBwYAAAAAAACUSoX2jJcWLVpo1apVhXU6AIABqnq7a1r/5nJzsmj9sXPqPmGTzl68YnQsAAAAAAAAoMQolOLlypUr+vzzzxUYGFgYpwMAGKh51fKa9UyYvD2cdSThknpN3qoknvkCAAAAAAAA3JQC3z+mXLlyMplM+a9tNpsuXbokNzc3zZo1q1DDAQCM0bRyOS2MbKme32xRzLl0PTltu+Y9H85txwAAAAAAAIB/UOB/Qfv000+vKl7MZrN8fHwUFhamcuXKFWo4AIBxKpVz0w8DWujhCRt1+Gyahs7Zo0lPhMpsNv3zwQAAAAAAAEApVeDipV+/fkUQAwBQHFWu4KbJfZup56Qt+v1woqZujNEzEdWMjgUAAAAAAAAUWzdVvOzbt++mT9iwYcNbDgMAKH6aVC6nN+6vozcWHdSHy46obU0f1fDzNDoWAAAAAAAAUCzdVPHSuHFjmUwm2Wy2G84zmUzKy8srlGAAgOKjT4sq+uNoslYfSdLoXw7pu6ebX3XbSQAAAAAAAAB/uqniJSYmpqhzAACKMZPJpLceqKcN0ee0Ifqcxv9+TEPvqcHzXgAAAAAAAID/cVPFS5UqVYo6BwCgmKtcwU3DOtTQ2GVH9dmqY4pOvqxPH20sJwez0dEAAAAAAACAYuOmipfrOXTokOLj45WdnX3V+IMPPnjboQAAxdOgdiHy9nDWawv2a8m+szp3KUtvPVhPdQK8jI4GAAAAAAAAFAsFLl5OnDihhx56SPv377/quS9/3eufZ7wAgH17tFmQfD2d9fysndoak6L7Pl+vHqGV9Nr9dVXG1dHoeAAAAAAAAIChCnx/mKFDh6pq1apKSkqSm5ubDh48qHXr1qlZs2Zas2ZNEUQEABQ37Wr5asWwtrq/YYBsNunHHaf00NcbFXMu3ehoAAAAAAAAgKEKXLxs3rxZb7/9try9vWU2m2U2m9W6dWuNGTNGQ4YMKYqMAIBiqHIFN33Vq6nmPR+ugDIuOpGcrm5fbdSm6HNGRwMAAAAAAAAMU+DiJS8vT56enpIkb29vnTlzRpJUpUoVHT16tHDTAQCKvWbB5bUospUaB5XVxSs5emLqNn2/Nc7oWAAAAAAAAIAhCly81K9fX3v37pUkhYWFaezYsdq4caPefvttVatWrdADAgCKP18vF815toW6Na6oPKtNry04oGUHzhodCwAAAAAAALjjCly8vP7667JarZKkt99+WzExMYqIiNDSpUv1+eefF3pAAEDJ4OJo0ac9G6tfy2BJ0ks/7dOJ5MvGhgIAAAAAAADuMIebndisWTM988wz6tWrl7y8vCRJISEhOnLkiFJSUlSuXDmZTKYiCwoAKP5MJpNeu7+ODp1J07bYFA2ctUsLIlvKzemm/3MDAAAAAAAAlGg3veOlUaNGeuWVVxQQEKC+fftqzZo1+e+VL1+e0gUAIElytJj1Za8m8vF01tHESxrywx7l5lmNjgUAAAAAAADcETddvEyZMkUJCQn66quvFB8fr3vuuUchISF6//33dfr06aLMCAAoYXy9XPR176ZycjDr98OJeumnvZQvAAAAAAAAKBUK9IwXNzc39evXT2vWrFFUVJQee+wxffPNNwoODtb999+vn3/+uahyAgBKmLuCy+urXk1lMZu0cM8ZRc7eRfkCAAAAAAAAu1eg4uW/Va9eXe+++65iY2P1ww8/aMuWLerRo0dhZgMAlHAd6/ppYp9QOTmYtfxgot5YdEA2m83oWAAAAAAAAECRueXiRZLWrFmjfv36qV+/fsrLy9OAAQMKKxcAwE50rOunr3o1ldkk/bDtpL5cHW10JAAAAAAAAKDIFLh4OXXqlN59912FhITo7rvvVmxsrL7++mudPXtWEydOLIqMAIASrmNdP43uWl+S9MnKKP2046TBiQAAAAAAAICi4XCzE3/88UdNnTpVq1atkq+vr5588kk99dRTCgkJKcp8AAA78USLKjqTekUT1hzXyJ/3y8/LRW1q+hgdCwAAAAAAAChUN73jpU+fPnJ1ddWCBQt08uRJvf/++5QuAIACeblTLXVrXFG5VpsGztqpA6cvGh0JAAAAAAAAKFQ3vePl1KlT8vX1LcosAAA7ZzabNPaRRkq6lKVNx8/ryanbNKlvM4VWKWd0NAAAAAAAAKBQ3PSOF0oXAEBhcHIwa+IToaof6KXz6dl6fPIWLd57xuhYAAAAAAAAQKG46eIFAIDC4uXiqLnPhqtDHT9l51o15Ifd+nj5UeVZbUZHAwAAAAAAAG4LxQsAwBDuzg765olQDYioKkn68o9oDf9xj2w2yhcAAAAAAACUXBQvAADDWMwmvXZ/XX3as5EczCYt2nNGY9n5AgAAAAAAgBKswMXLyZMnderUqfzX27Zt07BhwzRp0qRCDQYAKD0ealJJ73SrL0masOa4Hpm4SQkXMw1OBQAAAAAAABRcgYuXXr166Y8//pAkJSQkqGPHjtq2bZtee+01vf3224UeEABQOjzevLLef6iBPJ0dtDs+VQ9/vVEnki8bHQsAAAAAAAAokAIXLwcOHFDz5s0lST/++KPq16+vTZs26fvvv9f06dMLOx8AoBTpFVZZS4dGqJq3u85czNQTU7bp7MUrRscCAAAAAAAAblqBi5ecnBw5OztLkn7//Xc9+OCDkqTatWvr7NmzhZsOAFDqBJV304/Ph6uqt7tOp15R1y83antsitGxAAAAAAAAgJtS4OKlXr16mjhxotavX6+VK1eqS5cukqQzZ86oQoUKhR4QAFD6eHs4a+ZTzRXi66GkS1nqN3WbTqZkGB0LAAAAAAAA+EcFLl4+/PBDffPNN2rXrp0ef/xxNWrUSJK0ePHi/FuQAQBwu4LKu2nx4FZqVqWc0rPzNPzHPbqclWt0LAAAAAAAAOCGHAp6QLt27XTu3DmlpaWpXLly+ePPPvus3NzcCjUcAKB0c3Ny0LhHG+vez9Zpe+wFdf50nb7s1URNKpf754MBAAAAAAAAAxR4x8uVK1eUlZWVX7rExcVp/PjxOnr0qHx9fQs9IACgdKtcwU3Tn2quSuVcdTr1inp+s0W/7D1jdCwAAAAAAADgugpcvHTt2lUzZ86UJKWmpiosLEyffPKJunXrpgkTJhR6QAAA7gour2XD2qhjXT9l51k1dM5uLdpz2uhYAAAAAAAAwDUKXLzs2rVLERERkqR58+bJz89PcXFxmjlzpj7//PNCDwgAgCR5ODvomz6h6hFaSVabNHTOHn247IhsNpvR0QAAAAAAAIB8BS5eMjIy5OnpKUlasWKFHn74YZnNZrVo0UJxcXGFHhAAgL+YzSZ90L2hnmldVZI0Yc1xfboyyuBUAAAAAAAAwP9X4OIlJCRECxcu1MmTJ7V8+XJ16tRJkpSUlCQvL69CDwgAwH+zmE16/V919d5D9SVJn6+O1pxt8QanAgAAAAAAAP5U4OJl1KhReumllxQcHKzmzZsrPDxc0p+7X5o0aVLoAQEAuJ7eYVU05O4QSdJrCw/o131nDE4EAAAAAAAASA4FPeCRRx5R69atdfbsWTVq1Ch//J577tFDDz1UqOEAALiRFzrW1JmLmZq385QGz96tnXEX9GKnWvJwLvB/3gAAAAAAAIBCcUv/MuXv7y9/f3+dOnVKklSpUiU1b968UIMBAPBPTCaTPni4gcq5OWry+hhN2xirZQcS9GWvJgqtUt7oeAAAAAAAACiFCnyrMavVqrfffltlypRRlSpVVKVKFZUtW1bvvPOOrFZrUWQEAOBvOVjMeu3+upre/y5VqeCmsxcz1fObLVp+MMHoaAAAAAAAACiFCly8vPbaa/ryyy/1wQcfaPfu3dq9e7fef/99ffHFF3rjjTeKIiMAAP+oXS1fLRkSoXvr+yvXatO/f9itzcfPGx0LAAAAAAAApUyBi5cZM2bo22+/1cCBA9WwYUM1bNhQgwYN0uTJkzV9+vQiiAgAwM3xcHbQF483Uce6fsrOtWrAzB3af+qi0bEAAAAAAABQihS4eElJSVHt2rWvGa9du7ZSUlIKJRQAALfKwWLWF483UVjV8rqclase32zSjE2xupKdZ3Q0AAAAAAAAlAIFLl4aNWqkL7/88prxL7/8Uo0aNSqUUAAA3A4XR4u+fbKZImp4KzPHqjcXH1TrD1drRyy/IAAAAAAAAICi5VDQA8aOHav7779fv//+u8LDwyVJmzdv1smTJ7V06dJCDwgAwK3wdHHUjP7NNXNzrL7dEKNTF66oz5StGvNwA3VrHCiTyWR0RAAAAAAAANihAu94adu2raKiovTQQw8pNTVVqampevjhh3X06FFFREQURUYAAG6J2WxSv1ZVtfKFtmpb00eZOVa9MHevnpmxQwkXM42OBwAAAAAAADtU4OJFkipWrKj33ntP8+fP1/z58/Xuu+/KarXq2WefvaUQX331lYKDg+Xi4qKwsDBt27bthvNTU1MVGRmpgIAAOTs7q2bNmlfttgkODpbJZLrmJzIy8pbyAQBKNlenP2899mLHmnK0mLTqSJI6jlurH7eflM1mMzoeAAAAAAAA7MgtFS/Xc/78eU2ZMqXAx82dO1fDhw/Xm2++qV27dqlRo0bq3LmzkpKSrjs/OztbHTt2VGxsrObNm6ejR49q8uTJCgwMzJ+zfft2nT17Nv9n5cqVkqQePXrc2sUBAEo8R4tZ/76nhpYMiVCjoLK6lJWrV+bv0+hfDinPSvkCAAAAAACAwlFoxcutGjdunAYMGKD+/furbt26mjhxotzc3DR16tTrzp86dapSUlK0cOFCtWrVSsHBwWrbtq0aNWqUP8fHx0f+/v75P7/++quqV6+utm3b3qnLAgAUUzX9PPXzwJZ6uXMtmUzS9E2xGjx7lzJz8oyOBgAAAAAAADvgYOSHZ2dna+fOnRo5cmT+mNlsVocOHbR58+brHrN48WKFh4crMjJSixYtko+Pj3r16qURI0bIYrFc9zNmzZql4cOH/+2DlLOyspSVlZX/Oi0tTZKUk5OjnJyc27lEu/PX98H3AtiX0ri2n21dRZXKOOul+fv124EEJV/aoom9m6iMq6PR0YBCVRrXN1BasL4B+8X6BuwX6xsouQqybg0tXs6dO6e8vDz5+fldNe7n56cjR45c95gTJ05o9erV6t27t5YuXaro6GgNGjRIOTk5evPNN6+Zv3DhQqWmpqpfv35/m2PMmDEaPXr0NeMrVqyQm5tbwS6qlPjr9m0A7EtpXNvP1TJpylGzdsSlquPHq3V/Zauaedv0N109UGKVxvUNlBasb8B+sb4B+8X6BkqejIyMm55708XLww8/fMP3U1NTb/pDb4fVapWvr68mTZoki8Wi0NBQnT59Wh999NF1i5cpU6bo3nvvVcWKFf/2nCNHjtTw4cPzX6elpSkoKEidOnWSl5dXkVxHSZWTk6OVK1eqY8eOcnTkt8IBe1Ha13aXhEt6dtZunbmYqVnRFjn4VNbILrVkNtO+oOQr7esbsGesb8B+sb4B+8X6Bkquv+6UdTNuungpU6bMP77ft2/fm/5gSfL29pbFYlFiYuJV44mJifL397/uMQEBAXJ0dLzqtmJ16tRRQkKCsrOz5eTklD8eFxen33//XT///PMNczg7O8vZ2fmacUdHR/4C/Bt8N4B9Kq1ru35Qea16sZ2+WXdc438/pumb43Xw7CV92L2hqvl4GB0PKBSldX0DpQHrG7BfrG/AfrG+gZKnIGv2pouXadOm3VKYG3FyclJoaKhWrVqlbt26SfpzR8uqVas0ePDg6x7TqlUrzZ49W1arVWazWZIUFRWlgICAq0qXvzL7+vrq/vvvL/TsAAD74upk0bAONVXV210jf96v7bEX1OWz9frPvbXVr1VVo+MBAAAAAACghDAbHWD48OGaPHmyZsyYocOHD2vgwIFKT09X//79JUl9+/bVyJEj8+cPHDhQKSkpGjp0qKKiorRkyRK9//77ioyMvOq8VqtV06ZN05NPPikHB0MfZQMAKEG6Ng7UihfaKKKGt7JzrXrrl0P6dGWUrFab0dEAAAAAAABQAhjeSPTs2VPJyckaNWqUEhIS1LhxYy1btkx+fn6SpPj4+PydLZIUFBSk5cuX64UXXlDDhg0VGBiooUOHasSIEVed9/fff1d8fLyeeuqpO3o9AICSr1I5N818qrm+XnNcHy0/qs9WHdOWE+f12WNN5F/Gxeh4AAAAAAAAKMYML14kafDgwX97a7E1a9ZcMxYeHq4tW7bc8JydOnWSzcZvJwMAbo3JZFJk+xCVcXXUe0sOa2tMirp9tVFT+jVTvYo3fu4ZAAAAAAAASi/DbzUGAEBx1qdFFf02NEIhvh5KSMtUj4mbtfJQotGxAAAAAAAAUExRvAAA8A+Cvd01f2BLtQqpoIzsPA2YuUNv/3JIWbl5RkcDAAAAAABAMUPxAgDATSjj6qjp/Zurf6tgSdLUjTF6+OtNSr6UZWwwAAAAAAAAFCsULwAA3CRHi1lvPlBPU55spnJujjp4Jk3Df9wjq5VnigEAAAAAAOBPFC8AABTQPXX89NPz4XJ1tGj9sXMa89th2WyULwAAAAAAAKB4AQDgloT4eurdbvUlSZPXx2jwD7t1MiXD4FQAAAAAAAAwGsULAAC3qHtoJY15uIFMJmnJvrO6+5M1GrXogJLSMo2OBgAAAAAAAINQvAAAcBseb15ZiyJbKaKGt3LybJq5OU5tPvpD83aeMjoaAAAAAAAADEDxAgDAbWpYqay+ezpMPwxooaaVyyozx6qXftqrKRtijI4GAAAAAACAO4ziBQCAQhJevYLmD2ypARFVJUnv/HpIY5YeVmZOnsHJAAAAAAAAcKdQvAAAUIhMJpP+c18dvdSppiTpm3UndM8na7UzLsXgZAAAAAAAALgTKF4AAChkJpNJg++uoS8eb6KKZVx0OvWKen6zhee+AAAAAAAAlAIULwAAFJEHGlXUyuFt9a+GAcq12vTST3s1Yc1xWa02o6MBAAAAAACgiFC8AABQhNydHfTF4030VKs/n/vy4bIjemzSFv1xNEk2GwUMAAAAAACAvaF4AQCgiJlMJr3xrzp676H6cnYwa1tsivpP267O49fp8Nk0o+MBAAAAAACgEFG8AABwB5hMJvUOq6JVL7bVM62rysPZQVGJl9V36jbFnEs3Oh4AAAAAAAAKCcULAAB3UKVybnr9X3W1YUR71fb3VPKlLN372TqN+e0wBQwAAAAAAIAdoHgBAMAAZd2cNPOp5mpetbwyc6z6Zu0Jdfp0rf44mmR0NAAAAAAAANwGihcAAAzi6+Wiuc+20KQnQtW8annl5Nk0cNZOzd0eL6vVZnQ8AAAAAAAA3AKKFwAADGQymdSpnr++fyZM7Wv5KDPHqhHz9+vhCZu0/9RFo+MBAAAAAACggCheAAAoBhwtZk3q20yv319HHs4O2nMyVQ9+tUFvLjqgPHa/AAAAAAAAlBgULwAAFBOOFrOeiaim1S+2VbfGFWWzSTM2x+nNxQdks1G+AAAAAAAAlAQULwAAFDO+Xi4a/1gTfdmriUwmadaWeD09Y4dOp14xOhoAAAAAAAD+AcULAADF1L8aVtR73RrIyWLW6iNJ6jRurX7cftLoWAAAAAAAALgBihcAAIqxXmGVtXRoazWrUk7p2Xl6Zf4+ffVHtKw89wUAAAAAAKBYongBAKCYC/H11I/PhSuyfXVJ0kfLj+qhCZu071SqscEAAAAAAABwDYoXAABKALPZpJc719Y73erLw9lBe0+mqutXGzXkh92KO59udDwAAAAAAAD8H4oXAABKkCdaVNHql9rq4SaBstmkxXvPqNtXG3Xg9EWjowEAAAAAAEAULwAAlDi+ni4a17OxlgxprYaVyuhCRo4en7RFKw8lGh0NAAAAAACg1KN4AQCghKpXsYy+fyZMzauW16WsXA2YuUPjVhxVntVmdDQAAAAAAIBSi+IFAIASzNPFUd8/E6Z+LYMlSZ+vjtY9n6zRkn1nZbNRwAAAAAAAANxpFC8AAJRwjhaz3nqwnj7t2Uhl3RwVez5DkbN3afiPe9n9AgAAAAAAcIdRvAAAYCcealJJG0fcrSH31JCD2aQFu0/ro+VHjY4FAAAAAABQqlC8AABgR9ydHTS8Y0198mgjSdLEtcf16vx9upyVa3AyAAAAAACA0oHiBQAAO9S1caBe6VJLJpM0Z/tJtftojVYcTDA6FgAAAAAAgN2jeAEAwE4Nahei758JU1Vvd527nKXnZu3UZ78f07nLWUZHAwAAAAAAsFsULwAA2LGW1b21fFgb9QqrLJtN+vT3KLX8YLXeW3JIJ1MyjI4HAAAAAABgdyheAACwc04OZr3Xrb7GPtJQDSuVUXauVZPXxyhi7B968ce9upSZY3REAAAAAAAAu+FgdAAAAFD0TCaTHm0WpB6hlbQmKllT1sdo4/Fzmr/rlFYdSVTPZkHq06KKgsq7GR0VAAAAAACgRKN4AQCgFDGZTGpfy1fta/lqR2yKXvppr2LPZ+ibdSc0af0Jta/lq3/fHaImlcsZHRUAAAAAAKBE4lZjAACUUs2Cy2vVi+00uW8zRdTwls0mrT6SpIe+3qQXf9yrpLRMoyMCAAAAAACUOBQvAACUYhazSR3r+um7p8O0+sW2eiS0kiRp/q5T6jBurbaeOG9wQgAAAAAAgJKF4gUAAEiSqvl46OMejbQwspXqB3opLTNXT0zdpjnb4mWz2YyOBwAAAAAAUCJQvAAAgKs0Diqrec+3VKe6fsrOterVn/dr2Nw9upyVa3Q0AAAAAACAYo/iBQAAXMPF0aKJfUI1okttWcwmLdpzRg9+uUHRSZeNjgYAAAAAAFCsUbwAAIDrMptNGtiuun58roUCyrjoRHK6Hvp6o/aeTDU6GgAAAAAAQLFF8QIAAG4otEp5LR7cWqFVyulSZq56Td6iF+bu0aI9p5WRze3HAAAAAAAA/hvFCwAA+Ec+ns6a+VRzhVUtr/TsPC3YfVpD5+zRfZ+tV3TSJaPjAQAAAAAAFBsORgcAAAAlg7uzg2YPaKHtsSlaczRZC3efVuz5DHX6dJ1aVvdWaJVyeiK8irw9nI2OCgAAAAAAYBh2vAAAgJtmMZvUoloFvXpvbf06pLUianjLapM2RJ/TZ6uO6b7P1mv+zlO6lJljdFQAAAAAAABDsOMFAADcEm8PZ333dJhOJF/Whuhzmrk5TtFJl/XiT3tVYamTxj7SUPfU8TM6JgAAAAAAwB3FjhcAAHBbqvl4qG94sBYPbqVhHWqoSgU3nU/P1tMzduj1hft1JTvP6IgAAAAAAAB3DMULAAAoFG5ODhrWoaaWD2ujp1tXlSTN2hKvf32xXuuikmW12gxOCAAAAAAAUPQoXgAAQKFycbTojX/V1XdPN5evp7OOJ6er79RtuvuTNZq2MUZ5FDAAAAAAAMCOUbwAAIAiEVHDR8uHtVG/lsHydHZQ7PkMjf7lkPpP365d8Rdks1HAAAAAAAAA+0PxAgAAikw5dye99WA9bfnPPRr9YD25OJq1LipZD3+9Sd2+3qSdcSlGRwQAAAAAAChUFC8AAKDIuTs76MmWwVoY2UoPNqooV0eL9p5MVY+Jm/XR8iPKzMkzOiIAAAAAAEChoHgBAAB3TG1/L33+eBOte6W9Hm4aKKtN+uqP4+owbq0mrj2uC+nZRkcEAAAAAAC4LRQvAADgjvPxdNa4RxtrQu+m8vdy0akLV/TBb0fU+sPV+nL1MeXmWY2OCAAAAAAAcEscjA4AAABKr3sbBKhtLR8t3nNGMzfH6dDZNH28Ikq/7jurx5tXVp0ALzWtXFYOFn5XBAAAAAAAlAwULwAAwFBuTg56rHll9bwrSAv3nNaoRQd1JOGS3lx8UJJUJ8BLH3ZvoIaVyhobFAAAAAAA4Cbw66MAAKBYMJlMeqhJJa15qZ1G3ltbETW85enioMNn0/TYpC3642iSrFab0TEBAAAAAABuiOIFAAAUKxU8nPVc2+r67ukwrX25vSJqeCsjO0/9p21X2JhVWhuVbHREAAAAAACAv0XxAgAAiq3y7k6a3LeZujetJDcni5IvZan/tG0a/ctBpWZkGx0PAAAAAADgGhQvAACgWHNxtOiTRxtp96iOerx5kKw2adrGWLX9aI0+Xn5Ua44m6UI6JQwAAAAAACgeDC9evvrqKwUHB8vFxUVhYWHatm3bDeenpqYqMjJSAQEBcnZ2Vs2aNbV06dKr5pw+fVp9+vRRhQoV5OrqqgYNGmjHjh1FeRkAAKCIOTtYNObhhvru6eaq7e+pi1dy9OUf0eo3bbtC312pUYsO6FJmjtExAQAAAABAKedg5IfPnTtXw4cP18SJExUWFqbx48erc+fOOnr0qHx9fa+Zn52drY4dO8rX11fz5s1TYGCg4uLiVLZs2fw5Fy5cUKtWrdS+fXv99ttv8vHx0bFjx1SuXLk7eGUAAKCoRNTw0ZIh3lq057TWHzunvSdTdeJcumZujtOv+87qhQ411Dusisxmk9FRAQAAAABAKWRo8TJu3DgNGDBA/fv3lyRNnDhRS5Ys0dSpU/Xqq69eM3/q1KlKSUnRpk2b5OjoKEkKDg6+as6HH36ooKAgTZs2LX+satWqRXcRAADgjrOYTXq4aSU93LSSJGnDsXN6Y9EBxZxL1xuLDmrZwQQNiKimtjV9ZDJRwAAAAAAAgDvHsOIlOztbO3fu1MiRI/PHzGazOnTooM2bN1/3mMWLFys8PFyRkZFatGiRfHx81KtXL40YMUIWiyV/TufOndWjRw+tXbtWgYGBGjRokAYMGPC3WbKyspSVlZX/Oi0tTZKUk5OjnBxuWfLf/vo++F4A+8LaRkkXFlxGSweHa/b2Uxq7PEobo89rY/R5NQz00lsP1FGDwDJGRzQM6xuwX6xvwH6xvgH7xfoGSq6CrFuTzWazFWGWv3XmzBkFBgZq06ZNCg8Pzx9/5ZVXtHbtWm3duvWaY2rXrq3Y2Fj17t1bgwYNUnR0tAYNGqQhQ4bozTfflCS5uLhIkoYPH64ePXpo+/btGjp0qCZOnKgnn3zyulneeustjR49+prx2bNny83NrTAuFwAA3CFJV6T1CWZtTTIpy2qSxWTTo9WsauFryP/kAQAAAAAAdiAjI0O9evXSxYsX5eXldcO5Jap4qVmzpjIzMxUTE5O/w2XcuHH66KOPdPbsWUmSk5OTmjVrpk2bNuUfN2TIEG3fvv1vd9Jcb8dLUFCQzp07949fYGmTk5OjlStXqmPHjvm3ewNQ8rG2YY/OXc7S64sOadWRZElSj9BAPd+mqiqXL12/VMH6BuwX6xuwX6xvwH6xvoGSKy0tTd7e3jdVvBh2qzFvb29ZLBYlJiZeNZ6YmCh/f//rHhMQECBHR8f80kWS6tSpo4SEBGVnZ8vJyUkBAQGqW7fuVcfVqVNH8+fP/9sszs7OcnZ2vmbc0dGRvwD/Bt8NYJ9Y27AnAeUcNbnvXfryj2h9+nuUftp5Wj/tPK2IGt7q1byyOtT1k6PFbHTMO4b1Ddgv1jdgv1jfgP1ifQMlT0HWrGH/2uDk5KTQ0FCtWrUqf8xqtWrVqlVX7YD5b61atVJ0dLSsVmv+WFRUlAICAuTk5JQ/5+jRo1cdFxUVpSpVqhTBVQAAgOLMbDZpyD019P3TYYqo4S1JWn/snAZ+v0stP1itz34/puxc6z+cBQAAAAAA4OYZ+muew4cP1+TJkzVjxgwdPnxYAwcOVHp6uvr37y9J6tu3r0aOHJk/f+DAgUpJSdHQoUMVFRWlJUuW6P3331dkZGT+nBdeeEFbtmzR+++/r+joaM2ePVuTJk26ag4AAChdWoZ467unw7Tu5fYa1K66vD2clXwpS5/+HqWHJ2zUH0eTZLXyDBgAAAAAAHD7DLvVmCT17NlTycnJGjVqlBISEtS4cWMtW7ZMfn5+kqT4+HiZzf+/GwoKCtLy5cv1wgsvqGHDhgoMDNTQoUM1YsSI/Dl33XWXFixYoJEjR+rtt99W1apVNX78ePXu3fuOXx8AACheKldw0ytdamtYh5pauv+s3vrloA6cTlP/adtV3cddPZoFqVNdP1Xz8TA6KgAAAAAAKKEMLV4kafDgwRo8ePB131uzZs01Y+Hh4dqyZcsNz/mvf/1L//rXvwojHgAAsENODmZ1axKo8OoVNGndCc3dflLHk9P1wW9HNHbZEXVvWkkPNQlUi2oVZDabjI4LAAAAAABKkNLzRFkAAID/4eflojf+VVebR96td7rVV0QNb1lt0k87T6nXt1vVZ8pWnU69YnRMAAAAAABQglC8AACAUs/TxVFPtKii754O07znw/VIaCW5Olq06fh5tR37h4bP3aNDZ9KMjgkAAAAAAEoAihcAAID/0iy4vD7u0Ui/DmmtFtXKK9dq08+7T+u+z9frlXl7df5yltERAQAAAABAMUbxAgAAcB3VfTw059lwLYpspfsbBEiSftxxSu0/XqMJa47rYkaOwQkBAAAAAEBxRPECAABwA42Cyuqr3k01f2C46gZ4KS0zVx8uO6LWY1frm7XHlZNnNToiAAAAAAAoRiheAAAAbkJolfL65d+t9dEjDVXTz0OXMnM15rcjeujrjZqzLV6Hz6ZRwgAAAAAAADkYHQAAAKCksJhN6tEsSA83raT5u07pvSWHdeB0ml79eb8kqVI5V735QD21q+UjRwu/3wIAAAAAQGlE8QIAAFBAFrNJjzYLUrtaPvph60ltjD6nQ2fTdOrCFQ2YuUPuThaF+HmqXU0fPdCooqr7uMtkMhkdGwAAAAAA3AEULwAAALfI19NFQzvU0NAONZSelavPVh3TvJ2nlJKerb0nU7X3ZKo+W3VMgWVd1b62j55oEaxa/p5GxwYAAAAAAEWI4gUAAKAQuDs76D/31dGILrV1Ivmy9p26qMV7z2jz8fM6nXpFs7bEa9aWeFVwd1JgOVfV8PVUy+oVVNXHXY0rlZXZzI4YAAAAAADsAcULAABAIbKYTarh56kafp7qHlpJGdm52nLivObtPKXfDiTofHq2zqdna9+pi5q/65QkKbxaBX3Uo6EqlXMzOD0AAAAAALhdFC8AAABFyM3JQXfX9tPdtf2UlpmjkykZOnXhinbFX9Du+FTtP3VRm0+cV+sP/1DTymX1QfeGquHrwTNhAAAAAAAooSheAAAA7hAvF0fVq1hG9SqWUed6/pKk6KRL+s+CA9oRm6Jd8anq9Ok6OTuY1fOuIA1qFyL/Mi4GpwYAAAAAAAVB8QIAAGCgEF9P/fhcuBIuZurVn/dpzdFkZeVaNXNznL7bEqe7qpRXx7p+qhPgpZp+HvLxdGY3DAAAAAAAxRjFCwAAQDHgX8ZF0/s3V3pWrnbHp+qzVVHaHntB22JTtC02JX+eh7ODmlYpp8fvClKHun5ytJgNTA0AAAAAAP4XxQsAAEAx4u7soNY1vNW6hrfOpF7R0v1ntS0mRceSLivufLouZ+VqXVSy1kUly9vDWUPvCVGvsCqymNkFAwAAAABAcUDxAgAAUExVLOuqZyKq6ZmIapKkzJw8nUhO16/7zuinnaeUfClLbyw6qKkbYzXknhB1axzIbcgAAAAAADAYxQsAAEAJ4eJoUd2KXqpb0UsvdKypH7bFa9zKKMWcS9cLc/dqwprjqlzeTR3q+KlZcHlV9XY3OjIAAAAAAKUOxQsAAEAJ5Ggxq294sLo3raTpm2L1xepjikq8rKjEy/r9cJIkyc/LWT2aBso7y+CwAAAAAACUIhQvAAAAJZi7s4Mi24eoe9NK2nPygo4np2vFoUQdS7ykxLQsfbnmhEyyaEnKdjWpXE7l3JxUy99D7Wv5clsyAAAAAACKAMULAACAHfAv46IuZQIkSZHtQ5SVm6cVBxP13eZYbYu9oO3/9/OXViEV9EDDirq3foDKuDkaFRsAAAAAALtD8QIAAGCHnB0seqBRRXWp66NZC5bKHNhAsSlXlJKerd8OJGhj9HltjD6v0b8cUvvaPgqtUl51A7zk6eKgugFeMpvZDQMAAAAAwK2geAEAALBz5Z2l+5oHydHxz50tL5xL18+7TmnFoUQdSbikpfsTtHR/Qv782v6eerFTLXWow+3IAAAAAAAoKIoXAACAUibY213DO9XSCx1ralf8BW05kaIdsSmKPZ+hxLRMHUm4pAEzdyisanl9/ngT+Xm5GB0ZAAAAAIASg+IFAACglDKZTAqtUl6hVcrnj6VmZGvSuhOatjFWW2NSFPHhH/L1ctbzbaurd1hldsAAAAAAAPAPzEYHAAAAQPFR1s1Jr3Sprd+GRqi2v6ey86w6deGKXl94QN2+3qRlB84qO9dqdEwAAAAAAIotdrwAAADgGsHe7lo6JELxKRn6/XCiPl5xVHtPpur5WbtkMZvk4eygMq6O6lLfXw83DVRgWVd5ujgaHRsAAAAAAMNRvAAAAOC6zGaTgr3d9UxENXVtHKhpG2M0f9cpJaZl6eKVHF28kqNJ605o0roTcrSY1L6WrxoFlVWLahW0NipZ0UmXVKWCu56NqKZy7k5GXw4AAAAAAHcExQsAAAD+kY+ns17pUlsvdaql5MtZupSZq+iky5q8/oSOJ19WakaOVhxK1IpDidccu3D3aXWu56+2tXzUrqYPz4kBAAAAANg1ihcAAADcNLPZJD8vF/l5SSG+HupS31+SdOhMmv44mqS9J1O1NipZlcu76ZHQSpq7/aROnEvX9E2xmr4pVuXdneTp4qBHmlZS48pl1SCwjMq6sRsGAAAAAGA/KF4AAABw2+pW9FLdil6SJJvNlr+rpXeLKlq854wOnrmoeTtPKSU9Wynp2fpkZZQkyWI2qXFQWYVVLa/n2lZXGVeeEwMAAAAAKNkoXgAAAFCo/vtWYh7ODuoVVlmS9FKnWjqdekVRiZe0eO8ZxZ3PUMy5dO2Mu6CdcRe0YPdpvdy5lmr5eyqwrCs7YQAAAAAAJRLFCwAAAO6Icu5OKufupPqBZfRw00qSpPjzGdoac15f/hGtuPMZGv7jXkmSi6NZPZsFKbx6BbUK8ZanCzthAAAAAAAlA8ULAAAADFO5gpsqV3DT/Q0DNG1jrGZvjVdGdq4uZORoxuY4zdgcJyeLWW1qeqt1iLcaVCqrJkFlZTab/vnkAAAAAAAYgOIFAAAAhnNzclBk+xBFtg+RzWbTmqhkrTiYqK0nzuvEuXT9fjhJvx9OkiT5e7mopr+n2tb0Ud0AL1Uq56qg8m4GXwEAAAAAAH+ieAEAAECxYjKZ1L6Wr9rX8pXNZlNU4mUtO5Cg/adTteVEihLSMpWQlql1UcmSJIvZpFe71Fa3JoEq5+YoB4vZ4CsAAAAAAJRmFC8AAAAotkwmk2r5e6qWv6ckKTMnT7viL+jI2Utasv+ski5l6mTKFb239LDeW3pYkhRQxkWtQrz1Spda8vV0MTI+AAAAAKAUongBAABAieHiaFHL6t5qWd1bT7WuKpvNphmbYjVtU6ziUzJks0lnL2Zq3s5T+m3/WTUKKisXR4uq+7grooaPmlctLxdHi9GXAQAAAACwYxQvAAAAKLFMJpP6taqqfq2qKjvXqrTMHB1NuKQPfjui/acvatPx85Kk1Uekyetj5ORgVljV8nq+bXW1CvE2OD0AAAAAwB5RvAAAAMAuODmY5e3hLO8QZy2KbKWDZ9J0NPGSsnOt2nPygjYcO6czFzO1/tg5rT92ThE1vNWvZbDuru0rk8lkdHwAAAAAgJ2geAEAAIDdMZtNalCpjBpUKiNJ6hVWWTabTceT0zVrS5xmbo7NL2Bq+nmoUjk32Ww25Vptqh9YRs+3ra4yro4GXwUAAAAAoCSieAEAAECpYDKZFOLrobcerKenWlXVrK1/FjBRiZcVlXg5f976Y+f04/aTerFTLXVrUlFuTvxPZgAAAADAzeP/RQIAAKDUqVzBTf+5r44GRFTTzrgUpWbkyGw2KSfPqmkbYxWddFn/WbBfb/1yUN2bVtJzbaqpSgU3bkkGAAAAAPhHFC8AAAAotXw8ndWlfsBVY482C9J3m+M0dWOMTl24oh+2xeuHbfHy8XTWPbV9FVqlnDrW9VNZNyeDUgMAAAAAijOKFwAAAOC/OFrMeqp1VfVvFaxtMSkatzJKu+IvKPlSluZsP6k520/KzcmiTnX9FBpcXqGVy6mWv6csZnbDAAAAAAAoXgAAAIDrMplMCqtWQXOfC1dWbp62nkjR+mPJWn/snI4kXNLCPWe0cM8ZSZKTg1lWq03l3J0UWrmceoVVVoPAMirn/v93xWRk5+rQmTQ5O1gkSe7OFgWWc81/DQAAAACwDxQvAAAAwD9wdrCoTU0ftanpI5vNpq0xKdp8/Lx2xV/Q7vhUXc7KlSQlX8rSsoMJWnYwQZIUXMFNfl4uupSZq+jky8rOtV51Xk9nBw2+O0R31/ZVWmauKpZ1UUAZ1zt+fQAAAACAwkPxAgAAABSAyWRSi2oV1KJaBUlSntWmUxcy5Ggx6+zFTM3fdUp/HEnS2YuZij2fodjzGfnH+nk5/3kOmZSWmaNLWbka89sRjfntiCTJbJLurR+gR5pVUruaPjKZuH0ZAAAAAJQ0FC8AAADAbbCYTapSwV2SVLGsq0KrlJMkpWXmaE98qtIyc+TmZFGVCu6q5u2eX6ZYrTbN23VKs7fGKzrpstydLUpMy9KS/We1ZP9Ztanpo7HdG8q/jIth1wYAAAAAKDiKFwAAAKAIeLk4qk1Nn79932w26dFmQXq0WVD+2KEzafpxx0n9sC1e66KS1Xn8OvUOq6xqPh5qHeJNCQMAAAAAJQDFCwAAAFBM1K3opbcerKc+LSpr+I97te/URX295nj++zV8PRTs7a76FcsoxNdDktShrq+cHSxGRQYAAAAA/A+KFwAAAKCYCfH11PyBLTV3+0kdSUjTgdNp2ncqVceSLutY0mWtPJSYP7e6j7t6NAuSv5eLgsq7qWnlsjwbBgAAAAAMRPECAMD/a+/O4+uq6v3/v/aZ58zz0KTzXEpLSy0FhTIU5ApyVaAXK+rlp4BWuSoOV4GHV1H5XfQnIgj3OnyvA174yiiDpQxlaEspdG7TtOmQZp7PlDPv3x+BAyHpgKZNG97PxyOPR87ea++z1mk+aXLeWWuJiJyE7FYL/3LmuOzj3miC1/f30NTbz0v1nXRF4jR2R9nbEeFHT+3KtivyOynPcZHjcdDa14/LbuWMmnxqCr2My/dQne+hqbefNbs72HKoj1QmQ8aEmeUBrlpYTb7XQbFfS5qJiIiIiIj8vRS8iIiIiIicAnI9DpZOLwFgxYdqAOiLJnlwYyNvHOyhN5pkc2MvHaE4HaH4oGu3HOo76v03Hujhd2sPAPDR2WV8ZekkJhb7R3YQIiIiIiIiHwAKXkRERERETlE5HjufXzI++7g/kWZHS5DuSIKeSIKigJPeaIIth/po7I5yoCvKwe4ouR47SyYVsbA2H7/LRjyV4YHXGtlyqJdoMs0TW1p4YksLM8oDzCgP4HHYcNmtOGwWGjrCdIbjROJpmnv7WTyxkCvmVbKwNh+XfWCvmXA8RSKVYfOhXpp6+inLcXGgK0oknqK2yMvSaSXZtiIiIiIiImONghcRERERkTHC7bAyb1zekOOXz63Mfm6a5rB7wHzstAoAdjQH+emzu1m9s43tzUG2NweP+JyPbW7msc3NGAbUFnipLvDwUn0n6Yx52GsMA/I9Dor8zmxg47RZuGZRDUV+57EOV0RERERE5KSk4EVERERE5ANkuNDl3aaXB7j/0/PpCMV5dW8njd1RYskM/ck0sWSa8lw31fkeHDYLAZedx7c0s3pnG23BOA2dERo6I9l7FfqczCgP0BaMMa7AQ8Bl59W9XTT19tMVSdAVSbCrNZRtf99LDZwzuYhPzKuiptBDKmOS73WAObB3zdH6LiIiIiIicjJQ8CIiIiIiIkMU+Z3ZWTBHsmhCAeZlM+kIx9l0sJf69jAfmVLMpBIfNosxJCwxTZOOcJyucIKD3VFeqGvHMAy2NwfZ3NjLM9vbeGZ725DnmVTs48ZzJx5Tn0REREREREaTghcREREREfmHGIZBsd/FBTNKuWDGsbUt9ruYVhbgwhmlwEAgs60pyBNbmvnf1xtJpU1sVoOeaBLDgPr2MCsf2MQjbzYxpyqXT86vojzXfdzH1h1J4LJb8Dj0q5OIiIiIiBwb/fYgIiIiIiKjzjAMZlXmMKsyh28um5o9ZpomoXiK/1rTwF3P7+H5ug6er+vgF8/t4fTqPBw2C23BGIYBS6eV8Pkl48nz2OkIx+mJJAm4beS6HbjslmGXKkumM2xt6uPVPZ1sPNBDRzhOMmXid9mIJNLsbBnY42ZmRYDLTqsgz+NgZ0sQu83CvOo8ppUHaO3rp6UvRnckwZnjC5hc4icUS9IeijO+0Ksl0kREREREPmAUvIiIiIiIyEnl3UGFYRgEXHZuumAK508vZW1DJ8/v6mBtQxev7e8edN3utjAPbTyE3Wqhqbd/0DmH1UJZrouJRT58LhtvHOwhmTLpDMdJZcyj9mlbU5BtTcFj6DvUFng52B0llTE5oyaPr54/mUXjCxTAiIiIiIh8QCh4ERERERGRU8LbM2KuO3sCB7oirG/oxmoxKM1x0R1J8LNnd7O3IwKAxYAct51QLEUqY5JIZzjQFeVAV3TIfXM9ds6sLeDM8flUF3hwWK309SeJJdOcO7WYtGnyyJtNrN3bRV9/kpkVOcRTGVbtaKU3mqQk4KI814XNYmFtQxcNnQN9sFoMNuzv4er71+N1WJleHuDKM6pZOD6filw34XiK9lCcdMakrz9JbzSJzWIwsdhHVb7nhL62IiIiIiIychS8iIiIiIjIKWdcgZdxBd5Bx86bVswDrzVSmedmyaQi3A4rpmkSTaTpiSZo7O5nT3uI7kiS08flkuO2U+BzUp7jOupslM8vGc/nl4wfdOyHl8/ENMFieefaPe0h2kNxqvI8WC0Gv3h+D49taiYcT7Fhfw8b9vcAEHDZCMdTDDfZxjDgohmlLJ1WQm2Rl55IgnUNXTT3xZheFsBps5AxTfLdNvpiA/vjvB/9iTQWCzht1uyxUCyJy27FbrW8r3uJiIiIiMhQCl5ERERERGRM8DhsfPas2kHHDMPA67ThddqozPOwaELBiD2fYRi8N6+ZWOxnYrE/+/iHl8/i1ktncLA7yl+3tPDUthb2tIcJxlIA+F02bBaDHLedHLedeCrDrtYQT21r5altrUOe869bWt5zxMbPdj6P1WKweEIhSyYVku918OEpxdgsBl2RBBsPdFPXGmZ3e4j1Dd10huMAeB1WivxOCnxO3jjYQ4HXwafOqGLZzDJmlAe0NJqIiIiIyN9JwYuIiIiIiMhx5LBZmFjsY+XSSaxcOol4Kk1DR4Q8j4PSHNeQ9jtbgjyyqYmN+3to6u0nx21nRnkO44u81LeFMAGLYbCvM8zWQ72E3gpx/rq1hb9uHQhmXHYLiVRm2Bk1b4sk0kS6oux/a/m1znCCu5/fy93P76Uyz80n5lVRU+ghEk9jtQyESsV+JxW57kGzfEREREREZLCTIni5++67ueOOO2htbWXOnDncddddLFiw4LDte3t7+c53vsNf/vIXuru7GTduHD/72c+4+OKLAbj11lu57bbbBl0zZcoUdu3adVzHISIiIiIicjROm5VpZYHDnp9WFjji+bclk0kefeJJpp9xNv1p+L9vHKKtL8a25j7agvFsu/FFXuaPy6Mqz8OZEwqYXDIwI6cnkqC5t5/Gnijza/LZ2RLk8c3NvLi7g0M9/fz02d3DPm+ex47XOTBT5+OnV7JkUiHTywP0RZNgQLF/aJgkIiIiIvJBMurBy5///Gduuukm7r33XhYuXMjPfvYzLrzwQurq6iguLh7SPpFIcP7551NcXMxDDz1ERUUFBw4cIDc3d1C7GTNm8Oyzz2Yf22yjPlQREREREZERZbfApBIfdrudeePyAEimMxzoihJw2cjzOg67b0uO205N4Tv75Ewo8vHR2eX0J9I8s72VRzY1EU9m8LlsxJJp9rSH6Qon6Ikm6YkmAbhz1W7uXLUbu9UgmTYxDPiXheP47Fm1FPmd/G17Ky19MWoKvHidVvr6k1Tne5haGiBtmvic+j1NRERERMaeUf8p98477+Rf//VfufbaawG49957+etf/8qvf/1rvvnNbw5p/+tf/5ru7m5effVV7HY7ADU1NUPa2Ww2SktLj2vfRURERERETjZ268DSZn8vt8PKZXMruGxuxZBzyXSGrU19JFMZmnr7eWJLC5sae+mOJDAMME34n3UH+J91B476PIYBsytz+fSZ47hkdhkuuxWAeCrNuoZuGjrClAZcLJ1ekg2PUukMFsPQUmciIiIiclIb1eAlkUiwceNGvvWtb2WPWSwWli5dytq1a4e95rHHHmPRokXccMMNPProoxQVFXH11Vdz8803Y7Vas+3q6+spLy/H5XKxaNEibr/9dqqrq4e9ZzweJx5/Zyp+MBgEBqbuJ5PJkRjqmPH266HXRWRsUW2LjF2qb5Gxa7Tqe1bZQKhzelWAS2eVYJomzX0xPA4ru1pD3P/Sfl7Z20XGhMpcF6dV5dLSFyOSSONzWtnZEiKSSGOasLmxl39r7OXfHtyM12HFYbMQS6bpT2ayz+dz2ijLcWI1DPZ2Rgi47JxRk4fdarBi0TjmVOac0PGLnAj6/1tk7FJ9i5y63k/dGqZpHmG7xeOrubmZiooKXn31VRYtWpQ9/o1vfIMXX3yR9evXD7lm6tSp7N+/n+XLl3P99dezZ88err/+er785S9zyy23APDUU08RDoeZMmUKLS0t3HbbbTQ1NbFt2zb8fv+Qew63JwzAH//4RzwezwiOWEREREREZOxLZgY+3NaBmS3vlspA4q3zr3UYvNBsIZwa3CjHbjLOb7I/ZBBMHn52i4HJOWUm4/0m3XFwWWF2vonXfjxGJSIiIiIfZNFolKuvvpq+vj4CgSPvyXjKBS+TJ08mFouxb9++7AyXO++8kzvuuIOWlpZhn6e3t5dx48Zx55138rnPfW7I+eFmvFRVVdHZ2XnUF/CDJplMsmrVKs4///zsUm8icupTbYuMXapvkbFrrNS3aZr0RJOE4ikSqQwGML7Qi8VikExn2N8ZpT0cJ5XOUFPgpaEzwv6uKFsO9fHE1tYh93PYLEwt9TGhyMf86lwun1t+2H1uRE5WY6W+RWQo1bfIqSsYDFJYWHhMwcuoLjVWWFiI1Wqlra1t0PG2trbD7s9SVlaG3W4ftKzYtGnTaG1tJZFI4HA4hlyTm5vL5MmT2bNnz7D3dDqdOJ3OIcftdru+AR6GXhuRsUm1LTJ2qb5Fxq6xUN8lDgclwxy322F6pZPp7zo2sfSdpcWuqGvnJ0/XkUxnmF4WYG9HmO3NQbYcGvh4+M1m7lmzjxnlAeZW5/GhCQVMLwuwtamPv7zRhM1qMKHIR0WeG6fVgsdpY1dLkGd3tpNIZ5hVEeDiWWVMLwtgvHfqjsgJMBbqW0SGp/oWOfW8n5od1eDF4XAwb948Vq9ezWWXXQZAJpNh9erV3HjjjcNes3jxYv74xz+SyWSwWAb+amn37t2UlZUNG7oAhMNh9u7dyzXXXHNcxiEiIiIiIiIn3kemFPORKcXZx6Zp0tAZYXdriJ0tQX6//iCHevo51NPPM9sH/uDPajFIZ45t4Yc1uzu4+/m9TCjysmRSEedMHviwWBTCiIiIiMjhjWrwAnDTTTexYsUK5s+fz4IFC/jZz35GJBLh2muvBeDTn/40FRUV3H777QB88Ytf5Be/+AUrV67kS1/6EvX19fzwhz/ky1/+cvaeX/va17j00ksZN24czc3N3HLLLVitVq666qpRGaOIiIiIiIgcf4YxMINlQpGPZbPK+Nezx7OuoZsDXRHWNXSzvqGLUDwFwGWnlVMccLGzJUh3JEE8lSEUSzKuwMviCYUU+Z28VN/B6l3t7O2IsLcjwm9f3Y/bbsVptzCjPMBFM8tYND6fIr8Lu9Xg9f09VOS5OdTTTyZjsmhCAS679Si9Prmk0hn6+pM4bBb8rnf+qrMrHKe5N4bbYWV3Wwif00aex0FjT5SucJw5VbnMqsjRzCARERERToLg5VOf+hQdHR1873vfo7W1ldNOO42nn36akpKBieYHDx7MzmwBqKqq4plnnuGrX/0qs2fPpqKigpUrV3LzzTdn2xw6dIirrrqKrq4uioqKOOuss1i3bh1FRUUnfHwiIiIiIiIyOvwuO+dPH/jd8vNLxpPOmHSF4xiGQZF/6HLT73X1wmpCsSTP13Xw2r4uHt3UTCiWoj+Z5pU9XbyypwsAwwC33Uo0kR50vcUAm9VCoddBSY6L0oCLkoCLHLedlr5+agt91BZ66E+mKfA6KfQ5KfQ7KPI5j2uAkcmYNHSGOdgdpSMUp6Ejwqt7uwjFkjT3xUikMgAUeB0snlhIfXuYnS3Bo953QpGXaWUB2oIxSgIuzp5chMcxEDyV57qpzHMf97GJiIiInAxGPXgBuPHGGw+7tNgLL7ww5NiiRYtYt27dYe/3wAMPjFTXREREREREZIywWgyKA673dY3fZeef5pTzT3PK+fdLptPc2080kWZdQxd/eaOJpt5++vqTRBNpivxOeqMJ8r0ODAxagwMhRnNfjOa+2DE/57gCD1cvqCbHbac0x4XNYiGZyVDsdzKjPIdYMo3TZqEjHKcrnMDntOF32cj1DF5+O50x2XKol50tIQDKc13sbgtx35oGOsOJo/ajK5Lgsc3NwEC4VOB1EomnGF/kJRJPEUmkqcxz43fZ2bCvOzsz6G1PbGkZck+Pw8ppVbksnljIrIoc+pNp6lpD1LWGaOrtpysSJ5HKsKC2gFkVAQp9TmoLvZxWlXvYwKa1L8b/+7c69nVGCLhsFPicLJlUSG2hl7ZgnGB/kppCL7Mrc7BbLcPeQ0RERGQknRTBi4iIiIiIiMjJzmW3Mr7IB8DMihw+v2Q8AO3BGG3BODPKA5gMzHQxTegIx0mmM3SE4rQFY7T2xWgNxumNJij2O9l8qI9gLInHYaUrnKAzHKcrkuBAV5Tbn9o1bB8qct009fbjsFmyM1PeVhJwkjHBZbcwscjHlkN9dEWGD1hcdgsTinwU+52U5rg4c3wB5bluiv1OqvIGZuHsaAny7M42Cr1OPjG/ckiw826hWJLHNjfTG01Sle+hoSPMc7vasVstGEBTbz9twRjRRJpX93bx6t6uI77Wj29u5vG3Qh+AudW5XDKrjOKAC5fNgstupSMU5287Wnlxdwex5ODX4qGNh4bcM8dtZ3ZlDi19MdqDMaoLPJw9qYgLZpQyuyJHe/eIiIjIiFHwIiIiIiIiIvIPKA64hsykMQwoeetYZZ7nmO8VTaT4w7qDvHGwh1gyTUtfDNMcmK2zpz1MU28/AIlUBsOAfI+DSCJFLJmhLRjP3qexe6BdwGXj9HF5ALQF47jsFj41v4or5lUecfaH12njjJp8zqjJP6Z++112li8cN+jYV5ZOHvQ4kcqwvyvC2r1dvFTfycHuCC67lYlFPqaVBagu8FDoc5LOmKzd20V9e4ieaIIN+3t482Avbx7sPezzz63O5TMfqiGWTLO/K8rzu9rp60+S63GQ57GzqzVEdyTBS/Wd2Wu2NQXZ1hTkly/spTTg4qOzy/h/zplALJlmb0eYSDxN2jQJ9idpD8Y4Z0oR88Yd2+shIiIiH2wKXkREREREREROEh6HjX89e/yw59qDMba3BJlWGiCRypDrtRNw2YGBwGZ7cxCnzUJnOE5jdz/TygLMrc49aZbXctgsTC7xM7nEz4oP1Ryx7YLadwKO9mCMxzY3s66hm3A8STyVIZbM4LAanDN5YMbKjPLAoKXIbr5o6qD7vb3sWn1bmIDbRm2hj12tQf62vY0X6tppDcb4r5f38V8v7ztsn37+3B6mlQW4dE4Z/zyvkmL/kZetawvGeHxzM5sae8n12Cnxu3DaLbgdNmZV5DCnMkf73YiIiIxRCl5ERERERERETgHDzax5m8dhO+bZKaea4oCLzy8Zn13a7e9htRjMrc5jbnVe9tiUUj8fO62CWDLNS/Wd/HTVbna0BHFYLdQUesj1OLAaBh6HFZfDyqrtbexsCbKzJcidf9vN4omFxJJpxhf58DmtHOiKkuuxk+O209Tbz6odbSTT5mH7NLHYR08kwfTyAP/flXPJ9x5+KTcRERE5tSh4EREREREREZEPLJfdyvnTS1g6rZjOcIJ8rwPrMPu99EQSPLO9lQc3HmLjgR5e3N0BwPp93Ye99+nVuZw3rYRYMk17cGDPn55ognUN3expDwPwUn0n5/3nC5xWlUtJwEWBz0GOy0Z3r8H56Qx2+/EZt4iIiBw/Cl5ERERERERE5APPMAyK/M7Dns/zOrhyQTVXLqimrjXES/Ud5LjtbGrsJZ0xmVYWIBRL0htNUuh3sqA2n9PfNcPm3XoiCZ7b1Y7XaeWHT+7iYHeU5+s63tPKyp9/8iIXzyrjs2fVMr7Qi2mCxWLQn0jzfF07XeE4ibRJnsfOxbPKcNmtI/iKHD972sOkMhmiiTQ7W4JE4in8Lnt21s+qHW2s2tFGbaGXZTNLSaYz7GwJkeuxU+BzUuhz4HfZcNqseJ02Jpf4KA24tHSbiIicNBS8iIiIiIiIiIi8D1NK/Uwp9QPwiflV7/v6PK+DK+ZVAvCRqcVsOdRHfVuYrnCcrkiC9mA/L9W10hNN8of1B3lgQyN2q0E8lcHntJHOmEQT6UH3vP2pXSybWcrZk4pYNKEAr9NGKJakrjXE7rYwiVQa31t7ArUFY5w9qYhZlTnAQBBy35q9tIfi+Jw2ynPd1BR4OW9aMSWHWd7ubW/P5ukIx/A4bNitBge7o+xsCdERGhhPea6LEr+LfZ0R9ndFeKm+85hep02NvWxq7D2mtvleBwGXjdIcF9/96HRmlA+MzTRNBTIiInLCKXgRERERERERERklTpuVM2ryB+3Rk0wmefyvTeRPXcj/rGtk9a520pmB/WJCsRQAlXluZlXkYLda2High6befv7P2gP8n7UHsFsNinxOmvtih33eO56po8jvJM9jp7k3RjieGtroYQi4bMypyiXgthPsTzK9PEBDR4RDPf209vXTE02+7zFbLQN751gtBnOrcslx2wnHU3RFEpgmjCvw8Mn5VRzoivLwm4ewWgzOnVpMJJ6mKxKnM5QgkkgRT2XojSbY2xGhO5KgO5Jgf1eUf/rFK3xoQgGGYbC5sRevw8p1Z4/HajGwWixEEynyPA4umlmK16m3xg4nnTHZ2xGmPRhnapmfQt/AjDDTNNnTHqa5L0ZZjosJRT5e39/NmvoOzqjJ55zJRQq7ROQDT/+7iIiIiIiIiIicZKwGLJ5QwIenltLQMbAfjM9lIxRLkUhlmFziz+5Fk0hleKGunTX1Hby4u4PG7v5s6FIacDGl1I/PZSMcS5HKZHDbrTy3q52OUJyOUByABTX5/PO8SoKxJM29Md5s7OHNg70EY6lBM1SGm63itFko8juJJtJkTJNCn5PZFTmU5rjI8zjY0RIk2J9kapkfn9POhTNKqC30HvXN+cUT4eqF1Ud9rWLJNPVtYSKJFL99ZT9Pb28d1M++/iS3Pr5jyHW3Pradj84pY83uTjwOK8tmlfG5xbXkeEZ+Y51MxmTzoV5a+2IU+JzMrc4lkcqcNMFPbzTBU9taeX1/D067hVgyzat7umgNDnwdWQyozveQ53XQ15+koSNymDvtZWZFgE8vquHM2gKqCzwj3lfTNDnU08/25iAVue7szC0RkZPJyfHdXUREREREREREhjW+yJf9vNg/9LzDZuGCGaVcMKMU0zTZ3xWlMxxnUrGPXI9j2Hu2B2N0hOO0B+Mk0hnOnVqM3WoZ1CYYS3Kou5+X93SQTJv4nDZ2tQaZWOxnQpGX0hwXpQEXOW77qM5wcNmt2TffzxxfwJ72UDZMmVzqZ11DF8/vaifP48AE3HYrW5v62NcZ4U+vNWbvU7+6nt+8vI/zp5eAAXMqc1k2q5Qin3PI+EzTZO3eLh7d1Ew4kWJCoZdzphRTU+Ah3+sglszw6KYm3jzYS1soRn1bmKbe/iF9X1ibz6VzyjFNk70dEUKxFOF4klAsRcY0KQm4+OT8quwMnvcjkzHZ0RJky6E+om/NEEqlTVKZDKmMScY0yWRM9nUOLP8WT2WG3MPrsFLod3KgK8r+tz4GXnML1fkeDnRFiacyOG0Wlkwq4pU9nWxrCvKNh7YAcOGMEuZW5zG5xIeBwf6ugcBmSomfvW8FirkeBxsP9FDXGuKsSYV8fkktTtvg/Yr2tId5ub6DLU19rNvbNWg21/UfnsAZtfls3N+Dy25hYrGfSSU+xuV7sL3na1pE5EQxTNM0R7sTJ5tgMEhOTg59fX0EAoHR7s5JJZlM8uSTT3LxxRdjt4/8X4CIyOhQbYuMXapvkbFL9S0ydqm+j79MxuSxzc08sqmJZTNLcdqs3PPCXuraQkPa5nsdnDu1mFgyzYb93WRMME3oDMeHvbfDaiGVyZB5zztuPqeNKaUDgUPv+1yibWKxj+llAdx2K26HFY/DittuJW2aBFx2ppcHWFg7sFzdmvpOntzSwupd7Yft43CmlQU4f1rxwBhsFqaWBlgyuRCnzUpLXz+N3f10RxJkTJMlkwrxu+zEkmlCsRS5Hjt2q4XuSIL/WXuANfUdvHGwh7/nXceqfDeXnVZBNJFmV2uQdMbktX3dg15Pu9WgpsBLfXv4sPdx2ixceUYVH55STG9/gpd2d/JmYy9njs+nJODCwKA818V500rI89gxTdjVOvDvP708QCZjYrEMDbviqTQOq+WoQVhvNMG6hi6CsRS5bjs5bvvAkn3ROL98Yh2e/DI+NLGQ6eUBJhQdPiSVsaGlr5/71jTwQl0Hk0t85HkcRBNp8jx2Pr9kPFX5Q2eHpTMmFoOjfq2F4yncdmt2FqQcP+8nN9CMFxERERERERER+UCxWAwum1vBZXMrssf+aU456xq6eHVvFxaLwd+2t1LXFqI7kuChjYeG3MPrsHL56RXUFvpY19DF1kN9tIViJNIDM0cq89x8fG4FlXkeSnJcLKjJx+2wkkpn6I4kSKQzPPxGE+v3dWMYA2/257od+F02fE4bFovB6/u7+b8bD7GnPcyeI4QMAJOKfZTmuAYts+Z1WJlXk0++x47DZsFmtWB/a68bq2XgDd1iv5Mzxxcwozxw2Dd4y3LclOW4hxx32a247O/MTsn3Oli5dBIrl06irjXEX944RGswxtamPgCmlvpJpk22N/VRle/BYbPQ159kblUuFXlu7luzj8bufu56bs+Q51oyqZDTqnJZUJvPvHF5eBw2Htp4iD+9dpCeaILZFTlYDIP6t16r/mSa3609wO/WHhh0n32dwy+TZhhkg6LxhV72d0Uo8Dk5oyYPt93G6we66QzFiSTSVOS6OXtyEdX5HmaUB+gMx8n3OugIxdmwv5u61hBbm/qGhG/vsEBLG09tb8seKfA6sku5hWMpctx2SgJOigMuSgJOCrxOTKDI58ButRCOp5hdmcvc6twhs9Vk9KXSGaLJNFsP9bF2bxe/eWUfkUQaGPo1+MCGRj40oYBPnVHF5BI/r+3rZkdLkP99vZFU2mRaWYCl00qwWQ0SqQwd4TiZzMAsxIPdUf62ow233UpFnptpZQFuuXR6dk+mtmCMlr4Y6UyGdAYypsnUUr+CvhNAM16GoRkvh6e/uhEZm1TbImOX6ltk7FJ9i4xdqu+TRzyVZl1DN6/t68LvsjO7Mge33UoqYzK7MmfIkljxVJr2YBynzUKhzznsjIn3KxRL8uzONrojSfoTKfqTaaKJNLFkGsMw6A4nWFPfQfStN3VtFoMrF1Rx0YwyFtTm47CdOm/K9yfSPPTGIXY09+GyW5lWOvC+3PTyADMrjn0vl0zGZG1DF/e/1EBXOIHXaWVyiZ+FtQWsa+gibZqYJmxu7GVHSzB7ncdhJZnOkEyPzNulE4t9VOS66etPEuxPEowlsVstFFqifOS0Sbx5qI+97eFBS6e9X7keOzPLc5hY7GN+TR5TSvxMLPaN6vJ/o800TVIZ84QHUq19MZ7Y0szjW1rY3Ng75Pzp1bl87qzx7OsMk84M7Nv17I421jZ0jWg//C4bmbdSv7fDnndz2izUFnrJmCZXL6gmz+ugttDLzPKcQd+z4qn0kO9xH3Sa8SIiIiIiIiIiIvIPctqsnDO5iHMmFx1z++GWDPpH+F12Lp9becQ2ff1Jnt3Rxp6OMMtmljK7MndE+3CiuB1Wrjlz3D98H4vFYPHEQhZPLBxy7pLZZYMeh2JJEqkM6YxJrsdBeyjG5sY+ppX56YkmeG5XO7Fkhg9PKaIqz4PfZWPD/m52NAepbw9T1xqiyO+kPRTHbbdy7tRiJpX4WFCbP+wsoWyweu6EbLAaiado6IgQiifJcdvxOW30RpO0h+K0BWO0B2N0RhIY8NbsBROX3cKG/T10RxK8vKeTl/d08ttX9wNQEnCyoLaAshwXLpuF6gIv500tJs87urMcMhmT7miCcCxFOJ4iFEuRTGdImyZ2y8C+QVX57r87NOpPpHl1byc/emoXTb39LJtZRjKdIdcz8JpaLQZNPf28tKeTQp+TQp+DWDKNacJHphbz+v5uAm47Z44voCLXjcUw6I4mmFOZw7gCb/Z5Ysk0TpuFXa0hXqjroKk3SsBl579e3kfiPXslVeS6mV+Tx4enFPGxORVDwtjPLq5he3OQJ7a0cP9LDaQzJgtr86nM83DZ3HJqC738bXsbO1qC2CwGVotBgc+J3WIQiqcwgMtPr8BmMWjs7ue2x7dn92ICsBgDM9ZsVgOrYRBPZWjq7c8uq3fr4zuybfO9DmZV5Ax8PXZGWDyxkLuumvt3/VuIghcREREREREREZFTWo7bzhXzjhzOyPD8rsGzyirzPFTmvROezRuXP+Sai2aWcdHMsiHH/15ep41ZlYNn9IwrOPp1qXSGTY297OuMsLWpj82H+tjdGqItGOfxzc2D2jpsFr5wzgQ+v6SWVNok/zAhTDyV5s2DvdlwpLmvn+5wgsmlfop8TroiCXqjCZw2C/s6o9S3hwjGUvidNiaX+LlgRgk1BV7yvHashsH+rijtwRh/29HGX7e20BE68r5DH55SxNcumMK0ssBh9yzpiybp7U9QEnDhtFn469YW7n9pH9ub+ki9a323//vG0CUC3/befrx+oCf7+aObBr92FgNOq8rFBA52RemKJHDZLcSSg0MWGGj38dMrOG9aSTZEOxLDMJhZkcPMihyuPKOKjGkyvsg3qM1nz6o94j3eNrHYz8Lx+Ww51Eehz4nVMrCUoPddfTBNk82H+ugMxTnQHeWJLc3YLAY7WwaWVXxxd0e2bf0we17JsVPwIiIiIiIiIiIiInKKsVktzK/JZ35NPp+YXwUMzMZ4fX8Pmw/1DuwXE0/x5sFedrYE+fnqen6+uh6AYr+T/Lf2lImnMvhdNnI9Dna3huhPDl2e6li8vKeTX7+yDwCH1YLTZiEUTw1p53Pa8DqteJ02nDYrVgvEkxn2dUZ4oa6DF+o6KPI7uXxuBU6bhcbuKBV5bqaWBnhqWwtPbm0FwG41yHE76Ay/E6KUBJwsm1nGWRML2XiwhzyPnb7+JNFEmkzGxGUfmMUWTaQJxZM4rFYae6K8WNfB/Jo8UhmTHc1B2oIDM4vcDitbDvXxxsHeQWOIJTPYLAYfnlJMTYGHhs4I500r5uoF1X/3jJ2aQu/RGx2Fx2HjzPGHT+0Mw+C0qtzs48+9Feok0xneONDDga4oboeV2kIvE94TAMn7o+BFREREREREREREZAxw2a2cNamQsya9s8yaaZo8uqmZb/5lS3aWRnsoTvu7Zn10RxIceGuJqkKfk4o8N267hfJcNwGXnR0tQaKJFAGXnQKfk1gyTXW+h8klPgq8TrqjCTbs6+ZvO9qIxFMk0hkS6Qweh5V8r4MFNfl8dE4ZZ00sOuy+Q3vaQ9z+5C7W7+umIxTnvjUNhx2n02YhnsrQGY5jtxp88cMT+eT8Sipy31mqbOn0kmN+3b5wzoTDntvdFmJPexgDqMr3UJnnpiuSIN/jGPXl20aK3Wph4fgCFh4htJH3R8GLiIiIiIiIiIiIyBhlGAaXza3gwhml9CfTOGwW9rSH6etPkuu243ZY6Qon6I4kmFTiY2KRb8heJMfik/OruIOBoGd/V5RIPHXEJcPea2Kxn//+zBkk0xme3NrCa/u6ASjPdVPfFqKlL0a+18GXzp3EtDI/h3r6CcVSlOW4jmsAMrnEz+QS/6BjuZ6xEbjI8aPgRURERERERERERGSMczusuB1WgEHLTQFw7JNDjsowDGr/gWWz7FYLHzutgo+dVnHEdlX5niOeFxlNw8/rEhERERERERERERERkfdNwYuIiIiIiIiIiIiIiMgIUfAiIiIiIiIiIiIiIiIyQhS8iIiIiIiIiIiIiIiIjBAFLyIiIiIiIiIiIiIiIiNEwYuIiIiIiIiIiIiIiMgIUfAiIiIiIiIiIiIiIiIyQhS8iIiIiIiIiIiIiIiIjBAFLyIiIiIiIiIiIiIiIiNEwYuIiIiIiIiIiIiIiMgIUfAiIiIiIiIiIiIiIiIyQhS8iIiIiIiIiIiIiIiIjBAFLyIiIiIiIiIiIiIiIiNEwYuIiIiIiIiIiIiIiMgIUfAiIiIiIiIiIiIiIiIyQhS8iIiIiIiIiIiIiIiIjBAFLyIiIiIiIiIiIiIiIiNEwYuIiIiIiIiIiIiIiMgIsY12B05GpmkCEAwGR7knJ59kMkk0GiUYDGK320e7OyIyQlTbImOX6ltk7FJ9i4xdqm+RsUv1LXLqejsveDs/OBIFL8MIhUIAVFVVjXJPRERERERERERERETkZBEKhcjJyTliG8M8lnjmAyaTydDc3Izf78cwjNHuzkklGAxSVVVFY2MjgUBgtLsjIiNEtS0ydqm+RcYu1bfI2KX6Fhm7VN8ipy7TNAmFQpSXl2OxHHkXF814GYbFYqGysnK0u3FSCwQC+s9BZAxSbYuMXapvkbFL9S0ydqm+RcYu1bfIqeloM13eduRYRkRERERERERERERERI6ZghcREREREREREREREZERouBF3hen08ktt9yC0+kc7a6IyAhSbYuMXapvkbFL9S0ydqm+RcYu1bfIB4NhmqY52p0QEREREREREREREREZCzTjRUREREREREREREREZIQoeBERERERERERERERERkhCl5ERERERERERERERERGiIIXERERERERERERERGREaLgRY7Z3XffTU1NDS6Xi4ULF/Laa6+NdpdE5Ahuv/12zjjjDPx+P8XFxVx22WXU1dUNahOLxbjhhhsoKCjA5/NxxRVX0NbWNqjNwYMHueSSS/B4PBQXF/P1r3+dVCp1IociIkfxox/9CMMw+MpXvpI9pvoWOXU1NTXxL//yLxQUFOB2u5k1axavv/569rxpmnzve9+jrKwMt9vN0qVLqa+vH3SP7u5uli9fTiAQIDc3l8997nOEw+ETPRQReZd0Os13v/tdamtrcbvdTJgwge9///uYpplto/oWOTWsWbOGSy+9lPLycgzD4JFHHhl0fqRqecuWLSxZsgSXy0VVVRU/+clPjvfQRGSEKHiRY/LnP/+Zm266iVtuuYU33niDOXPmcOGFF9Le3j7aXRORw3jxxRe54YYbWLduHatWrSKZTHLBBRcQiUSybb761a/y+OOP8+CDD/Liiy/S3NzMxz/+8ez5dDrNJZdcQiKR4NVXX+V3v/sdv/3tb/ne9743GkMSkWFs2LCBX/3qV8yePXvQcdW3yKmpp6eHxYsXY7fbeeqpp9ixYwf/+Z//SV5eXrbNT37yE37+859z7733sn79erxeLxdeeCGxWCzbZvny5Wzfvp1Vq1bxxBNPsGbNGq677rrRGJKIvOXHP/4x99xzD7/4xS/YuXMnP/7xj/nJT37CXXfdlW2j+hY5NUQiEebMmcPdd9897PmRqOVgMMgFF1zAuHHj2LhxI3fccQe33nor991333Efn4iMAFPkGCxYsMC84YYbso/T6bRZXl5u3n777aPYKxF5P9rb203AfPHFF03TNM3e3l7TbrebDz74YLbNzp07TcBcu3ataZqm+eSTT5oWi8VsbW3NtrnnnnvMQCBgxuPxEzsAERkiFAqZkyZNMletWmWec8455sqVK03TVH2LnMpuvvlm86yzzjrs+UwmY5aWlpp33HFH9lhvb6/pdDrNP/3pT6ZpmuaOHTtMwNywYUO2zVNPPWUahmE2NTUdv86LyBFdcskl5mc/+9lBxz7+8Y+by5cvN01T9S1yqgLMhx9+OPt4pGr5l7/8pZmXlzfoZ/Obb77ZnDJlynEekYiMBM14kaNKJBJs3LiRpUuXZo9ZLBaWLl3K2rVrR7FnIvJ+9PX1AZCfnw/Axo0bSSaTg2p76tSpVFdXZ2t77dq1zJo1i5KSkmybCy+8kGAwyPbt209g70VkODfccAOXXHLJoDoG1bfIqeyxxx5j/vz5fOITn6C4uJi5c+dy//33Z8/v27eP1tbWQfWdk5PDwoULB9V3bm4u8+fPz7ZZunQpFouF9evXn7jBiMggH/rQh1i9ejW7d+8GYPPmzbz88sssW7YMUH2LjBUjVctr167l7LPPxuFwZNtceOGF1NXV0dPTc4JGIyJ/L9tod0BOfp2dnaTT6UFvzACUlJSwa9euUeqViLwfmUyGr3zlKyxevJiZM2cC0NraisPhIDc3d1DbkpISWltbs22Gq/23z4nI6HnggQd444032LBhw5Bzqm+RU1dDQwP33HMPN910E9/+9rfZsGEDX/7yl3E4HKxYsSJbn8PV77vru7i4eNB5m81Gfn6+6ltkFH3zm98kGAwydepUrFYr6XSaH/zgByxfvhxA9S0yRoxULbe2tlJbWzvkHm+fe/cypCJy8lHwIiLyAXDDDTewbds2Xn755dHuioiMgMbGRlauXMmqVatwuVyj3R0RGUGZTIb58+fzwx/+EIC5c+eybds27r33XlasWDHKvRORf8T//u//8oc//IE//vGPzJgxg02bNvGVr3yF8vJy1beIiMgYo6XG5KgKCwuxWq20tbUNOt7W1kZpaeko9UpEjtWNN97IE088wfPPP09lZWX2eGlpKYlEgt7e3kHt313bpaWlw9b+2+dEZHRs3LiR9vZ2Tj/9dGw2GzabjRdffJGf//zn2Gw2SkpKVN8ip6iysjKmT58+6Ni0adM4ePAg8E59Huln89LSUtrb2wedT6VSdHd3q75FRtHXv/51vvnNb3LllVcya9YsrrnmGr761a9y++23A6pvkbFipGpZP6+LnNoUvMhRORwO5s2bx+rVq7PHMpkMq1evZtGiRaPYMxE5EtM0ufHGG3n44Yd57rnnhkxRnjdvHna7fVBt19XVcfDgwWxtL1q0iK1btw76gXDVqlUEAoEhbwqJyIlz3nnnsXXrVjZt2pT9mD9/PsuXL89+rvoWOTUtXryYurq6Qcd2797NuHHjAKitraW0tHRQfQeDQdavXz+ovnt7e9m4cWO2zXPPPUcmk2HhwoUnYBQiMpxoNIrFMvhtGKvVSiaTAVTfImPFSNXyokWLWLNmDclkMttm1apVTJkyRcuMiZwKTJFj8MADD5hOp9P87W9/a+7YscO87rrrzNzcXLO1tXW0uyYih/HFL37RzMnJMV944QWzpaUl+xGNRrNtvvCFL5jV1dXmc889Z77++uvmokWLzEWLFmXPp1Ipc+bMmeYFF1xgbtq0yXz66afNoqIi81vf+tZoDElEjuCcc84xV65cmX2s+hY5Nb322mumzWYzf/CDH5j19fXmH/7wB9Pj8Zi///3vs21+9KMfmbm5ueajjz5qbtmyxfzYxz5m1tbWmv39/dk2F110kTl37lxz/fr15ssvv2xOmjTJvOqqq0ZjSCLylhUrVpgVFRXmE088Ye7bt8/8y1/+YhYWFprf+MY3sm1U3yKnhlAoZL755pvmm2++aQLmnXfeab755pvmgQMHTNMcmVru7e01S0pKzGuuucbctm2b+cADD5gej8f81a9+dcLHKyLvn4IXOWZ33XWXWV1dbTocDnPBggXmunXrRrtLInIEwLAfv/nNb7Jt+vv7zeuvv97My8szPR6Pefnll5stLS2D7rN//35z2bJlptvtNgsLC81/+7d/M5PJ5AkejYgczXuDF9W3yKnr8ccfN2fOnGk6nU5z6tSp5n333TfofCaTMb/73e+aJSUlptPpNM877zyzrq5uUJuuri7zqquuMn0+nxkIBMxrr73WDIVCJ3IYIvIewWDQXLlypVldXW26XC5z/Pjx5ne+8x0zHo9n26i+RU4Nzz///LC/b69YscI0zZGr5c2bN5tnnXWW6XQ6zYqKCvNHP/rRiRqiiPyDDNM0zdGZayMiIiIiIiIiIiIiIjK2aI8XERERERERERERERGREaLgRUREREREREREREREZIQoeBERERERERERERERERkhCl5ERERERERERERERERGiIIXERERERERERERERGREaLgRUREREREREREREREZIQoeBERERERERERERERERkhCl5ERERERERERERERERGiIIXERERERGREWYYBo888shod0NEREREREaBghcRERERERlTPvOZz2AYxpCPiy66aLS7JiIiIiIiHwC20e6AiIiIiIjISLvooov4zW9+M+iY0+kcpd6IiIiIiMgHiWa8iIiIiIjImON0OiktLR30kZeXBwwsA3bPPfewbNky3G4348eP56GHHhp0/datWzn33HNxu90UFBRw3XXXEQ6HB7X59a9/zYwZM3A6nZSVlXHjjTcOOt/Z2cnll1+Ox+Nh0qRJPPbYY8d30CIiIiIiclJQ8CIiIiIiIh843/3ud7niiivYvHkzy5cv58orr2Tnzp0ARCIRLrzwQvLy8tiwYQMPPvggzz777KBg5Z577uGGG27guuuuY+vWrTz22GNMnDhx0HPcdtttfPKTn2TLli1cfPHFLF++nO7u7hM6ThEREREROfEM0zTN0e6EiIiIiIjISPnMZz7D73//e1wu16Dj3/72t/n2t7+NYRh84Qtf4J577smeO/PMMzn99NP55S9/yf3338/NN99MY2MjXq8XgCeffJJLL72U5uZmSkpKqKio4Nprr+U//uM/hu2DYRj8+7//O9///veBgTDH5/Px1FNPaa8ZEREREZExTnu8iIiIiIjImPORj3xkULACkJ+fn/180aJFg84tWrSITZs2AbBz507mzJmTDV0AFi9eTCaToa6uDsMwaG5u5rzzzjtiH2bPnp393Ov1EggEaG9v/3uHJCIiIiIipwgFLyIiIiIiMuZ4vd4hS3+NFLfbfUzt7Hb7oMeGYZDJZI5Hl0RERERE5CSiPV5EREREROQDZ926dUMeT5s2DYBp06axefNmIpFI9vwrr7yCxWJhypQp+P1+ampqWL169Qnts4iIiIiInBo040VERERERMaceDxOa2vroGM2m43CwkIAHnzwQebPn89ZZ53FH/7wB1577TX++7//G4Dly5dzyy23sGLFCm699VY6Ojr40pe+xDXXXENJSQkAt956K1/4whcoLi5m2bJlhEIhXnnlFb70pS+d2IGKiIiIiMhJR8GLiIiIiIiMOU8//TRlZWWDjk2ZMoVdu3YBcNttt/HAAw9w/fXXU1ZWxp/+9CemT58OgMfj4ZlnnmHlypWcccYZeDwerrjiCu68887svVasWEEsFuOnP/0pX/va1ygsLOSf//mfT9wARURERETkpGWYpmmOdidEREREREROFMMwePjhh7nssstGuysiIiIiIjIGaY8XERERERERERERERGREaLgRUREREREREREREREZIRojxcREREREflA0WrLIiIiIiJyPGnGi4iIiIiIiIiIiIiIyAhR8CIiIiIiIiIiIiIiIjJCFLyIiIiIiIiIiIiIiIiMEAUvIiIiIiIiIiIiIiIiI0TBi4iIiIiIiIiIiIiIyAhR8CIiIiIiIiIiIiIiIjJCFLyIiIiIiIiIiIiIiIiMEAUvIiIiIiIiIiIiIiIiI+T/B4FiuWsMAf2lAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 2000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title Plot loss\n",
    "\n",
    "def plot_line(labels, data_dicts, x_name='X-Axis', y_name='Y-Axis'):\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    for label, data_dict in zip(labels, data_dicts):\n",
    "        for key, values in data_dict.items():\n",
    "            plt.plot(values, label=f\"{label} - {key}\")\n",
    "    plt.xlabel(x_name)\n",
    "    plt.ylabel(y_name)\n",
    "    plt.title('Loss Plot')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "loss_values = model.loss\n",
    "smoothed_loss = smooth(np.array(loss_values), window=300)  # Adjust smoothness if needed\n",
    "\n",
    "info = {'Loss': smoothed_loss}\n",
    "plot_line(['Model'], [info], x_name='Epoch', y_name='Loss Value')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d23b5d8",
   "metadata": {
    "papermill": {
     "duration": 0.019409,
     "end_time": "2025-07-17T10:08:24.764128",
     "exception": false,
     "start_time": "2025-07-17T10:08:24.744719",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train HAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18d99ca7",
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2025-07-17T10:08:24.803832Z",
     "iopub.status.busy": "2025-07-17T10:08:24.803559Z",
     "iopub.status.idle": "2025-07-17T10:08:24.808988Z",
     "shell.execute_reply": "2025-07-17T10:08:24.808402Z"
    },
    "id": "Q7IfdzX4LFTk",
    "papermill": {
     "duration": 0.026584,
     "end_time": "2025-07-17T10:08:24.809974",
     "exception": false,
     "start_time": "2025-07-17T10:08:24.783390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title Cost function\n",
    "def mean_with_cost(feedback, zero_reward_cost=0.1):\n",
    "  B, L = feedback.shape\n",
    "  cost = torch.zeros_like(feedback)\n",
    "  cost[feedback == 0] = -zero_reward_cost\n",
    "  reward = torch.mean(feedback + cost, dim=-1)\n",
    "  return reward\n",
    "\n",
    "def min_with_cost(feedback, zero_reward_cost=0.1):\n",
    "    cost = torch.zeros_like(feedback)\n",
    "    cost[feedback == 0] = -zero_reward_cost\n",
    "    reward = feedback + cost\n",
    "    return torch.min(reward, dim=-1).values  #  get values only\n",
    "    \n",
    "def nsw(avg_r, min_r, lambda_nsw=1e-4, epsilon=1e-8):\n",
    "    r_vec = torch.stack([avg_r, min_r + lambda_nsw], dim=-1)\n",
    "    r_vec = torch.clamp(r_vec, min=epsilon)\n",
    "    return torch.sum(torch.log(r_vec), dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc40b811",
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2025-07-17T10:08:24.848621Z",
     "iopub.status.busy": "2025-07-17T10:08:24.848421Z",
     "iopub.status.idle": "2025-07-17T10:08:24.852464Z",
     "shell.execute_reply": "2025-07-17T10:08:24.851795Z"
    },
    "id": "KP_0zmoEFwta",
    "papermill": {
     "duration": 0.024685,
     "end_time": "2025-07-17T10:08:24.853515",
     "exception": false,
     "start_time": "2025-07-17T10:08:24.828830",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title BaseRL Environment\n",
    "class BaseEnv():\n",
    "  def __init__(self, params):\n",
    "    super().__init__()\n",
    "    self.reward_func = params['reward_function']\n",
    "    self.max_step_per_episode = params['max_step']\n",
    "    self.initial_temper = params[\"initial_temper\"]\n",
    "\n",
    "  def reset(self, paras):\n",
    "    pass\n",
    "  def step(self, action):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b105d8",
   "metadata": {
    "papermill": {
     "duration": 0.018804,
     "end_time": "2025-07-17T10:08:24.891369",
     "exception": false,
     "start_time": "2025-07-17T10:08:24.872565",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 1. Environment define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee2b9f89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T10:08:24.930705Z",
     "iopub.status.busy": "2025-07-17T10:08:24.930489Z",
     "iopub.status.idle": "2025-07-17T10:08:24.948236Z",
     "shell.execute_reply": "2025-07-17T10:08:24.947518Z"
    },
    "papermill": {
     "duration": 0.039167,
     "end_time": "2025-07-17T10:08:24.949432",
     "exception": false,
     "start_time": "2025-07-17T10:08:24.910265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class KREnvironment(BaseEnv):\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        super().__init__(params)\n",
    "        self.reader = KRDataReader(params)\n",
    "        self.user_response_model = KRUserResponse(self.reader, params)\n",
    "        checkpoint = torch.load(params['model_path'] + \".checkpoint\", map_location=device)\n",
    "        self.user_response_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        self.user_response_model.to(device)\n",
    "    \n",
    "        # spaces\n",
    "        stats = self.reader.get_statistics()\n",
    "        self.action_space = {'item_id': ('nomial', stats['n_item']),\n",
    "                             'item_feature': ('continuous', stats['item_vec_size'], 'normal')}\n",
    "        self.observation_space = {'user_profile': ('continuous', stats['user_portrait_len'], 'positive'),\n",
    "                                  'history': ('sequence', stats['max_seq_len'], ('continuous', stats['item_vec_size']))}\n",
    "        self.n_worker = params['n_worker']\n",
    "        \n",
    "        \n",
    "    def reset(self, params = {'batch_size': 1, 'empty_history': True}):\n",
    "        '''\n",
    "        Reset environment with new sampled users\n",
    "        @input:\n",
    "        - params: {'batch_size': scalar, \n",
    "                    'empty_history': True if start from empty history, \n",
    "                    'initial_history': start with initial history, mu }\n",
    "        @output:\n",
    "        - observation: {'user_profile': (B, portrait_dim), \n",
    "                        'history': (B, H), \n",
    "                        'history_feature': (B, H, item_dim)\n",
    "                        'history_feedback': (B, H, item_dim)}\n",
    "        '''\n",
    "        self.empty_history_flag = params['empty_history'] if 'empty_history' not in params else True\n",
    "        BS = params['batch_size']\n",
    "        observation = {'batch_size': BS}\n",
    "        if 'sample' in params:\n",
    "            sample_info = params['sample']\n",
    "        else:\n",
    "            self.batch_iter = iter(DataLoader(self.reader, batch_size = BS, shuffle = True, \n",
    "                                              pin_memory = True, num_workers = self.n_worker))\n",
    "            sample_info = next(self.batch_iter)\n",
    "            sample_info = wrap_batch(sample_info, device = self.user_response_model.device)\n",
    "        self.current_observation = {\n",
    "            'user_profile': sample_info['user_profile'],  # (B, user_dim)\n",
    "            'history': sample_info['history'],  # (B, H)\n",
    "            'history_features': sample_info['history_features'], # (B, H, item_dim)\n",
    "            'cummulative_reward': torch.zeros(BS).to(self.user_response_model.device),\n",
    "            'max_reward': torch.full((BS,), -float('inf'), device=self.user_response_model.device),\n",
    "            'temper': torch.ones(BS).to(self.user_response_model.device) * self.initial_temper,\n",
    "            'step': torch.zeros(BS).to(self.user_response_model.device),\n",
    "        }\n",
    "        self.reward_history = [0.]\n",
    "        self.step_history = [0.]\n",
    "        return copy.deepcopy(self.current_observation)\n",
    "        \n",
    "    \n",
    "    def step(self, step_dict):\n",
    "        '''\n",
    "        @input:\n",
    "        - step_dict: {'action': (B, slate_size),\n",
    "                        'action_features': (B, slate_size, item_dim) }\n",
    "        '''\n",
    "        # actions (exposures)\n",
    "        action = step_dict['action'] # (B, slate_size), should be item ids only\n",
    "        action_features = step_dict['action_features']\n",
    "        batch_data = {\n",
    "            'user_profile': self.current_observation['user_profile'],\n",
    "            'history_features': self.current_observation['history_features'],\n",
    "            'exposure_features': action_features\n",
    "        }\n",
    "        # URM forward\n",
    "        with torch.no_grad():\n",
    "            output_dict = self.user_response_model(batch_data)\n",
    "            response = torch.bernoulli(output_dict['probs']) # (B, slate_size)\n",
    "#             prob_scale = (self.current_observation['temper'].clone().detach().view(-1,1) + self.temper_prob_lag) / (self.initial_temper + self.temper_prob_lag)\n",
    "            probs_under_temper = output_dict['probs'] # * prob_scale\n",
    "            response = torch.bernoulli(probs_under_temper).detach() # (B, slate_size)\n",
    "\n",
    "            # reward (B,)\n",
    "            immediate_reward = self.reward_func(response).detach()\n",
    "\n",
    "            self.current_observation['max_reward'] = torch.max(immediate_reward, self.current_observation['max_reward'])\n",
    "\n",
    "\n",
    "            # (B, H+slate_size)\n",
    "            H_prime = torch.cat((self.current_observation['history'], action), dim = 1) \n",
    "            # (B, H+slate_size, item_dim)\n",
    "            H_prime_features = torch.cat((self.current_observation['history_features'], action_features), dim = 1) \n",
    "            # (B, H+slate_size)\n",
    "            F_prime = torch.cat((torch.ones_like(self.current_observation['history']), response), dim = 1).to(torch.long) \n",
    "            # vector, vector\n",
    "            row_indices, col_indices = (F_prime == 1).nonzero(as_tuple=True) \n",
    "            # (B,), the number of positive iteraction as history length\n",
    "            L = F_prime.sum(dim = 1) \n",
    "            \n",
    "            # user history update\n",
    "            offset = 0\n",
    "            newH = torch.zeros_like(self.current_observation['history'])\n",
    "            newH_features = torch.zeros_like(self.current_observation['history_features'])\n",
    "            for row_id in range(action.shape[0]):\n",
    "                right = offset + L[row_id]\n",
    "                left = right - self.reader.max_seq_len\n",
    "                newH[row_id] = H_prime[row_id, col_indices[left:right]]\n",
    "                newH_features[row_id] = H_prime_features[row_id,col_indices[left:right],:]\n",
    "                offset += L[row_id]\n",
    "            self.current_observation['history'] = newH\n",
    "            self.current_observation['history_features'] = newH_features\n",
    "            self.current_observation['cummulative_reward'] += immediate_reward\n",
    "\n",
    "            # temper update for leave model\n",
    "            temper_down = (-immediate_reward+1) * response.shape[1] + 1\n",
    "#             temper_down = -(torch.sum(response, dim = 1) - response.shape[1] - 1)\n",
    "#             temper_down = torch.abs(torch.sum(response, dim = 1) - response.shape[1] * self.temper_sweet_point) + 1\n",
    "            self.current_observation['temper'] -= temper_down\n",
    "            # leave signal\n",
    "            done_mask = self.current_observation['temper'] < 1\n",
    "            # step update\n",
    "            self.current_observation['step'] += 1\n",
    "\n",
    "            # update rows where user left\n",
    "#             refresh_rows = done_mask.nonzero().view(-1)\n",
    "#             print(f\"#refresh: {refresh_rows}\")\n",
    "            if done_mask.sum() > 0:\n",
    "                final_rewards = self.current_observation['cummulative_reward'][done_mask].detach().cpu().numpy()\n",
    "                final_steps = self.current_observation['step'][done_mask].detach().cpu().numpy()\n",
    "                self.reward_history.append(final_rewards[-1])\n",
    "                self.step_history.append(final_steps[-1])\n",
    "                # sample new users to fill in the blank\n",
    "                new_sample_flag = False\n",
    "                try:\n",
    "                    sample_info = next(self.iter)\n",
    "                    if sample_info['user_profile'].shape[0] != done_mask.shape[0]:\n",
    "                        new_sample_flag = True\n",
    "                except:\n",
    "                    new_sample_flag = True\n",
    "                if new_sample_flag:\n",
    "                    self.iter = iter(DataLoader(self.reader, batch_size = done_mask.shape[0], shuffle = True, \n",
    "                                                pin_memory = True, num_workers = params['n_worker']))\n",
    "                    sample_info = next(self.iter)\n",
    "                sample_info = wrap_batch(sample_info, device = self.user_response_model.device)\n",
    "                for obs_key in ['user_profile', 'history', 'history_features']:\n",
    "                    self.current_observation[obs_key][done_mask] = sample_info[obs_key][done_mask]\n",
    "                self.current_observation['cummulative_reward'][done_mask] *= 0\n",
    "                self.current_observation['max_reward'][done_mask] = -float('inf')\n",
    "                self.current_observation['temper'][done_mask] *= 0\n",
    "                self.current_observation['temper'][done_mask] += self.initial_temper\n",
    "            self.current_observation['step'][done_mask] *= 0\n",
    "#         print(f\"step: {self.current_observation['step']}\")\n",
    "        \n",
    "        return copy.deepcopy(self.current_observation), immediate_reward, done_mask, {'response': response}\n",
    "    def stop(self):\n",
    "        self.iter = None\n",
    "    \n",
    "    def get_new_iterator(self, B):\n",
    "        return iter(DataLoader(self.reader, batch_size = B, shuffle = True, \n",
    "                               pin_memory = True, num_workers = params['n_worker']))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e688fa77",
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2025-07-17T10:08:24.987922Z",
     "iopub.status.busy": "2025-07-17T10:08:24.987720Z",
     "iopub.status.idle": "2025-07-17T10:08:24.996908Z",
     "shell.execute_reply": "2025-07-17T10:08:24.996238Z"
    },
    "id": "3BvTCi5yZvxS",
    "papermill": {
     "duration": 0.029879,
     "end_time": "2025-07-17T10:08:24.998101",
     "exception": false,
     "start_time": "2025-07-17T10:08:24.968222",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title Self-Attentive Sequential Recommendation\n",
    "\n",
    "class SASRec(nn.Module):\n",
    "  def __init__(self, environment, params):\n",
    "    super().__init__()\n",
    "    self.n_layer = params['sasrec_n_layer']\n",
    "    self.d_model = params['sasrec_d_model']\n",
    "    self.n_head = params['sasrec_n_head']\n",
    "    self.dropout_rate = params['sasrec_dropout']\n",
    "    self.d_forward = params['sasrec_d_forward']\n",
    "\n",
    "    # item space\n",
    "    self.item_space = environment.action_space['item_id'][1]\n",
    "    self.item_dim = environment.action_space['item_feature'][1]\n",
    "    self.maxlen = environment.observation_space['history'][1]\n",
    "    self.state_dim = self.d_model\n",
    "    self.action_dim = self.d_model\n",
    "\n",
    "    # policy network modules\n",
    "    self.item_map = nn.Linear(self.item_dim, self.d_model)\n",
    "    self.pos_emb = nn.Embedding(self.maxlen, self.d_model)\n",
    "    self.pos_emb_getter = torch.arange(self.maxlen, dtype = torch.long)\n",
    "    self.emb_dropout = nn.Dropout(self.dropout_rate)\n",
    "    self.emb_norm = nn.LayerNorm(self.d_model)\n",
    "    self.attn_mask = ~torch.tril(torch.ones((self.maxlen, self.maxlen), dtype=torch.bool))\n",
    "    encoder_layer = nn.TransformerEncoderLayer(d_model=self.d_model,\n",
    "                                               nhead=self.n_head,\n",
    "                                               dim_feedforward= self.d_forward,\n",
    "                                               dropout=self.dropout_rate,\n",
    "                                               batch_first = True\n",
    "                                               )\n",
    "    self.transformer = nn.TransformerEncoder(encoder_layer= encoder_layer,\n",
    "                                             num_layers = self.n_layer)\n",
    "\n",
    "  def score(self, action_emb, item_emb, do_softmax=True):\n",
    "    item_emb = self.item_map(item_emb)\n",
    "    output = dot_scorer(action_emb, item_emb, self.d_model)\n",
    "    if do_softmax:\n",
    "      return torch.softmax(output, dim=-1)\n",
    "    else:\n",
    "      return output\n",
    "\n",
    "  def get_scorer_parameters(self):\n",
    "    return self.item_map.parameters()\n",
    "\n",
    "  def encode_state(self, feed_dict):\n",
    "    user_history = feed_dict['history_features']\n",
    "    # (1, H, d_model)\n",
    "    # for item in feed_dict.items():\n",
    "    #   print(item)\n",
    "    # print(\"user_history device:\", user_history.device)\n",
    "    # print(\"self.pos_emb_getter device:\", self.pos_emb_getter.device)\n",
    "    # print(\"self.pos_emb device\", self.pos_emb.device)\n",
    "\n",
    "    pos_emb = self.pos_emb(self.pos_emb_getter.to(user_history.device)).view(1, self.maxlen, self.d_model)\n",
    "\n",
    "    # (B, H, d_model)\n",
    "    history_item_emb = self.item_map(user_history).view(-1, self.maxlen, self.d_model)\n",
    "    history_item_emb = self.emb_norm(self.emb_dropout(history_item_emb + pos_emb))\n",
    "\n",
    "    # (B, H, d_model)\n",
    "    output_seq = self.transformer(history_item_emb, mask = self.attn_mask.to(user_history.device))\n",
    "\n",
    "    return {'output_seq': output_seq, 'state_emb': output_seq[:, -1, :]}\n",
    "\n",
    "  def forward(self, feed_dict):\n",
    "    '''\n",
    "    @input\n",
    "    - feed_dict: {'user_profile': (B, user_dim),\n",
    "                  'history_features': (B, H, item_dim),\n",
    "                  'history_mask': (B),\n",
    "                  'candicate_features': (B, L, item_dim) or (1, L, item_dim)\n",
    "                  }\n",
    "    @model\n",
    "    - user_profile --> user_emb (B, 1, f_dim)\n",
    "    - hisotry_items --> history_item_emb (B, H, f_dim)\n",
    "    - (Q:user_emb, K&V: history_item_emb) --(multi-head attn) --> user_state(B, 1, feature_dim)\n",
    "    - user_state --> action_prob (B, n_item)\n",
    "    '''\n",
    "    hist_enc = self.encode_state(feed_dict)\n",
    "\n",
    "    # user embedding (B, 1, d_model)\n",
    "    user_state = hist_enc['state_emb'].view(-1, self.d_model)\n",
    "\n",
    "    # action embedding (B, d_model)\n",
    "    action_emb = user_state\n",
    "\n",
    "    # regularization\n",
    "    reg = get_regularization(self.item_map, self.transformer)\n",
    "\n",
    "    out_dict = {\n",
    "        'action_emb': action_emb,\n",
    "        'state_emb': user_state,\n",
    "        'seq_emb': hist_enc['output_seq'],\n",
    "        'reg': reg\n",
    "    }\n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22fca1d9",
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2025-07-17T10:08:25.036796Z",
     "iopub.status.busy": "2025-07-17T10:08:25.036436Z",
     "iopub.status.idle": "2025-07-17T10:08:25.041300Z",
     "shell.execute_reply": "2025-07-17T10:08:25.040591Z"
    },
    "id": "Dz4XwAAtzx1d",
    "papermill": {
     "duration": 0.025291,
     "end_time": "2025-07-17T10:08:25.042345",
     "exception": false,
     "start_time": "2025-07-17T10:08:25.017054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# @title General Critic class\n",
    "class GeneralCritic(nn.Module):\n",
    "  def __init__(self, policy, params):\n",
    "    super().__init__()\n",
    "    self.state_dim = policy.state_dim\n",
    "    self.action_dim = policy.action_dim\n",
    "    self.net = DNN(self.state_dim + self.action_dim, params['critic_hidden_dims'], 1,\n",
    "                   dropout_rate=params['critic_dropout_rate'], do_batch_norm=True)\n",
    "\n",
    "  def forward(self, feed_dict):\n",
    "    '''\n",
    "    @input:\n",
    "    - feed_dict: {'state_emb': (B, state_dim), 'action_emb': (B, action_dim)}\n",
    "    '''\n",
    "    state_emb = feed_dict['state_emb']\n",
    "    action_emb = feed_dict['action_emb'].view(-1, self.action_dim)\n",
    "\n",
    "    Q = self.net(torch.cat((state_emb, action_emb), dim = -1)).view(-1)\n",
    "\n",
    "    reg = get_regularization(self.net)\n",
    "    return {'q': Q, 'reg': reg}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fabeed6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T10:08:25.081839Z",
     "iopub.status.busy": "2025-07-17T10:08:25.081174Z",
     "iopub.status.idle": "2025-07-17T10:08:25.086059Z",
     "shell.execute_reply": "2025-07-17T10:08:25.085321Z"
    },
    "papermill": {
     "duration": 0.025883,
     "end_time": "2025-07-17T10:08:25.087220",
     "exception": false,
     "start_time": "2025-07-17T10:08:25.061337",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ValueCritic(nn.Module):\n",
    "    \n",
    "    def __init__(self, policy, params):\n",
    "        super().__init__()\n",
    "        self.state_dim = policy.state_dim\n",
    "        self.action_dim = policy.action_dim\n",
    "#         self.state_encoder = policy.state_encoder\n",
    "        self.net = DNN(self.state_dim, params['critic_hidden_dims'], 1, \n",
    "                       dropout_rate = params['critic_dropout_rate'], do_batch_norm = True)\n",
    "        \n",
    "    def forward(self, feed_dict):\n",
    "        '''\n",
    "        @input:\n",
    "        - feed_dict: {'state_emb': (B, state_dim), 'action_emb': (B, action_dim)}\n",
    "        '''\n",
    "        state_emb = feed_dict['state_emb']\n",
    "        V = self.net(state_emb).view(-1)\n",
    "        reg = get_regularization(self.net)\n",
    "        return {'v': V, 'reg': reg}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ac43947",
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2025-07-17T10:08:25.126717Z",
     "iopub.status.busy": "2025-07-17T10:08:25.126474Z",
     "iopub.status.idle": "2025-07-17T10:08:25.148383Z",
     "shell.execute_reply": "2025-07-17T10:08:25.147835Z"
    },
    "id": "Jd56qLlk99ge",
    "papermill": {
     "duration": 0.04325,
     "end_time": "2025-07-17T10:08:25.149431",
     "exception": false,
     "start_time": "2025-07-17T10:08:25.106181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title One Stage Facade\n",
    "class OneStageFacade():\n",
    "  def __init__(self, environment, actor, critic, params):\n",
    "    super().__init__()\n",
    "    self.device = device\n",
    "    self.env = environment\n",
    "    self.actor = actor\n",
    "    self.critic = critic\n",
    "\n",
    "    self.slate_size = params['slate_size']\n",
    "    self.noise_var = params['noise_var']\n",
    "    self.noise_decay = params['noise_var'] / params['n_iter'][-1]\n",
    "    self.q_laplace_smoothness = params['q_laplace_smoothness']\n",
    "    self.topk_rate = params['topk_rate']\n",
    "    self.empty_start_rate = params['empty_start_rate']\n",
    "\n",
    "    self.n_item = self.env.action_space['item_id'][1]\n",
    "\n",
    "    # (N)\n",
    "    self.candidate_iids = np.arange(1, self.n_item + 1)\n",
    "\n",
    "    # (N, item_dim)\n",
    "    self.candidate_features = torch.FloatTensor(\n",
    "        self.env.reader.get_item_list_meta(self.candidate_iids)).to(self.device)\n",
    "    self.candidate_iids = torch.tensor(self.candidate_iids).to(self.device)\n",
    "\n",
    "    # replay buffer is initialized in initialize_train()\n",
    "    self.buffer_size = params['buffer_size']\n",
    "    self.start_timestamp = params['start_timestamp']\n",
    "\n",
    "  def initialize_train(self):\n",
    "    '''\n",
    "    Procedures before training\n",
    "    '''\n",
    "    self.buffer = {\n",
    "        \"user_profile\": torch.zeros(self.buffer_size, self.env.reader.portrait_len),\n",
    "        \"history\":torch.zeros(self.buffer_size, self.env.reader.max_seq_len).to(torch.long),\n",
    "        \"next_history\":torch.zeros(self.buffer_size, self.env.reader.max_seq_len).to(torch.long),\n",
    "        \"state_emb\": torch.zeros(self.buffer_size, self.actor.state_dim),\n",
    "        \"action_emb\":torch.zeros(self.buffer_size, self.actor.action_dim),\n",
    "        \"action\":torch.zeros(self.buffer_size, self.slate_size, dtype=torch.long),\n",
    "        \"reward\":torch.zeros(self.buffer_size),\n",
    "        \"max_reward\": torch.zeros(self.buffer_size),\n",
    "        \"feedback\": torch.zeros(self.buffer_size, self.slate_size),\n",
    "        \"done\": torch.zeros(self.buffer_size, dtype=torch.bool)\n",
    "    }\n",
    "\n",
    "    for k, v in self.buffer.items():\n",
    "      self.buffer[k] = v.to(self.device)\n",
    "    self.buffer_head = 0\n",
    "    self.current_buffer_size = 0\n",
    "    self.n_stream_record = 0\n",
    "    self.is_training_available = False\n",
    "\n",
    "  def reset_env(self, initial_params = {'batch_size': 1}):\n",
    "    '''\n",
    "    Reset user response environment\n",
    "    '''\n",
    "    initial_params['empty_history'] = True if np.random.rand() < self.empty_start_rate else False\n",
    "    initial_observation = self.env.reset(initial_params)\n",
    "    return initial_observation\n",
    "\n",
    "  def env_step(self, policy_output):\n",
    "    action_dict = {\n",
    "      'action': policy_output['action'],\n",
    "      'action_features': policy_output['action_features']\n",
    "    }\n",
    "    observation, reward, done, info = self.env.step(action_dict)\n",
    "    return observation, reward, done, info\n",
    "\n",
    "  def stop_env(self):\n",
    "    self.env.stop()\n",
    "\n",
    "  def get_episode_report(self, n_recent = 10):\n",
    "    recent_rewards = self.env.reward_history[-n_recent:]\n",
    "    recent_steps = self.env.step_history[-n_recent:]\n",
    "    epsiode_report = {\n",
    "        'average_total_reward': np.mean(recent_rewards),\n",
    "        'reward_variance': np.var(recent_rewards),\n",
    "        'max_total_reward': np.max(recent_rewards),\n",
    "        'min_total_reward': np.min(recent_rewards),\n",
    "        'average_n_step': np.mean(recent_steps),\n",
    "        'max_n_step': np.max(recent_steps),\n",
    "        'min_n_step': np.min(recent_steps),\n",
    "        'buffer_size': self.current_buffer_size\n",
    "    }\n",
    "    return epsiode_report\n",
    "\n",
    "  def apply_critic(self, observation, policy_output, critic_model):\n",
    "    feed_dict = {\n",
    "        'state_emb': policy_output['state_emb'],\n",
    "        'action_emb': policy_output['action_emb']\n",
    "    }\n",
    "    critic_output = critic_model(feed_dict)\n",
    "    return critic_output\n",
    "\n",
    "  def apply_policy(self, observation, policy_model, epsilon = 0, \n",
    "                 do_explore = False, do_softmax = True):\n",
    "    '''\n",
    "    @input:\n",
    "    - observation: input of policy model\n",
    "    - policy_model\n",
    "    - epsilon: greedy epsilon, effective only when do_explore == True\n",
    "    - do_explore: exploration flag, True if adding noise to action\n",
    "    - do_softmax: output softmax score\n",
    "    '''\n",
    "#         feed_dict = utils.wrap_batch(observation, device = self.device)\n",
    "    feed_dict = observation\n",
    "    out_dict = policy_model(feed_dict)\n",
    "    if do_explore:\n",
    "        action_emb = out_dict['action_emb']\n",
    "        # sampling noise of action embedding\n",
    "        if np.random.rand() < epsilon:\n",
    "            action_emb = torch.clamp(torch.rand_like(action_emb)*self.noise_var, -1, 1)\n",
    "        else:\n",
    "            action_emb = action_emb + torch.clamp(torch.rand_like(action_emb)*self.noise_var, -1, 1)\n",
    "#                 self.noise_var -= self.noise_decay\n",
    "        out_dict['action_emb'] = action_emb\n",
    "        \n",
    "    if 'candidate_ids' in feed_dict:\n",
    "        # (B, L, item_dim)\n",
    "        out_dict['candidate_features'] = feed_dict['candidate_features']\n",
    "        # (B, L)\n",
    "        out_dict['candidate_ids'] = feed_dict['candidate_ids']\n",
    "        batch_wise = True\n",
    "    else:\n",
    "        # (1,L,item_dim)\n",
    "        out_dict['candidate_features'] = self.candidate_features.unsqueeze(0)\n",
    "        # (L,)\n",
    "        out_dict['candidate_ids'] = self.candidate_iids\n",
    "        batch_wise = False\n",
    "        \n",
    "    # action prob (B,L)\n",
    "    action_prob = policy_model.score(out_dict['action_emb'], \n",
    "                                     out_dict['candidate_features'], \n",
    "                                     do_softmax = do_softmax)\n",
    "\n",
    "    # two types of greedy selection\n",
    "    if np.random.rand() >= self.topk_rate:\n",
    "        # greedy random: categorical sampling\n",
    "        action, indices = utils.sample_categorical_action(action_prob, out_dict['candidate_ids'], \n",
    "                                                          self.slate_size, with_replacement = False, \n",
    "                                                          batch_wise = batch_wise, return_idx = True)\n",
    "    else:\n",
    "        # indices on action_prob\n",
    "        _, indices = torch.topk(action_prob, k = self.slate_size, dim = 1)\n",
    "        # topk action\n",
    "        if batch_wise:\n",
    "            action = torch.gather(out_dict['candidate_ids'], 1, indices).detach() # (B, slate_size)\n",
    "        else:\n",
    "            action = out_dict['candidate_ids'][indices].detach() # (B, slate_size)\n",
    "    # (B,K)\n",
    "    out_dict['action'] = action \n",
    "    # (B,K,item_dim)\n",
    "    out_dict['action_features'] = self.candidate_features[action-1]\n",
    "    # (B,K)\n",
    "    out_dict['action_prob'] = torch.gather(action_prob, 1, indices) \n",
    "    # (B,L)\n",
    "    out_dict['candidate_prob'] = action_prob\n",
    "    return out_dict\n",
    "\n",
    "  def sample_buffer(self, batch_size):\n",
    "    '''\n",
    "    @output:\n",
    "    - observation\n",
    "    - policy output\n",
    "    - reward\n",
    "    - done_mask\n",
    "    - next_observation\n",
    "    '''\n",
    "    indices = np.random.randint(0, self.current_buffer_size, size = batch_size)\n",
    "    U, H, N, S, HA, A, R, F, D, MR = self.read_buffer(indices)\n",
    "    observation = {\n",
    "        'user_profile': U,\n",
    "        'history_features': H,\n",
    "        'max_reward': MR\n",
    "    }\n",
    "    policy_output = {\n",
    "        'state_emb': S,\n",
    "        'action_emb': HA,\n",
    "        'action': A\n",
    "    }\n",
    "    reward = R\n",
    "    done_mask = D\n",
    "    next_observation = {\n",
    "        'user_profile': U,\n",
    "        'history_features': N,\n",
    "        'max_reward': MR,\n",
    "        'previous_feedback': F\n",
    "    }\n",
    "    return observation, policy_output, reward, done_mask, next_observation\n",
    "\n",
    "  # def sample_raw_data(self, batch_size):\n",
    "  #   '''\n",
    "  #   Sample supervise data from raw training data\n",
    "  #   '''\n",
    "  #   batch = self.env.sample_user(batch_size)\n",
    "\n",
    "  def update_buffer(self, observation, policy_output, reward, done_mask,\n",
    "                    next_observation, info):\n",
    "    # Overwrite old entries in buffer\n",
    "    if self.buffer_head + reward.shape[0] >= self.buffer_size:\n",
    "      tail = self.buffer_size - self.buffer_head\n",
    "      indices = [self.buffer_head + i for i in range(tail)] + \\\n",
    "       [i for i in range(reward.shape[0] - tail)]\n",
    "    else:\n",
    "      indices = [self.buffer_head  + i for i in range(reward.shape[0])]\n",
    "\n",
    "    # update buffer\n",
    "    \n",
    "    self.buffer[\"user_profile\"][indices] = observation['user_profile']\n",
    "    self.buffer[\"max_reward\"][indices] = observation['max_reward']\n",
    "    self.buffer[\"history\"][indices] = observation['history']\n",
    "    self.buffer[\"next_history\"][indices] = next_observation['history']\n",
    "    self.buffer[\"state_emb\"][indices] = policy_output['state_emb']\n",
    "    self.buffer[\"action\"][indices] = policy_output['action']\n",
    "    self.buffer[\"action_emb\"][indices] = policy_output['action_emb']\n",
    "    self.buffer[\"reward\"][indices] = reward\n",
    "    self.buffer[\"feedback\"][indices] = info['response']\n",
    "    self.buffer[\"done\"][indices] = done_mask\n",
    "\n",
    "    # update buffer pointer\n",
    "    self.buffer_head = (self.buffer_head + reward.shape[0]) % self.buffer_size\n",
    "    self.n_stream_record += reward.shape[0]\n",
    "    self.current_buffer_size = min(self.n_stream_record, self.buffer_size)\n",
    "\n",
    "    # available training when sufficient sample buffer\n",
    "    if self.n_stream_record >= self.start_timestamp:\n",
    "      self.is_training_available = True\n",
    "  def read_buffer(self, indices):\n",
    "    U = self.buffer['user_profile'][indices]\n",
    "    # (L, item_dim)\n",
    "    H = self.candidate_features[self.buffer[\"history\"][indices] - 1]\n",
    "    N = self.candidate_features[self.buffer[\"next_history\"][indices] - 1]\n",
    "    S = self.buffer[\"state_emb\"][indices]\n",
    "    HA = self.buffer[\"action_emb\"][indices]\n",
    "    A = self.buffer[\"action\"][indices]\n",
    "    R = self.buffer[\"reward\"][indices]\n",
    "    F = self.buffer[\"feedback\"][indices]\n",
    "    D = self.buffer[\"done\"][indices]\n",
    "    MR = self.buffer['max_reward'][indices]\n",
    "    return U, H, N, S, HA, A, R, F, D, MR\n",
    "\n",
    "  def extract_behavior_data(self, observation, policy_output, next_observation):\n",
    "    '''\n",
    "    Extract supervised data from RL samples\n",
    "    '''\n",
    "    observation = {\n",
    "        \"user_profile\": observation['user_profile'],\n",
    "        \"history_features\": observation['history_features']\n",
    "    }\n",
    "    exposed_items = policy_output['action']\n",
    "    exposure = {\n",
    "        \"ids\": exposed_items,\n",
    "        \"features\": self.candidate_features[exposed_items - 1]\n",
    "    }\n",
    "    user_feedback = next_observation[\"previous_feedback\"]\n",
    "    return observation, exposure, user_feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "752ce7ea",
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2025-07-17T10:08:25.189802Z",
     "iopub.status.busy": "2025-07-17T10:08:25.189542Z",
     "iopub.status.idle": "2025-07-17T10:08:25.198899Z",
     "shell.execute_reply": "2025-07-17T10:08:25.198406Z"
    },
    "id": "_H8nHNkrZdnk",
    "papermill": {
     "duration": 0.030775,
     "end_time": "2025-07-17T10:08:25.200049",
     "exception": false,
     "start_time": "2025-07-17T10:08:25.169274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title One Stage Facade with Hyper Action\n",
    "\n",
    "class OneStageFacade_HyperAction(OneStageFacade):\n",
    "  def __init__(self, environment, actor, critic, params):\n",
    "    super().__init__(environment, actor, critic, params)\n",
    "\n",
    "  def apply_policy(self, observation, policy_model, epsilon = 0,\n",
    "                   do_explore = False, do_softmax = True):\n",
    "    feed_dict = wrap_batch(observation, device=device)\n",
    "    # print(feed_dict.device)\n",
    "    out_dict = policy_model(feed_dict)\n",
    "    if do_explore:\n",
    "      action_emb = out_dict['action_emb']\n",
    "      # explore and exploit + clamping\n",
    "      if np.random.rand() < epsilon:\n",
    "        action_emb = torch.clamp(torch.rand_like(action_emb) * self.noise_var, -1, 1)\n",
    "      else:\n",
    "        action_emb = action_emb + torch.clamp(torch.rand_like(action_emb) * self.noise_var, -1, 1)\n",
    "\n",
    "      out_dict['action_emb'] = action_emb\n",
    "\n",
    "    # Z latent space\n",
    "    out_dict['Z'] = out_dict['action_emb']\n",
    "\n",
    "    if 'candidate_ids' in feed_dict:\n",
    "      # (B, L, item_dim)\n",
    "      out_dict['candidate_features']  = feed_dict['candidate_features']\n",
    "      # (B, L)\n",
    "      out_dict['candidate_ids'] = feed_dict['candidate_ids']\n",
    "      batch_wise = True\n",
    "    else:\n",
    "      # (1, L, item_dim)\n",
    "      out_dict['candidate_features'] = self.candidate_features.unsqueeze(0)\n",
    "      #(L, )\n",
    "      out_dict['candidate_ids'] = self.candidate_iids\n",
    "      batch_wise = False\n",
    "\n",
    "    # action pron (B, L)\n",
    "    action_prob = policy_model.score(out_dict['action_emb'],\n",
    "                                      out_dict['candidate_features'],\n",
    "                                      do_softmax=do_softmax)\n",
    "\n",
    "    # two types of greedy selection\n",
    "    if np.random.rand() >= self.topk_rate:\n",
    "      # greedy random\n",
    "      action, indices = sample_categorical_action(action_prob, out_dict['candidate_ids'],\n",
    "                                                  self.slate_size, with_replacement=False,\n",
    "                                                  batch_wise=batch_wise,\n",
    "                                                  return_idx=True)\n",
    "    else:\n",
    "      # indices on action_prob\n",
    "      _, indices = torch.topk(action_prob, k = self.slate_size, dim = 1)\n",
    "      # print(indices.shape)\n",
    "      # print(self.candidate_features.shape)\n",
    "      # top k action:\n",
    "      # (B, slate_size)\n",
    "      if batch_wise:\n",
    "        action = torch.gather(out_dict['candidate_ids'], 1, indices).detach()\n",
    "      else:\n",
    "        action = out_dict['candidate_ids'][indices].detach()\n",
    "\n",
    "    # (B, K)\n",
    "    out_dict['action'] = action\n",
    "    # (B, K, item_dim)\n",
    "    out_dict['action_features'] = self.candidate_features[indices]\n",
    "    # (B, K)\n",
    "    out_dict['action_prob'] = torch.gather(action_prob, 1, indices)\n",
    "    # (B, L)\n",
    "    out_dict['candidate_prob'] = action_prob\n",
    "\n",
    "    return out_dict\n",
    "\n",
    "  def infer_hyper_action(self, observation, policy_output, actor):\n",
    "    '''\n",
    "    Inverse function A -> Z\n",
    "    '''\n",
    "    # (B, K)\n",
    "    A = policy_output['action']\n",
    "\n",
    "    # (B, K, item_dim)\n",
    "    item_embs = self.candidate_features[A - 1]\n",
    "\n",
    "    # (B, K, kernel_dim)\n",
    "    Z = torch.mean(actor.item_map(item_embs).view(A.shape[0], A.shape[1], -1), dim = 1)\n",
    "    return {\n",
    "        'Z': Z,\n",
    "        'action_emb': Z,\n",
    "        'state_emb': policy_output['state_emb']\n",
    "    }\n",
    "\n",
    "  def apply_critic(self, observation, policy_output, critic_model):\n",
    "    feed_dict = {\n",
    "        'state_emb': policy_output['state_emb'],\n",
    "        'action_emb': policy_output['action_emb']\n",
    "    }\n",
    "    critic_output = critic_model(feed_dict)\n",
    "    return critic_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4223d7d6",
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2025-07-17T10:08:25.238919Z",
     "iopub.status.busy": "2025-07-17T10:08:25.238702Z",
     "iopub.status.idle": "2025-07-17T10:08:25.249595Z",
     "shell.execute_reply": "2025-07-17T10:08:25.248856Z"
    },
    "id": "wJKb3K_326B5",
    "papermill": {
     "duration": 0.031483,
     "end_time": "2025-07-17T10:08:25.250637",
     "exception": false,
     "start_time": "2025-07-17T10:08:25.219154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title Base RL Agent\n",
    "\n",
    "class BaseRLAgent():\n",
    "  def __init__(self, facade, params):\n",
    "    self.device = params['device']\n",
    "    self.gamma = params['gamma']\n",
    "    self.n_iter = [0] + params['n_iter']\n",
    "    self.train_every_n_step = params['train_every_n_step']\n",
    "    self.check_episode = params['check_episode']\n",
    "    self.save_path = params['save_path']\n",
    "    self.facade = facade\n",
    "    self.check_episode = params['check_episode']\n",
    "    self.exploration_scheduler = LinearScheduler(int(sum(self.n_iter) * params['elbow_greedy']),\n",
    "                                                 params['final_greedy_epsilon'],\n",
    "                                                 params['initial_greedy_epsilon'])\n",
    "    # if len(self.n_iter) == 2:\n",
    "    #   with open(self.save_path + \".report\", 'w') as outfile:\n",
    "    #     outfile.write()\n",
    "\n",
    "  def train(self):\n",
    "    if len(self.n_iter) > 2:\n",
    "      self.load()\n",
    "\n",
    "    t = time()\n",
    "    start_time = t\n",
    "    print(\"Run procedure before training\")\n",
    "    self.action_before_train()\n",
    "\n",
    "    print(\"Start training\")\n",
    "    observation = self.facade.reset_env({\n",
    "        'batch_size': self.episode_batch_size,\n",
    "    })\n",
    "    step_offset = sum(self.n_iter[:-1])\n",
    "    for i in tqdm(range(step_offset, step_offset + self.n_iter[-1])):\n",
    "      observation = self.run_episode_step(i, self.exploration_scheduler.value(i),\n",
    "                                          observation, True)\n",
    "      if i % self.train_every_n_step == 0:\n",
    "        self.step_train()\n",
    "\n",
    "      if i % self.check_episode == 0:\n",
    "        t_ = time()\n",
    "        # print(f\"Episode step {i}, time diff {t_ - t}, total time dif {t - start_time})\")\n",
    "        self.log_iteration(i)\n",
    "        t = t_\n",
    "        if i % (3*self.check_episode) == 0:\n",
    "            self.save()\n",
    "\n",
    "    self.action_after_train()\n",
    "\n",
    "\n",
    "  def action_before_train(self):\n",
    "    pass\n",
    "\n",
    "  def action_after_train(self):\n",
    "    self.facade.stop_env()\n",
    "\n",
    "\n",
    "  def get_report(self):\n",
    "    episode_report = self.facade.get_episode_report(10)\n",
    "    train_report = {k: np.mean(v[-10:]) for k, v in self.training_history.items()}\n",
    "    return episode_report, train_report\n",
    "\n",
    "  def log_iteration(self, step):\n",
    "    episode_report, train_report = self.get_report()\n",
    "    run.log(episode_report | train_report)\n",
    "    log_str = f\"step: {step} @ episode report: {episode_report} @ step loss: {train_report}\\n\"\n",
    "    with open(self.save_path + \".report\", 'a') as outfile:\n",
    "        outfile.write(log_str)\n",
    "    return log_str\n",
    "\n",
    "  def test(self):\n",
    "    self.load()\n",
    "    self.facade.initialize_train()\n",
    "\n",
    "    t = time()\n",
    "    start_time = t\n",
    "\n",
    "    print(\"Start testing\")\n",
    "    observation = self.facade.reset_env({\n",
    "        'batch_size': self.episode_batch_size,\n",
    "    })\n",
    "    step_offset = sum(self.n_iter[:-1])\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(step_offset, step_offset + self.n_iter[-1])):\n",
    "          observation = self.run_episode_step(i, self.exploration_scheduler.value(i),\n",
    "                                              observation, True)\n",
    "          if i % self.check_episode == 0:\n",
    "            t_ = time()\n",
    "            episode_report = self.facade.get_episode_report(10)\n",
    "            log_str = f\"step: {i} @ episode report: {episode_report}\\n\"\n",
    "            run.log(episode_report)\n",
    "            with open(self.save_path + \"_eval.report\", 'a') as outfile:\n",
    "              outfile.write(log_str)\n",
    "            # print(f\"Episode step {i}, time diff {t_ - t}, total time dif {t - start_time})\")\n",
    "            # print(log_str)\n",
    "            t = t_\n",
    "    \n",
    "\n",
    "  #######################################\n",
    "  #           Abstract function         #\n",
    "  #######################################\n",
    "  def run_episode_step(self, *episode_args):\n",
    "    pass\n",
    "\n",
    "  def step_train(self):\n",
    "    pass\n",
    "\n",
    "  def save(self):\n",
    "    pass\n",
    "\n",
    "  def load(self):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "412c420b",
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2025-07-17T10:08:25.289805Z",
     "iopub.status.busy": "2025-07-17T10:08:25.289615Z",
     "iopub.status.idle": "2025-07-17T10:08:25.303812Z",
     "shell.execute_reply": "2025-07-17T10:08:25.303138Z"
    },
    "id": "VBbJXf-KfvDA",
    "papermill": {
     "duration": 0.035383,
     "end_time": "2025-07-17T10:08:25.304922",
     "exception": false,
     "start_time": "2025-07-17T10:08:25.269539",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title Deep Deterministic Policy Gradient\n",
    "\n",
    "\n",
    "class DDPG(BaseRLAgent):\n",
    "  def __init__(self, facade, params):\n",
    "    super().__init__(facade, params)\n",
    "    self.actor = facade.actor\n",
    "    self.actor_target = copy.deepcopy(self.actor)\n",
    "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr = params['actor_lr'], weight_decay = params['actor_decay'])\n",
    "\n",
    "    self.critic = facade.critic\n",
    "    self.critic_target = copy.deepcopy(self.critic)\n",
    "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr = params['critic_lr'], weight_decay = params['critic_decay'])\n",
    "\n",
    "    self.episode_batch_size = params['episode_batch_size']\n",
    "    self.tau = params['target_mitigate_coef']\n",
    "    self.actor_lr = params['actor_lr']\n",
    "    self.critic_lr = params['critic_lr']\n",
    "    self.actor_decay = params['actor_decay']\n",
    "    self.critic_decay = params['critic_decay']\n",
    "\n",
    "    self.batch_size = params['batch_size']\n",
    "\n",
    "    with open(self.save_path + \".report\", 'w') as outfile:\n",
    "      pass\n",
    "\n",
    "  def action_before_train(self):\n",
    "    '''\n",
    "    - facade setup\n",
    "      - buffer setup\n",
    "    - run random episodes to build-up the initial buffer\n",
    "    '''\n",
    "    self.facade.initialize_train()\n",
    "    # print(\"Facade Parameters:\")\n",
    "    # for param, value in vars(self.facade).items():\n",
    "    #     print(f\"{param}: {value}\")\n",
    "    prepare_step = 0\n",
    "    # random explore before training\n",
    "    initial_epsilon = 1.0\n",
    "    observation = self.facade.reset_env({\n",
    "        'batch_size': self.episode_batch_size,\n",
    "    })\n",
    "    while not self.facade.is_training_available:\n",
    "      observation = self.run_episode_step(0, initial_epsilon, observation, True)\n",
    "      # print(observation)\n",
    "      prepare_step += 1\n",
    "\n",
    "    # training records\n",
    "    self.training_history = {\"critic_loss\": [], \"actor_loss\": []}\n",
    "\n",
    "    print(f\"Total {prepare_step} prepare steps\")\n",
    "\n",
    "  def run_episode_step(self, *episode_args):\n",
    "    '''\n",
    "    One step of interaction\n",
    "    '''\n",
    "    episode_iter, epsilon, observation, do_buffer_update = episode_args\n",
    "    with torch.no_grad():\n",
    "      # sample action\n",
    "      policy_output = self.facade.apply_policy(observation, self.actor, epsilon,\n",
    "                                               do_explore=True)\n",
    "\n",
    "      # apply action on environment and update replay buffer\n",
    "      next_observation, reward, done, info = self.facade.env_step(policy_output)\n",
    "\n",
    "      # update replay buffer\n",
    "      if do_buffer_update:\n",
    "        self.facade.update_buffer(observation, policy_output, reward, done,\n",
    "                                  next_observation, info)\n",
    "    return next_observation\n",
    "\n",
    "  def step_train(self):\n",
    "    observation , policy_output, reward, done_mask, next_observation = self.facade.sample_buffer(params['batch_size'])\n",
    "\n",
    "    critic_loss, actor_loss = self.get_ddpg_loss(observation, policy_output, reward,\n",
    "                                                  done_mask, next_observation)\n",
    "    self.training_history[\"critic_loss\"].append(critic_loss.item())\n",
    "    self.training_history[\"actor_loss\"].append(actor_loss.item())\n",
    "\n",
    "    # Update the frozen target models\n",
    "    for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "      target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)\n",
    "\n",
    "    for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "      target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)\n",
    "\n",
    "    return {'step_loss': (self.training_history['actor_loss'][-1],\n",
    "                          self.training_history['critic_loss'][-1])}\n",
    "\n",
    "  def get_ddpg_loss(self, observation, policy_output, reward, done_mask, next_observation,\n",
    "                    do_actor_update = True, do_critic_update = True):\n",
    "    # Get current Q estimate\n",
    "    current_critic_output = self.facade.apply_critic(observation,\n",
    "                                                     wrap_batch(policy_output, device=self.device),\n",
    "                                                     self.critic)\n",
    "    current_Q = current_critic_output['q']\n",
    "\n",
    "    # Compute the target Q value\n",
    "    next_policy_output = self.facade.apply_policy(next_observation, self.actor_target)\n",
    "    target_critic_output = self.facade.apply_critic(next_observation, next_policy_output,\n",
    "                                                    self.critic_target)\n",
    "\n",
    "    target_Q = target_critic_output['q']\n",
    "    target_Q = reward + self.gamma * (done_mask * target_Q).detach()\n",
    "\n",
    "    # compute critic loss\n",
    "    # minimize current_Q predict and target_Q predict\n",
    "    critic_loss = F.mse_loss(current_Q, target_Q).mean()\n",
    "\n",
    "    if do_critic_update and self.critic_lr > 0:\n",
    "      # Optimize the critic\n",
    "      self.critic_optimizer.zero_grad()\n",
    "      critic_loss.backward()\n",
    "      self.critic_optimizer.step()\n",
    "\n",
    "    # compute actor loss\n",
    "    policy_output = self.facade.apply_policy(observation, self.actor)\n",
    "    critic_output = self.facade.apply_critic(observation, policy_output, self.critic)\n",
    "\n",
    "    # Maximize Q value\n",
    "    actor_loss = -critic_output['q'].mean()\n",
    "\n",
    "    if do_actor_update and self.actor_lr > 0:\n",
    "      # Optimize the actor\n",
    "      self.actor_optimizer.zero_grad()\n",
    "      actor_loss.backward()\n",
    "      self.actor_optimizer.step()\n",
    "    return critic_loss, actor_loss\n",
    "\n",
    "  def save(self):\n",
    "    torch.save(self.critic.state_dict(), self.save_path + \"_critic\")\n",
    "    torch.save(self.critic_optimizer.state_dict(), self.save_path + \"_critic_optimizer\")\n",
    "    torch.save(self.actor.state_dict(), self.save_path + \"_actor\")\n",
    "    torch.save(self.actor_optimizer.state_dict(), self.save_path + \"_actor_optimizer\")\n",
    "\n",
    "  def load(self):\n",
    "    self.critic.load_state_dict(torch.load(self.save_path + \"_critic\", map_location=self.device))\n",
    "    self.critic_optimizer.load_state_dict(torch.load(self.save_path + \"_critic_optimizer\", map_location=self.device))\n",
    "    self.critic_target = copy.deepcopy(self.critic)\n",
    "\n",
    "    self.actor.load_state_dict(torch.load(self.save_path + \"_actor\", map_location=self.device))\n",
    "    self.actor_optimizer.load_state_dict(torch.load(self.save_path + \"_actor_optimizer\", map_location=self.device))\n",
    "    self.actor_target = copy.deepcopy(self.actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a7259da",
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2025-07-17T10:08:25.344193Z",
     "iopub.status.busy": "2025-07-17T10:08:25.343833Z",
     "iopub.status.idle": "2025-07-17T10:08:25.357890Z",
     "shell.execute_reply": "2025-07-17T10:08:25.357199Z"
    },
    "id": "HLBYcDpMSfuw",
    "papermill": {
     "duration": 0.034862,
     "end_time": "2025-07-17T10:08:25.358892",
     "exception": false,
     "start_time": "2025-07-17T10:08:25.324030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# @title Hyper - Actor Critic\n",
    "class HAC(DDPG):\n",
    "  def __init__(self, facade, params):\n",
    "    super().__init__(facade, params)\n",
    "    self.behavior_lr = params['behavior_lr']\n",
    "    self.behavior_decay = params['behavior_decay']\n",
    "    self.hyper_actor_coef = params['hyper_actor_coef']\n",
    "    self.actor_behavior_optimizer = torch.optim.Adam(self.actor.parameters(),\n",
    "                                                     lr=params['behavior_lr'],\n",
    "                                                     weight_decay=params['behavior_decay'])\n",
    "\n",
    "  def action_before_train(self):\n",
    "    super().action_before_train()\n",
    "    self.training_history['hyper_actor_loss'] = []\n",
    "    self.training_history['behavior_loss'] = []\n",
    "\n",
    "  def run_episode_step(self, *episode_args):\n",
    "    '''\n",
    "    One step of interaction\n",
    "    '''\n",
    "    episode_iter, epsilon, observation, do_buffer_update = episode_args\n",
    "\n",
    "    with torch.no_grad():\n",
    "      # sample action\n",
    "      policy_output = self.facade.apply_policy(observation, self.actor, epsilon,\n",
    "                                               do_explore=True)\n",
    "\n",
    "      # apply action on environment and update replay buffer\n",
    "      next_observation, reward, done, info = self.facade.env_step(policy_output)\n",
    "\n",
    "      # update replay buffer\n",
    "      if do_buffer_update:\n",
    "        self.facade.update_buffer(observation, policy_output, reward, done,\n",
    "                                  next_observation, info)\n",
    "    return next_observation\n",
    "\n",
    "  def step_train(self):\n",
    "    observation , policy_output, reward, done_mask, next_observation = self.facade.sample_buffer(params['batch_size'])\n",
    "    # reward  = torch.FloatTensor(reward)\n",
    "    # done_mask = torch.FloatTensor(done_mask)\n",
    "\n",
    "    critic_loss, actor_loss, hyper_actor_loss = self.get_hac_loss(observation, policy_output, reward,\n",
    "                                                  done_mask, next_observation)\n",
    "    behavior_loss = self.get_behavior_loss(observation, policy_output, next_observation)\n",
    "\n",
    "    self.training_history[\"critic_loss\"].append(critic_loss.item())\n",
    "    self.training_history[\"actor_loss\"].append(actor_loss.item())\n",
    "    self.training_history['hyper_actor_loss'].append(hyper_actor_loss.item())\n",
    "    self.training_history['behavior_loss'].append(behavior_loss.item())\n",
    "\n",
    "    # Update frozen target models\n",
    "    for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "      target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)\n",
    "\n",
    "    for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "      target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)\n",
    "\n",
    "    return {\"step_loss\": (self.training_history['actor_loss'][-1],\n",
    "                          self.training_history['critic_loss'][-1],\n",
    "                          self.training_history['hyper_actor_loss'][-1],\n",
    "                          self.training_history['behavior_loss'][-1])}\n",
    "\n",
    "  def get_hac_loss(self, observation, policy_output, reward, done_mask, next_observation,\n",
    "                    do_actor_update = True, do_critic_update = True):\n",
    "\n",
    "\n",
    "    # nsw reward\n",
    "    cummulative_r = reward\n",
    "    max_r = observation['max_reward']\n",
    "    max_r = torch.where(torch.isinf(max_r), torch.zeros_like(max_r), max_r)\n",
    "    \n",
    "    # Current Q estimate\n",
    "    hyper_output = self.facade.infer_hyper_action(observation, policy_output, self.actor)\n",
    "    current_critic_output = self.facade.apply_critic(observation, hyper_output, self.critic)\n",
    "    current_Q = current_critic_output['q']\n",
    "\n",
    "    # Compute target Q value\n",
    "    next_policy_output = self.facade.apply_policy(next_observation, self.actor_target)\n",
    "    target_critic_output = self.facade.apply_critic(next_observation, next_policy_output, self.critic_target)\n",
    "\n",
    "    target_Q = target_critic_output['q']\n",
    "    target_Q = reward + self.gamma * ((1 - done_mask.float()) * target_Q).detach()\n",
    "\n",
    "    critic_loss = F.mse_loss(current_Q, target_Q).mean()\n",
    "\n",
    "    # critic loss\n",
    "    if do_critic_update and self.critic_lr > 0:\n",
    "      self.critic_optimizer.zero_grad()\n",
    "      critic_loss.backward()\n",
    "      self.critic_optimizer.step()\n",
    "\n",
    "    # actor loss\n",
    "\n",
    "    if do_actor_update and self.actor_lr > 0:\n",
    "      self.actor_optimizer.zero_grad()\n",
    "      policy_output = self.facade.apply_policy(observation, self.actor)\n",
    "      critic_output = self.facade.apply_critic(observation, policy_output, self.critic)\n",
    "      actor_loss = -nsw(critic_output['q'], max_r).mean()\n",
    "      actor_loss.backward()\n",
    "      self.actor_optimizer.step()\n",
    "\n",
    "    # hyper actor loss\n",
    "\n",
    "    if do_actor_update and self.hyper_actor_coef > 0:\n",
    "      self.actor_optimizer.zero_grad()\n",
    "      policy_output = self.facade.apply_policy(observation, self.actor)\n",
    "      inferred_hyper_output = self.facade.infer_hyper_action(observation, policy_output, self.actor)\n",
    "      hyper_actor_loss = self.hyper_actor_coef * F.mse_loss(inferred_hyper_output['Z'],\n",
    "                                                            policy_output['Z']).mean()\n",
    "\n",
    "      hyper_actor_loss.backward()\n",
    "      self.actor_optimizer.step()\n",
    "\n",
    "    return critic_loss, actor_loss, hyper_actor_loss\n",
    "\n",
    "  def get_behavior_loss(self, observation, policy_output, next_observation, do_update = True):\n",
    "    observation, exposure, feedback = self.facade.extract_behavior_data(observation, policy_output, next_observation)\n",
    "    observation['candidate_ids'] = exposure['ids']\n",
    "    observation['candidate_features'] = exposure['features']\n",
    "    policy_output = self.facade.apply_policy(observation, self.actor, do_softmax=False)\n",
    "    action_prob = torch.sigmoid(policy_output['candidate_prob'])\n",
    "    behavior_loss = F.binary_cross_entropy(action_prob, feedback)\n",
    "\n",
    "    if do_update and self.behavior_lr > 0:\n",
    "      self.actor_behavior_optimizer.zero_grad()\n",
    "      behavior_loss.backward()\n",
    "      self.actor_behavior_optimizer.step()\n",
    "\n",
    "    return behavior_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20500dfb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T10:08:25.397756Z",
     "iopub.status.busy": "2025-07-17T10:08:25.397549Z",
     "iopub.status.idle": "2025-07-17T10:39:44.087200Z",
     "shell.execute_reply": "2025-07-17T10:39:44.086344Z"
    },
    "papermill": {
     "duration": 1878.71114,
     "end_time": "2025-07-17T10:39:44.088880",
     "exception": false,
     "start_time": "2025-07-17T10:08:25.377740",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m23020082\u001b[0m (\u001b[33m23020082-uet\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250717_100825-y7zou8w7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdivine-brook-123\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/23020082-uet/HAC\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/23020082-uet/HAC/runs/y7zou8w7\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init kr reader\n",
      "Load item meta data\n",
      "{'length': 72757, 'n_item': 11643, 'item_vec_size': 32, 'user_portrait_len': 32, 'max_seq_len': 50, 'n_feedback': 2}\n",
      "Run procedure before training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/4185689321.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(params['model_path'] + \".checkpoint\", map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 63 prepare steps\n",
      "Start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50000/50000 [31:10<00:00, 26.73it/s]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           actor_loss \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       average_n_step \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: average_total_reward \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          buffer_size \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          critic_loss \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           max_n_step \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     max_total_reward \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           min_n_step \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     min_total_reward \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      reward_variance \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           actor_loss -0.9728\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       average_n_step 7.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: average_total_reward 5.803\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          buffer_size 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          critic_loss 0.15565\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           max_n_step 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     max_total_reward 13.45\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           min_n_step 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     min_total_reward -0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      reward_variance 24.69184\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run \u001b[33mdivine-brook-123\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/23020082-uet/HAC/runs/y7zou8w7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at: \u001b[34m\u001b[4mhttps://wandb.ai/23020082-uet/HAC\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250717_100825-y7zou8w7/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# KR setting\n",
    "\n",
    "params['n_worker'] = 4\n",
    "params['max_seq_len'] = 50\n",
    "\n",
    "params['loss_type'] = 'bce'\n",
    "params['device'] = device\n",
    "params['l2_coef'] = 0.001\n",
    "params['lr'] = 0.0003\n",
    "params['feature_dim'] = 16\n",
    "params['hidden_dims'] = [256]\n",
    "params['attn_n_head'] = 2\n",
    "params['batch_size'] = 128\n",
    "params['epoch'] = 2\n",
    "params['dropout_rate'] = 0.2\n",
    "params['max_step'] = 20\n",
    "params['initial_temper'] = 20\n",
    "params['reward_function'] = mean_with_cost\n",
    "params['sasrec_n_layer'] = 2\n",
    "params['sasrec_d_model'] = 32\n",
    "params['sasrec_n_head'] = 4\n",
    "params['sasrec_dropout'] = 0.1\n",
    "params['sasrec_d_forward'] = 64\n",
    "params['critic_hidden_dims'] = [256, 64]\n",
    "params['critic_dropout_rate'] = 0.2\n",
    "params['n_iter']= [50000]\n",
    "params['slate_size'] = 10\n",
    "params['noise_var'] = 0.1\n",
    "params['q_laplace_smoothness'] = 0.5\n",
    "params['topk_rate'] = 1\n",
    "params['empty_start_rate'] = 0\n",
    "params['buffer_size'] = 100000\n",
    "params['start_timestamp'] = 2000\n",
    "params['gamma'] = 0.9\n",
    "params['train_every_n_step']= 1\n",
    "params['initial_greedy_epsilon'] = 0\n",
    "params['final_greedy_epsilon'] = 0\n",
    "params['elbow_greedy'] = 0.1\n",
    "params['check_episode'] = 10\n",
    "params['with_eval'] = False\n",
    "\n",
    "params['episode_batch_size'] = 32\n",
    "params['batch_size'] = 64\n",
    "params['actor_lr'] = 0.00001\n",
    "params['critic_lr'] = 5e-4\n",
    "params['actor_decay'] = 0.00001\n",
    "params['critic_decay'] = 0.00001\n",
    "params['target_mitigate_coef'] = 0.01\n",
    "params['behavior_lr'] = 0.00005\n",
    "params['behavior_decay'] = 0.00001\n",
    "params['hyper_actor_coef'] = 1e-3\n",
    "params['advantage_bias'] = 0\n",
    "params['entropy_coef'] = 0.0001\n",
    "config = params.copy()\n",
    "config.pop(\"train\", None)\n",
    "config.pop(\"val\", None)\n",
    "config.pop(\"item_meta\", None)\n",
    "config.pop(\"user_meta\", None)\n",
    "\n",
    "for seed in [7]:\n",
    "    params['seed'] = seed\n",
    "    set_random_seed(params['seed'])\n",
    "    params['save_path'] = os.path.join(path_to_output, f\"agent/kr_model_seed{params['seed']}\")\n",
    "    run = wandb.init(\n",
    "    # Set the wandb entity where your project will be logged (generally your team name).\n",
    "    entity=\"23020082-uet\",\n",
    "    # Set the wandb project where this run will be logged.\n",
    "    project=\"HAC\",\n",
    "    # Track hyperparameters and run metadata.\n",
    "    config=config\n",
    "    )\n",
    "    os.makedirs(os.path.dirname(params['save_path']), exist_ok=True)\n",
    "    \n",
    "    env = KREnvironment(params)\n",
    "    \n",
    "    policy = SASRec(env, params)\n",
    "    policy.to(device)\n",
    "    \n",
    "    \n",
    "    critic = GeneralCritic(policy, params)\n",
    "    critic.to(device)\n",
    "    \n",
    "    facade = OneStageFacade(env, policy, critic, params)\n",
    "    \n",
    "    agent = DDPG(facade, params)\n",
    "    \n",
    "    agent.train()\n",
    "    run.finish()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7440a7d",
   "metadata": {
    "papermill": {
     "duration": 0.772016,
     "end_time": "2025-07-17T10:39:45.599809",
     "exception": false,
     "start_time": "2025-07-17T10:39:44.827793",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb2e3405",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T10:39:47.067114Z",
     "iopub.status.busy": "2025-07-17T10:39:47.066733Z",
     "iopub.status.idle": "2025-07-17T10:39:48.537331Z",
     "shell.execute_reply": "2025-07-17T10:39:48.536718Z"
    },
    "papermill": {
     "duration": 2.22295,
     "end_time": "2025-07-17T10:39:48.538418",
     "exception": false,
     "start_time": "2025-07-17T10:39:46.315468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>sequence_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>96532.000000</td>\n",
       "      <td>96532.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>489.196204</td>\n",
       "      <td>75.844321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>288.493175</td>\n",
       "      <td>67.271119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>233.000000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>489.000000</td>\n",
       "      <td>58.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>744.000000</td>\n",
       "      <td>107.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>985.000000</td>\n",
       "      <td>446.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            user_id   sequence_id\n",
       "count  96532.000000  96532.000000\n",
       "mean     489.196204     75.844321\n",
       "std      288.493175     67.271119\n",
       "min        0.000000      0.000000\n",
       "25%      233.000000     25.000000\n",
       "50%      489.000000     58.000000\n",
       "75%      744.000000    107.000000\n",
       "max      985.000000    446.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>sequence_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>23775.000000</td>\n",
       "      <td>23775.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>489.122944</td>\n",
       "      <td>134.036383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>288.520211</td>\n",
       "      <td>77.999722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>233.000000</td>\n",
       "      <td>77.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>489.000000</td>\n",
       "      <td>118.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>744.000000</td>\n",
       "      <td>176.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>985.000000</td>\n",
       "      <td>446.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            user_id   sequence_id\n",
       "count  23775.000000  23775.000000\n",
       "mean     489.122944    134.036383\n",
       "std      288.520211     77.999722\n",
       "min        0.000000      4.000000\n",
       "25%      233.000000     77.000000\n",
       "50%      489.000000    118.000000\n",
       "75%      744.000000    176.000000\n",
       "max      985.000000    446.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(11643, 32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(986, 32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title Load data for ml1m\n",
    "item_info = np.load(os.path.join(path_to_data, \"item_info.npy\"))\n",
    "user_info = np.load(os.path.join(path_to_data, \"user_info.npy\"))\n",
    "train = pd.read_csv(os.path.join(path_to_data, \"all.csv\"), sep=\"@\")\n",
    "test = pd.read_csv(os.path.join(path_to_data, \"test.csv\"), sep=\"@\")\n",
    "display(train.describe())\n",
    "display(test.describe())\n",
    "display(item_info.shape)\n",
    "display(user_info.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "76469cef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T10:39:49.931868Z",
     "iopub.status.busy": "2025-07-17T10:39:49.931388Z",
     "iopub.status.idle": "2025-07-17T10:40:37.816852Z",
     "shell.execute_reply": "2025-07-17T10:40:37.815978Z"
    },
    "papermill": {
     "duration": 48.551631,
     "end_time": "2025-07-17T10:40:37.818167",
     "exception": false,
     "start_time": "2025-07-17T10:39:49.266536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init kr reader\n",
      "Load item meta data\n",
      "{'length': 96532, 'n_item': 11643, 'item_vec_size': 32, 'user_portrait_len': 32, 'max_seq_len': 50, 'n_feedback': 2}\n",
      "epoch 0 is training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "96640it [00:19, 5038.29it/s]                           \n",
      "23808it [00:04, 5468.99it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 validating; auc: 0.6837\n",
      "epoch 1 is training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "96640it [00:19, 4866.78it/s]                           \n",
      "23808it [00:04, 5482.69it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 validating; auc: 0.6856\n"
     ]
    }
   ],
   "source": [
    "params = dict()\n",
    "params['train'] = train\n",
    "params['val'] = test\n",
    "params['item_meta'] = item_info\n",
    "params['user_meta'] = user_info\n",
    "params['n_worker'] = 4\n",
    "params['max_seq_len'] = 50\n",
    "\n",
    "params['loss_type'] = 'bce'\n",
    "params['device'] = device\n",
    "params['l2_coef'] = 0.001\n",
    "params['lr'] = 0.0003\n",
    "params['feature_dim'] = 16\n",
    "params['hidden_dims'] = [256]\n",
    "params['attn_n_head'] = 2\n",
    "params['batch_size'] = 128\n",
    "params['seed'] = 13\n",
    "params['epoch'] = 2\n",
    "params['dropout_rate'] = 0.2\n",
    "params['model_path'] = os.path.join(path_to_output, \n",
    "                          f\"env/kr_env_lr{params['lr']}_reg{params['l2_coef']}_eval.model\")\n",
    "set_random_seed(params['seed'])\n",
    "\n",
    "# @title Train user response\n",
    "reader = KRDataReader(params)\n",
    "model = KRUserResponse(reader, params).to(device)\n",
    "\n",
    "\n",
    "# reader = RL4RSDataReader(params)\n",
    "# model = RL4RSUserResponse(reader, params).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "model.optimizer = optimizer\n",
    "\n",
    "\n",
    "epo = 0\n",
    "while epo < params['epoch']:\n",
    "  print(f\"epoch {epo} is training\")\n",
    "  epo += 1\n",
    "\n",
    "  model.train()\n",
    "  reader.set_phase(\"train\")\n",
    "  train_loader = DataLoader(reader, params['batch_size'], shuffle = True, pin_memory = True,\n",
    "                            num_workers= params['n_worker'])\n",
    "\n",
    "  t1 = time()\n",
    "  pbar = tqdm(total=len(train_loader.dataset))\n",
    "  step_loss = []\n",
    "  for i, batch_data in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    wrapped_batch = wrap_batch(batch_data, device)\n",
    "\n",
    "    out_dict = model.do_forward_and_loss(wrapped_batch)\n",
    "    loss = out_dict['loss']\n",
    "    loss.backward()\n",
    "    step_loss.append(loss.item())\n",
    "    optimizer.step()\n",
    "    pbar.update(params['batch_size'])\n",
    "    # print(model.loss)\n",
    "    # if (i + 1) % 10 == 0:\n",
    "      # print(f\"Iteration {i + 1}, loss {np.mean(step_loss[-100:])}\")\n",
    "  pbar.close()\n",
    "    # print(\"Epoch {}; time {:.4f}\".format(epo, time() - t1))\n",
    "\n",
    "  # validation\n",
    "  t2 = time()\n",
    "  reader.set_phase(\"val\")\n",
    "  val_loader = DataLoader(reader, params['batch_size'], shuffle = False, pin_memory = False,\n",
    "                          num_workers= params['n_worker'])\n",
    "  valid_probs, valid_true =  [], []\n",
    "  pbar = tqdm(total = len(val_loader.dataset))\n",
    "  with torch.no_grad():\n",
    "    for i, batch_data in enumerate(val_loader):\n",
    "      wrapped_batch = wrap_batch(batch_data, device)\n",
    "      out_dict = model.forward(wrapped_batch)\n",
    "      valid_probs.append(out_dict['probs'].cpu().numpy())\n",
    "      valid_true.append(batch_data['feedback'].cpu().numpy())\n",
    "      pbar.update(params['batch_size'])\n",
    "  pbar.close()\n",
    "  auc = roc_auc_score(np.concatenate(valid_true), np.concatenate(valid_probs))\n",
    "  print(f\"epoch {epo} validating\" + \"; auc: {:.4f}\".format(np.mean(auc)))\n",
    "  model.save_checkpoint()\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "92874c73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T10:40:39.236623Z",
     "iopub.status.busy": "2025-07-17T10:40:39.235852Z",
     "iopub.status.idle": "2025-07-17T10:40:39.654946Z",
     "shell.execute_reply": "2025-07-17T10:40:39.654320Z"
    },
    "papermill": {
     "duration": 1.166463,
     "end_time": "2025-07-17T10:40:39.656044",
     "exception": false,
     "start_time": "2025-07-17T10:40:38.489581",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>sequence_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>23775.000000</td>\n",
       "      <td>23775.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>489.122944</td>\n",
       "      <td>134.036383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>288.520211</td>\n",
       "      <td>77.999722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>233.000000</td>\n",
       "      <td>77.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>489.000000</td>\n",
       "      <td>118.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>744.000000</td>\n",
       "      <td>176.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>985.000000</td>\n",
       "      <td>446.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            user_id   sequence_id\n",
       "count  23775.000000  23775.000000\n",
       "mean     489.122944    134.036383\n",
       "std      288.520211     77.999722\n",
       "min        0.000000      4.000000\n",
       "25%      233.000000     77.000000\n",
       "50%      489.000000    118.000000\n",
       "75%      744.000000    176.000000\n",
       "max      985.000000    446.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>sequence_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>23775.000000</td>\n",
       "      <td>23775.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>489.122944</td>\n",
       "      <td>134.036383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>288.520211</td>\n",
       "      <td>77.999722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>233.000000</td>\n",
       "      <td>77.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>489.000000</td>\n",
       "      <td>118.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>744.000000</td>\n",
       "      <td>176.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>985.000000</td>\n",
       "      <td>446.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            user_id   sequence_id\n",
       "count  23775.000000  23775.000000\n",
       "mean     489.122944    134.036383\n",
       "std      288.520211     77.999722\n",
       "min        0.000000      4.000000\n",
       "25%      233.000000     77.000000\n",
       "50%      489.000000    118.000000\n",
       "75%      744.000000    176.000000\n",
       "max      985.000000    446.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(11643, 32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(986, 32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title Load data for ml1m\n",
    "item_info = np.load(os.path.join(path_to_data, \"item_info.npy\"))\n",
    "user_info = np.load(os.path.join(path_to_data, \"user_info.npy\"))\n",
    "train = pd.read_csv(os.path.join(path_to_data, \"test.csv\"), sep=\"@\")\n",
    "test = pd.read_csv(os.path.join(path_to_data, \"test.csv\"), sep=\"@\")\n",
    "display(train.describe())\n",
    "display(test.describe())\n",
    "display(item_info.shape)\n",
    "display(user_info.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "47684e8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T10:40:41.156287Z",
     "iopub.status.busy": "2025-07-17T10:40:41.155485Z",
     "iopub.status.idle": "2025-07-17T10:57:27.232736Z",
     "shell.execute_reply": "2025-07-17T10:57:27.231695Z"
    },
    "papermill": {
     "duration": 1007.924121,
     "end_time": "2025-07-17T10:57:28.315300",
     "exception": false,
     "start_time": "2025-07-17T10:40:40.391179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250717_104041-e71ph33f\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfast-cloud-124\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/23020082-uet/HAC\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/23020082-uet/HAC/runs/e71ph33f\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init kr reader\n",
      "Load item meta data\n",
      "{'length': 23775, 'n_item': 11643, 'item_vec_size': 32, 'user_portrait_len': 32, 'max_seq_len': 50, 'n_feedback': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/4185689321.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(params['model_path'] + \".checkpoint\", map_location=device)\n",
      "/tmp/ipykernel_19/928354164.py:137: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.critic.load_state_dict(torch.load(self.save_path + \"_critic\", map_location=self.device))\n",
      "/tmp/ipykernel_19/928354164.py:138: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.critic_optimizer.load_state_dict(torch.load(self.save_path + \"_critic_optimizer\", map_location=self.device))\n",
      "/tmp/ipykernel_19/928354164.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.actor.load_state_dict(torch.load(self.save_path + \"_actor\", map_location=self.device))\n",
      "/tmp/ipykernel_19/928354164.py:142: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.actor_optimizer.load_state_dict(torch.load(self.save_path + \"_actor_optimizer\", map_location=self.device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50000/50000 [16:41<00:00, 49.91it/s]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       average_n_step \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: average_total_reward \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          buffer_size \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           max_n_step \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     max_total_reward \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           min_n_step \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     min_total_reward \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      reward_variance \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       average_n_step 7.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: average_total_reward 5.97\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          buffer_size 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           max_n_step 17\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     max_total_reward 16.78\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           min_n_step 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     min_total_reward -0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      reward_variance 45.08964\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run \u001b[33mfast-cloud-124\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/23020082-uet/HAC/runs/e71ph33f\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at: \u001b[34m\u001b[4mhttps://wandb.ai/23020082-uet/HAC\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250717_104041-e71ph33f/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# KR setting\n",
    "params['train'] = test\n",
    "params['val'] = test\n",
    "params['n_worker'] = 4\n",
    "params['max_seq_len'] = 50\n",
    "\n",
    "params['loss_type'] = 'bce'\n",
    "params['device'] = device\n",
    "params['l2_coef'] = 0.001\n",
    "params['lr'] = 0.0003\n",
    "params['feature_dim'] = 16\n",
    "params['hidden_dims'] = [256]\n",
    "params['attn_n_head'] = 2\n",
    "params['batch_size'] = 128\n",
    "params['epoch'] = 2\n",
    "params['dropout_rate'] = 0.2\n",
    "params['max_step'] = 20\n",
    "params['initial_temper'] = 20\n",
    "params['reward_function'] = mean_with_cost\n",
    "params['sasrec_n_layer'] = 2\n",
    "params['sasrec_d_model'] = 32\n",
    "params['sasrec_n_head'] = 4\n",
    "params['sasrec_dropout'] = 0.1\n",
    "params['sasrec_d_forward'] = 64\n",
    "params['critic_hidden_dims'] = [256, 64]\n",
    "params['critic_dropout_rate'] = 0.2\n",
    "params['n_iter']= [50000]\n",
    "params['slate_size'] = 10\n",
    "params['noise_var'] = 0.1\n",
    "params['q_laplace_smoothness'] = 0.5\n",
    "params['topk_rate'] = 1\n",
    "params['empty_start_rate'] = 0\n",
    "params['buffer_size'] = 100000\n",
    "params['start_timestamp'] = 2000\n",
    "params['gamma'] = 0.9\n",
    "params['train_every_n_step']= 1\n",
    "params['initial_greedy_epsilon'] = 0\n",
    "params['final_greedy_epsilon'] = 0\n",
    "params['elbow_greedy'] = 0.1\n",
    "params['check_episode'] = 10\n",
    "params['with_eval'] = False\n",
    "\n",
    "params['episode_batch_size'] = 32\n",
    "params['batch_size'] = 64\n",
    "params['actor_lr'] = 0.00001\n",
    "params['critic_lr'] = 0.001\n",
    "params['actor_decay'] = 0.00001\n",
    "params['critic_decay'] = 0.00001\n",
    "params['target_mitigate_coef'] = 0.01\n",
    "params['behavior_lr'] = 0.00005\n",
    "params['behavior_decay'] = 0.00001\n",
    "params['hyper_actor_coef'] = 0.1\n",
    "params['advantage_bias'] = 0\n",
    "params['entropy_coef'] = 0.0001\n",
    "config = params.copy()\n",
    "config.pop(\"train\", None)\n",
    "config.pop(\"val\", None)\n",
    "config.pop(\"item_meta\", None)\n",
    "config.pop(\"user_meta\", None)\n",
    "\n",
    "\n",
    "for seed in [7]:\n",
    "    params['seed'] = seed\n",
    "    set_random_seed(params['seed'])\n",
    "    params['save_path'] = os.path.join(path_to_output, f\"agent/kr_model_seed{params['seed']}\")\n",
    "    run = wandb.init(\n",
    "    # Set the wandb entity where your project will be logged (generally your team name).\n",
    "    entity=\"23020082-uet\",\n",
    "    # Set the wandb project where this run will be logged.\n",
    "    project=\"HAC\",\n",
    "    # Track hyperparameters and run metadata.\n",
    "    config=config\n",
    "    )\n",
    "    os.makedirs(os.path.dirname(params['save_path']), exist_ok=True)\n",
    "    \n",
    "    env = KREnvironment(params)\n",
    "    \n",
    "    policy = SASRec(env, params)\n",
    "    policy.to(device)\n",
    "    \n",
    "    \n",
    "    critic = GeneralCritic(policy, params)\n",
    "    critic.to(device)\n",
    "    \n",
    "    facade = OneStageFacade(env, policy, critic, params)\n",
    "    \n",
    "    agent = DDPG(facade, params)\n",
    "    \n",
    "    agent.test()\n",
    "    run.finish()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7186157,
     "sourceId": 11467320,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3006.291031,
   "end_time": "2025-07-17T10:57:32.118489",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-17T10:07:25.827458",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
