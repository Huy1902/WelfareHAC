{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"gpuType":"T4","provenance":[]},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11683390,"sourceType":"datasetVersion","datasetId":7122510},{"sourceId":334090,"sourceType":"modelInstanceVersion","modelInstanceId":279738,"modelId":300659}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Setup\n","metadata":{"id":"36GFFPEquVlH"}},{"cell_type":"code","source":"!wandb login --relogin c2aabf528c3a17ca15b2306fdef1f0f0d24798bf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T11:44:15.120321Z","iopub.execute_input":"2025-07-04T11:44:15.120541Z","iopub.status.idle":"2025-07-04T11:44:18.301883Z","shell.execute_reply.started":"2025-07-04T11:44:15.120517Z","shell.execute_reply":"2025-07-04T11:44:18.301126Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\nimport os\nimport random\nfrom tqdm import tqdm\nimport wandb\nimport torch.utils.data as data\nfrom torch.utils.data import Dataset\nimport torch.nn as nn\nfrom sklearn.metrics import roc_auc_score\nfrom torch.utils.data import DataLoader\nfrom time import time\nimport copy\nimport torch.nn.functional as F\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"id":"lJ3KHm7bdEpB","outputId":"e85fb76f-ac93-4310-ed56-b532d2b2cbc9","trusted":true,"execution":{"iopub.status.busy":"2025-07-04T11:44:18.303696Z","iopub.execute_input":"2025-07-04T11:44:18.303941Z","iopub.status.idle":"2025-07-04T11:44:24.200094Z","shell.execute_reply.started":"2025-07-04T11:44:18.303918Z","shell.execute_reply":"2025-07-04T11:44:24.199376Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mkdir -p /kaggle/working/ml1m/agent\n!mkdir -p /kaggle/working/ml1m/env","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T11:44:24.200979Z","iopub.execute_input":"2025-07-04T11:44:24.201457Z","iopub.status.idle":"2025-07-04T11:44:24.463570Z","shell.execute_reply.started":"2025-07-04T11:44:24.201435Z","shell.execute_reply":"2025-07-04T11:44:24.462028Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# @title Hyperparameter\n\npath_to_data = \"/kaggle/input/ml1m-dataset\"\npath_to_output = \"/kaggle/working/ml1m/\"\n\n\ncuda = 0\nif cuda >= 0 and torch.cuda.is_available():\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(cuda)\n    torch.cuda.set_device(cuda)\n    device = f\"cuda:{cuda}\"\nelse:\n    device = \"cpu\"","metadata":{"id":"batCUlrPrPpR","trusted":true,"execution":{"iopub.status.busy":"2025-07-04T11:44:24.465145Z","iopub.execute_input":"2025-07-04T11:44:24.465718Z","iopub.status.idle":"2025-07-04T11:44:24.559953Z","shell.execute_reply.started":"2025-07-04T11:44:24.465671Z","shell.execute_reply":"2025-07-04T11:44:24.559262Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# @title Load data\n\n# @title Load data for ml1m\nitem_info = np.load(os.path.join(path_to_data, \"item_info.npy\"))\nuser_info = np.load(os.path.join(path_to_data, \"user_info.npy\"))\ntrain = pd.read_csv(os.path.join(path_to_data, \"train.csv\"), sep=\"@\")\ntest = pd.read_csv(os.path.join(path_to_data, \"test.csv\"), sep=\"@\")\ndisplay(train.head())\ndisplay(test.head())\ndisplay(item_info.shape)\ndisplay(user_info.shape)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T11:44:24.560911Z","iopub.execute_input":"2025-07-04T11:44:24.561434Z","iopub.status.idle":"2025-07-04T11:44:26.408283Z","shell.execute_reply.started":"2025-07-04T11:44:24.561404Z","shell.execute_reply":"2025-07-04T11:44:26.407708Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"display(train.describe())\ndisplay(test.describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T11:44:26.409167Z","iopub.execute_input":"2025-07-04T11:44:26.409493Z","iopub.status.idle":"2025-07-04T11:44:26.437386Z","shell.execute_reply.started":"2025-07-04T11:44:26.409467Z","shell.execute_reply":"2025-07-04T11:44:26.436763Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# @title Support function\n\ndef set_random_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\ndef padding_and_clip(sequence, max_len, padding_direction = 'left'):\n    if len(sequence) < max_len:\n        sequence = [0] * (max_len - len(sequence)) + sequence if padding_direction == 'left' else sequence + [0] * (max_len - len(sequence))\n    sequence = sequence[-max_len:] if padding_direction == 'left' else sequence[:max_len]\n    # print(f\"sequence{sequence}\")\n    return sequence\n\ndef get_regularization(*modules):\n  \"\"\"\n  Customized L2 regularization\n  \"\"\"\n  reg = 0\n  for m in modules:\n    for p in m.parameters():\n      reg = torch.mean(p * p) + reg\n  return reg\n\ndef wrap_batch(batch, device):\n  \"\"\"\n  Build feed_dict from batch data and move data to device\n  \"\"\"\n  for k,val in batch.items():\n    if type(val).__module__ == np.__name__:\n        batch[k] = torch.from_numpy(val)\n    elif torch.is_tensor(val):\n        batch[k] = val\n    elif type(val) is list:\n        batch[k] = torch.tensor(val)\n    else:\n        continue\n    if batch[k].type() == \"torch.DoubleTensor\":\n        batch[k] = batch[k].float()\n    batch[k] = batch[k].to(device)\n  return batch\n\ndef sample_categorical_action(action_prob, candidate_ids, slate_size,\n                              with_replacement=True, batch_wise=False,\n                              return_idx=False):\n  '''\n  @input:\n  - action_prob: (B, L)\n  - candidate_ids: (B, L) or (1, L)\n  - slate_size: K\n  - with_replacement: sample with replacement\n  - batch_wise: do batch wise candidate selection\n  '''\n  if with_replacement:\n    # (K, B)\n    indices = Categorical(action_prob).sample(sample_shape = (slate_size,))\n    # (B, K)\n    indices = torch.transpose(indices, 0, 1)\n  else:\n    indices = torch.cat([torch.multinomial(prob, slate_size, replacement=False).view(1, -1) \\\n                         for prob in action_prob], dim = 0)\n  action = torch.gather(candidate_ids, 1, indices) if batch_wise else candidate_ids[indices]\n  if return_idx:\n    return action.detach(), indices.detach()\n  else:\n    return action.detach()\n\n\n##################\n#   Learning     #\n##################\n\nclass LinearScheduler(object):\n  def __init__(self, schedule_timesteps, final_p, initial_p=1.0):\n    self.schedule_timesteps = schedule_timesteps\n    self.final_p = final_p\n    self.initial_p = initial_p\n\n  def value(self, t):\n    '''\n    see Schedule.value\n    '''\n    fraction = min(float(t) / self.schedule_timesteps, 1.0)\n    return self.initial_p + fraction * (self.final_p - self.initial_p)\n","metadata":{"cellView":"form","id":"6uAs4kp4whrk","trusted":true,"execution":{"iopub.status.busy":"2025-07-04T11:44:26.439681Z","iopub.execute_input":"2025-07-04T11:44:26.439947Z","iopub.status.idle":"2025-07-04T11:44:26.450243Z","shell.execute_reply.started":"2025-07-04T11:44:26.439932Z","shell.execute_reply":"2025-07-04T11:44:26.449530Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# @title Plot Function\n\ndef smooth(values, window = 3):\n  left = window // 2\n  new_values = [np.mean(values[max(0,idx-left):min(idx-left+window,len(values))]) for idx in range(len(values))]\n  return new_values\n\n\ndef get_rl_training_info(log_path, training_losses = ['actor_loss', 'critic_loss']):\n  episode = []\n  average_total_reward, reward_variance, max_total_reward, min_total_reward, average_n_step, max_n_step, min_n_step \\\n          = [], [], [], [], [], [], []\n  training_loss_records = {k: [] for k in training_losses}\n  with open(log_path, 'r') as infile:\n    for line in tqdm(infile):\n      split = line.split('@')\n      # episode\n      episode.append(eval(split[0].split(':')[1]))\n      # episode report\n      episode_report = eval(split[1].strip()[len(\"episode report:\"):])\n      average_total_reward.append(episode_report['average_total_reward'])\n      reward_variance.append(episode_report['reward_variance'])\n      max_total_reward.append(episode_report['max_total_reward'])\n      min_total_reward.append(episode_report['min_total_reward'])\n      average_n_step.append(episode_report['average_n_step'])\n      max_n_step.append(episode_report['max_n_step'])\n      min_n_step.append(episode_report['min_n_step'])\n      # loss report\n      if training_losses:\n          loss_report = eval(split[2].strip()[len(\"step loss:\"):])\n          for k in training_losses:\n              training_loss_records[k].append(loss_report[k])\n  info = {\n      \"episode\": episode,\n      \"average_total_reward\": average_total_reward,\n      \"reward_variance\": reward_variance,\n      \"max_total_reward\": max_total_reward,\n      \"min_total_reward\": min_total_reward,\n      \"average_depth_per_episode\": average_n_step,\n      \"max_depth_per_episode\": max_n_step,\n      \"min_depth_per_episode\": min_n_step\n  }\n  if training_losses:\n      for k in training_losses:\n        info[k] = training_loss_records[k]\n  return info\n\ndef plot_multiple_line(legend_names, list_of_stats, x_name, ncol = 2, row_height = 4, save_path=\"/kaggle/working/fig/rl.png\"):\n  '''\n  @input:\n  - legend_names: [legend]\n  - list_of_stats: [{field_name: [values]}]\n  - x_name: x-axis field_name\n  - ncol: number of subplots in each row\n  '''\n  plt.rcParams.update({'font.size': 14})\n  assert ncol > 0\n  features = list(list_of_stats[0].keys())\n  features.remove(x_name)\n  N = len(features)\n  fig_height = 12 // ncol if len(features) == 1 else row_height*((N-1)//ncol+1)\n  plt.figure(figsize = (16, fig_height))\n  for i,field in enumerate(features):\n      plt.subplot((N-1)//ncol+1,ncol,i+1)\n      minY,maxY = float('inf'),float('-inf')\n      for j,L in enumerate(legend_names):\n          X = list_of_stats[j][x_name]\n          value_list = list_of_stats[j][field]\n          minY,maxY = min(minY,min(value_list)),max(maxY,max(value_list))\n          plt.plot(X[:len(value_list)], value_list, label = L)\n      plt.ylabel(field)\n      plt.xlabel(x_name)\n      scale = 1e-4 + maxY - minY\n      plt.ylim(minY - scale * 0.05, maxY + scale * 0.05)\n      plt.legend()\n  plt.savefig(save_path)\n  plt.show()\n  ","metadata":{"cellView":"form","id":"I_-339N7XCzA","trusted":true,"execution":{"iopub.status.busy":"2025-07-04T11:44:26.451107Z","iopub.execute_input":"2025-07-04T11:44:26.451468Z","iopub.status.idle":"2025-07-04T11:44:26.474204Z","shell.execute_reply.started":"2025-07-04T11:44:26.451424Z","shell.execute_reply":"2025-07-04T11:44:26.473413Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# @title Socrer function\ndef dot_scorer(action_emb, item_emb, item_dim):\n  '''\n  score = item_emb * weight\n\n  @input:\n  - action_emb: (B, i_dim)\n  - item_emb: (B, L, i_dim) or (1, L, i_dim)\n  @output:\n  - score: (B, L)\n  '''\n  output = torch.sum(action_emb.view(-1, 1, item_dim) * item_emb, dim=-1)\n\n  return output","metadata":{"cellView":"form","id":"HVUXZjicg3kg","trusted":true,"execution":{"iopub.status.busy":"2025-07-04T11:44:26.475072Z","iopub.execute_input":"2025-07-04T11:44:26.475410Z","iopub.status.idle":"2025-07-04T11:44:26.492893Z","shell.execute_reply.started":"2025-07-04T11:44:26.475390Z","shell.execute_reply":"2025-07-04T11:44:26.492168Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# @title Dense Neural Network\n\nclass DNN(nn.Module):\n  def __init__(self, in_dim, hidden_dims, out_dim=1, dropout_rate= 0.,\n               do_batch_norm=True):\n    super(DNN, self).__init__()\n    self.in_dim = in_dim\n    layers = []\n\n    for hidden_dim in hidden_dims:\n      linear_layer = nn.Linear(in_dim, hidden_dim)\n\n      layers.append(linear_layer)\n      in_dim = hidden_dim\n      layers.append(nn.ReLU())\n      if dropout_rate > 0:\n        layers.append(nn.Dropout(dropout_rate))\n      if do_batch_norm:\n        layers.append(nn.LayerNorm(hidden_dim))\n\n    # Prediction layer\n    last_layer = nn.Linear(in_dim, out_dim)\n    layers.append(last_layer)\n\n    self.layers = nn.Sequential(*layers)\n\n  def forward(self, x):\n    x = x.view(-1, self.in_dim)\n    logit = self.layers(x)\n    return logit\n\n","metadata":{"cellView":"form","id":"t-f8lshQUlDz","trusted":true,"execution":{"iopub.status.busy":"2025-07-04T11:44:26.493755Z","iopub.execute_input":"2025-07-04T11:44:26.494051Z","iopub.status.idle":"2025-07-04T11:44:26.503777Z","shell.execute_reply.started":"2025-07-04T11:44:26.494022Z","shell.execute_reply":"2025-07-04T11:44:26.503143Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# @title # Data Reader class\n\nclass BaseDataReader(Dataset):\n  def __init__(self, params):\n    self.phase = 'train'\n    self.n_worker = params['n_worker']\n    self._read_data(params)\n\n  def _read_data(self, params):\n    self.data = dict()\n    self.data['train'] = params['train']\n    self.data['val'] = params['val']\n\n\n  def __getitem__(self, idx):\n    pass\n\n  def __len__(self):\n    return len(self.data[self.phase])\n\n  def get_statistics(self):\n    return {'length': len(self)}\n\n  def set_phase(self, phase):\n    assert phase in ['train', 'val', 'test']\n    self.phase = phase\n\n","metadata":{"cellView":"form","id":"EcNXI4e7tyGR","trusted":true,"execution":{"iopub.status.busy":"2025-07-04T11:44:26.504526Z","iopub.execute_input":"2025-07-04T11:44:26.504802Z","iopub.status.idle":"2025-07-04T11:44:26.525283Z","shell.execute_reply.started":"2025-07-04T11:44:26.504776Z","shell.execute_reply":"2025-07-04T11:44:26.524509Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# @title ML1M Data Reader\n\nclass ML1MDataReader(BaseDataReader):\n        \n    def __init__(self, params):\n        '''\n        - from BaseReader:\n            - phase\n            - data: will add Position column\n        '''\n        super().__init__(params)\n        self.max_seq_len = params['max_seq_len']\n        \n    def _read_data(self, params):\n        # read data_file\n        super()._read_data(params)\n        print(\"Load item meta data\")\n        self.item_meta = params['item_meta']\n        self.user_meta = params['user_meta']\n        self.item_vec_size = len(self.item_meta[0])\n        self.user_vec_size = len(self.user_meta[0])\n        self.portrait_len = len(self.user_meta[0])\n    \n    ###########################\n    #        Iterator         #\n    ###########################\n        \n    def __getitem__(self, idx):\n        user_ID, slate_of_items, user_feedback, user_history, sequence_id = self.data[self.phase].iloc[idx]\n        user_profile = self.user_meta[user_ID]\n    \n        exposure = eval(slate_of_items)\n    \n        history = eval(user_history)\n    \n        hist_length = len(history)\n        history = padding_and_clip(history, self.max_seq_len)\n        # print(f\"history{}\")\n        feedback = eval(user_feedback)\n    \n        record = {\n            'timestamp': int(1), # timestamp is irrelevant, just a hack temporal\n            'exposure': np.array(exposure).astype(int),\n            'exposure_features': self.get_item_list_meta(exposure).astype(float),\n            'feedback': np.array(feedback).astype(float),\n            'history': np.array(history).astype(int),\n            'history_features': self.get_item_list_meta(history).astype(float),\n            'history_length': int(min(hist_length, self.max_seq_len)),\n            'user_profile': np.array(user_profile)\n            }\n        return record\n        \n    def get_item_list_meta(self, item_list):\n        return np.array([self.item_meta[item] for item in item_list])\n    \n    def get_statistics(self):\n        '''\n        - n_user\n        - n_item\n        - s_parsity\n        - from BaseReader:\n            - length\n            - fields\n        '''\n        stats = super().get_statistics()\n        stats['length'] = len(self.data[self.phase])\n        stats['n_item'] = len(self.item_meta) - 1\n        stats['item_vec_size'] = self.item_vec_size\n        stats['user_portrait_len'] = self.user_vec_size\n        stats['max_seq_len'] = self.max_seq_len\n        return stats\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T11:44:26.526147Z","iopub.execute_input":"2025-07-04T11:44:26.526525Z","iopub.status.idle":"2025-07-04T11:44:26.541976Z","shell.execute_reply.started":"2025-07-04T11:44:26.526499Z","shell.execute_reply":"2025-07-04T11:44:26.541261Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model","metadata":{"id":"BZu5Gr-atrEE"}},{"cell_type":"code","source":"# @title Base Model\n\nclass BaseModel(nn.Module):\n  def __init__(self, reader, params):\n    super().__init__()\n    self.display_name = \"BaseModel\"\n    self.reader = reader\n    self.model_path = params['model_path']\n    self.loss_type = params['loss_type']\n    self.l2_coef = params['l2_coef']\n    self.device = params['device']\n    self.sigmoid = nn.Sigmoid()\n    self._define_params(reader, params)\n\n  def get_regularization(self, *modules):\n    return get_regularization(*modules)\n\n  def do_forward_and_loss(self, feed_dict: dict) -> dict:\n    '''\n    Used during training to compute predictions and the loss.\n    '''\n    out_dict = self.get_forward(feed_dict)\n    out_dict['loss'] = self.get_loss(feed_dict, out_dict)\n    return out_dict\n\n  def forward(self, feed_dict: dict, return_prob=True) -> dict:\n    '''\n      Used during evaluation/prediction to generate predictions and probabilities\n    '''\n    out_dict = self.get_forward(feed_dict)\n    if return_prob:\n      out_dict['probs'] = self.sigmoid(out_dict['preds'])\n    return out_dict\n\n  def wrap_batch (self, batch):\n    '''\n    Build feed_dict from batch data and move data to self.device\n    '''\n    for k, val in batch.items():\n      if type(val).__module__ == np.__name__:\n        batch[k] = torch.from_numpy(val)\n      elif torch.is_tensor(val):\n        batch[k] = val\n      elif type(val) is list:\n        batch[k] = torch.tensor(val)\n      else:\n        continue # No compatiable type\n      if batch[k].type() == 'torch.DoubleTensor':\n        batch[k] = batch[k].type(torch.FloatTensor)\n      batch[k] = batch[k].to(self.device)\n    return batch\n\n  def save_checkpoint(self):\n    torch.save({\n        \"model_state_dict\": self.state_dict(),\n        \"optimizer_state_dict\": self.optimizer.state_dict(),\n    }, self.model_path + \".checkpoint\")\n\n  def load_checkpoint(self, model_path, with_optimizer=True):\n    checkpoint = torch.load(model_path + \".checkpoint\",\n                            map_location=self.device)\n    self.load_state_dict(checkpoint[\"model_state_dict\"])\n    if with_optimizer:\n      self.optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n    self.model_path = model_path\n\n  def _define_params(self, reader, params):\n    pass\n\n  def get_forward(self, feed_dict: dict) -> dict:\n    pass\n\n  def get_loss(self, feed_dict: dict, out_dict: dict) -> dict:\n    pass","metadata":{"cellView":"form","id":"aE05j2QDr6lh","trusted":true,"execution":{"iopub.status.busy":"2025-07-04T11:44:26.542952Z","iopub.execute_input":"2025-07-04T11:44:26.543716Z","iopub.status.idle":"2025-07-04T11:44:26.564793Z","shell.execute_reply.started":"2025-07-04T11:44:26.543687Z","shell.execute_reply":"2025-07-04T11:44:26.563968Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# @title ML1M Response Model\n\nclass ML1MUserResponse(BaseModel):\n  def __init__(self, reader, params):\n    super().__init__(reader, params)\n    self.bce_loss = nn.BCEWithLogitsLoss(reduction= 'none')\n\n  def _define_params(self, reader, params):\n    stats = reader.get_statistics()\n    print(stats)\n    self.potrait_len = stats['user_portrait_len']\n    self.item_dim = stats['item_vec_size']\n    self.feature_dim = params['feature_dim']\n    self.hidden_dim = params['hidden_dims']\n    self.attn_n_head = params['attn_n_head']\n    self.dropout_rate = params['dropout_rate']\n    self.uEmb = nn.Embedding.from_pretrained(torch.FloatTensor(self.reader.user_meta), freeze=False)\n    self.iEmb = nn.Embedding.from_pretrained(torch.FloatTensor(self.reader.item_meta), freeze=False)\n\n    # fuse information\n    self.concat_layer = nn.Linear(self.feature_dim * 2, self.feature_dim)\n\n    # portrait embedding\n    self.portrait_encoding_layer = DNN(self.potrait_len, self.hidden_dim,\n                                        self.feature_dim, self.dropout_rate,\n                                        do_batch_norm= False)\n    # item embedding\n    self.item_emb_layer = nn.Linear(self.item_dim, self.feature_dim)\n\n    # user history encoder\n    self.seq_self_attn_layer = nn.MultiheadAttention(self.feature_dim, self.attn_n_head, batch_first= True)\n    self.seq_user_attn_layer = nn.MultiheadAttention(self.feature_dim, self.attn_n_head, batch_first= True)\n\n    self.loss = []\n\n  def get_forward(self, feed_dict: dict) -> dict:\n    user_emb = self.portrait_encoding_layer(feed_dict['user_profile']).view(-1, 1, self.feature_dim)\n    history_item_emb = self.item_emb_layer(feed_dict['history_features'])\n\n    seq_encoding, attn_weight = self.seq_self_attn_layer(history_item_emb, history_item_emb, history_item_emb)\n\n    user_interest, attn_weight = self.seq_user_attn_layer(user_emb, seq_encoding, seq_encoding)\n\n    user_interest = torch.concat([user_interest, user_emb], axis=-1)\n    user_interest = self.concat_layer(user_interest)\n\n    exposure_item_emb = self.item_emb_layer(feed_dict['exposure_features'])\n\n    score = torch.sum(exposure_item_emb * user_interest, dim=-1)\n\n    # regularization\n    reg = self.get_regularization(self.uEmb, self.iEmb, self.portrait_encoding_layer,\n                                  self.item_emb_layer, self.seq_user_attn_layer,\n                                  self.seq_self_attn_layer)\n    return {'preds': score, 'reg': reg}\n\n  def get_loss(self, feed_dict: dict, out_dict: dict):\n    preds, reg = out_dict[\"preds\"].view(-1), out_dict[\"reg\"]\n    target = feed_dict['feedback'].view(-1).to(torch.float)\n\n    # print(f\"preds: {self.sigmoid(preds)}\")\n    # print(f\"target: \", target)\n\n    loss = torch.mean(self.bce_loss(self.sigmoid(preds), target))\n    # print(f\"loss: {loss} l2: {reg} l2*coef: {self.l2_coef * reg}\")\n    self.loss.append(loss.item())\n    loss = loss + self.l2_coef * reg\n    return loss\n","metadata":{"cellView":"form","id":"EM4gIUpwL6Bu","trusted":true,"execution":{"iopub.status.busy":"2025-07-04T11:44:26.565813Z","iopub.execute_input":"2025-07-04T11:44:26.566091Z","iopub.status.idle":"2025-07-04T11:44:26.588819Z","shell.execute_reply.started":"2025-07-04T11:44:26.566060Z","shell.execute_reply":"2025-07-04T11:44:26.587984Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train Environment","metadata":{"id":"EPzAu5L7uFO5"}},{"cell_type":"code","source":"params = dict()\nparams['train'] = train\nparams['val'] = test\nparams['item_meta'] = item_info\nparams['user_meta'] = user_info\nparams['n_worker'] = 4\nparams['max_seq_len'] = 50\n\nparams['loss_type'] = 'bce'\nparams['device'] = device\nparams['l2_coef'] = 0.001\nparams['lr'] = 0.0003\nparams['feature_dim'] = 16\nparams['hidden_dims'] = [256]\nparams['attn_n_head'] = 2\nparams['batch_size'] = 128\nparams['seed'] = 26\nparams['epoch'] = 2\nparams['dropout_rate'] = 0.2\nparams['model_path'] = os.path.join(path_to_output, \n                          f\"env/ml1m_user_env_lr{params['lr']}_reg{params['l2_coef']}.model\")\nset_random_seed(params['seed'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T11:44:26.589638Z","iopub.execute_input":"2025-07-04T11:44:26.589951Z","iopub.status.idle":"2025-07-04T11:44:26.612783Z","shell.execute_reply.started":"2025-07-04T11:44:26.589926Z","shell.execute_reply":"2025-07-04T11:44:26.612178Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# @title Train user response\nreader = ML1MDataReader(params)\nmodel = ML1MUserResponse(reader, params).to(device)\n\n\n# reader = RL4RSDataReader(params)\n# model = RL4RSUserResponse(reader, params).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\nmodel.optimizer = optimizer\n\n\nepo = 0\nwhile epo < params['epoch']:\n  print(f\"epoch {epo} is training\")\n  epo += 1\n\n  model.train()\n  reader.set_phase(\"train\")\n  train_loader = DataLoader(reader, params['batch_size'], shuffle = True, pin_memory = True,\n                            num_workers= params['n_worker'])\n\n  t1 = time()\n  pbar = tqdm(total=len(train_loader.dataset))\n  step_loss = []\n  for i, batch_data in enumerate(train_loader):\n    optimizer.zero_grad()\n    wrapped_batch = wrap_batch(batch_data, device)\n\n    out_dict = model.do_forward_and_loss(wrapped_batch)\n    loss = out_dict['loss']\n    loss.backward()\n    step_loss.append(loss.item())\n    optimizer.step()\n    pbar.update(params['batch_size'])\n    # print(model.loss)\n    # if (i + 1) % 10 == 0:\n      # print(f\"Iteration {i + 1}, loss {np.mean(step_loss[-100:])}\")\n  pbar.close()\n    # print(\"Epoch {}; time {:.4f}\".format(epo, time() - t1))\n\n  # validation\n  t2 = time()\n  reader.set_phase(\"val\")\n  val_loader = DataLoader(reader, params['batch_size'], shuffle = False, pin_memory = False,\n                          num_workers= params['n_worker'])\n  valid_probs, valid_true =  [], []\n  pbar = tqdm(total = len(val_loader.dataset))\n  with torch.no_grad():\n    for i, batch_data in enumerate(val_loader):\n      wrapped_batch = wrap_batch(batch_data, device)\n      out_dict = model.forward(wrapped_batch)\n      valid_probs.append(out_dict['probs'].cpu().numpy())\n      valid_true.append(batch_data['feedback'].cpu().numpy())\n      pbar.update(params['batch_size'])\n  pbar.close()\n  auc = roc_auc_score(np.concatenate(valid_true), np.concatenate(valid_probs))\n  print(f\"epoch {epo} validating\" + \"; auc: {:.4f}\".format(np.mean(auc)))\n  model.save_checkpoint()\n  \n","metadata":{"id":"pUiyHg58pQk3","outputId":"de7d8a66-a100-4cfe-b476-963ac617cda0","trusted":true,"execution":{"iopub.status.busy":"2025-07-04T11:44:26.613701Z","iopub.execute_input":"2025-07-04T11:44:26.613948Z","iopub.status.idle":"2025-07-04T11:45:08.392317Z","shell.execute_reply.started":"2025-07-04T11:44:26.613931Z","shell.execute_reply":"2025-07-04T11:45:08.391264Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# @title Plot loss\n\ndef plot_line(labels, data_dicts, x_name='X-Axis', y_name='Y-Axis'):\n    plt.figure(figsize=(20, 6))\n    for label, data_dict in zip(labels, data_dicts):\n        for key, values in data_dict.items():\n            plt.plot(values, label=f\"{label} - {key}\")\n    plt.xlabel(x_name)\n    plt.ylabel(y_name)\n    plt.title('Loss Plot')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n\nloss_values = model.loss\nsmoothed_loss = smooth(np.array(loss_values), window=300)  # Adjust smoothness if needed\n\ninfo = {'Loss': smoothed_loss}\nplot_line(['Model'], [info], x_name='Epoch', y_name='Loss Value')\n\n","metadata":{"id":"eqgPbcAsID16","outputId":"ca1dea56-cb24-4412-b8bb-133c693198cd","trusted":true,"execution":{"iopub.status.busy":"2025-07-04T11:45:08.393638Z","iopub.execute_input":"2025-07-04T11:45:08.394508Z","iopub.status.idle":"2025-07-04T11:45:08.697416Z","shell.execute_reply.started":"2025-07-04T11:45:08.394480Z","shell.execute_reply":"2025-07-04T11:45:08.696599Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train HAC","metadata":{}},{"cell_type":"code","source":"# @title Cost function\ndef mean_with_cost(feedback, zero_reward_cost=0.1):\n  B, L = feedback.shape\n  cost = torch.zeros_like(feedback)\n  cost[feedback == 0] = -zero_reward_cost\n  reward = torch.mean(feedback + cost, dim=-1)\n  return reward\n\ndef nsw(avg_r, min_r, lambda_nsw=1e-4, epsilon=1e-8):\n    r_vec = torch.stack([avg_r, min_r + lambda_nsw], dim=-1)\n    r_vec = torch.clamp(r_vec, min=epsilon)\n    return torch.sum(torch.log(r_vec), dim=-1)","metadata":{"cellView":"form","id":"Q7IfdzX4LFTk","trusted":true,"execution":{"iopub.status.busy":"2025-07-04T11:46:06.210260Z","iopub.execute_input":"2025-07-04T11:46:06.210600Z","iopub.status.idle":"2025-07-04T11:46:06.218541Z","shell.execute_reply.started":"2025-07-04T11:46:06.210559Z","shell.execute_reply":"2025-07-04T11:46:06.217593Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# @title BaseRL Environment\nclass BaseEnv():\n  def __init__(self, params):\n    super().__init__()\n    self.reward_func = params['reward_function']\n    self.max_step_per_episode = params['max_step']\n    self.initial_temper = params[\"initial_temper\"]\n\n  def reset(self, paras):\n    pass\n  def step(self, action):\n    pass","metadata":{"cellView":"form","id":"KP_0zmoEFwta","trusted":true,"execution":{"iopub.status.busy":"2025-07-04T11:45:08.704256Z","iopub.execute_input":"2025-07-04T11:45:08.704510Z","iopub.status.idle":"2025-07-04T11:45:08.719830Z","shell.execute_reply.started":"2025-07-04T11:45:08.704486Z","shell.execute_reply":"2025-07-04T11:45:08.718985Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 1. Environment define","metadata":{}},{"cell_type":"code","source":"# @title ML1M Environment\n\n\nclass ML1MEnvironment(BaseEnv):\n  def __init__(self, params):\n    super().__init__(params)\n    self.reader = ML1MDataReader(params)\n    self.user_response_model = ML1MUserResponse(self.reader, params)\n    checkpoint = torch.load(params['model_path'] + \".checkpoint\", map_location=device)\n    self.user_response_model.load_state_dict(checkpoint[\"model_state_dict\"])\n    self.user_response_model.to(device)\n    self.n_worker = params['n_worker']\n\n    # spaces\n    stats = self.reader.get_statistics()\n    self.action_space = {'item_id': ('nomial', stats['n_item']),\n                         'item_feature': ('continuous', stats['item_vec_size'], 'normal')}\n    self.observation_space = {'user_profile': ('continuous', stats['user_portrait_len'], 'positive'),\n                              'history': ('sequence', stats['max_seq_len'], ('continuous', stats['item_vec_size']))}\n\n  def reset(self, params = {'batch_size': 1, 'empty_history': True}):\n      self.empty_history_flag = params['empty_history'] if 'empty_history' in params else True\n      BS = params['batch_size']\n      observation = {'batch_size': BS}\n      if 'sample' in params:\n          sample_info = params['sample']\n      else:\n          self.batch_iter = iter(DataLoader(self.reader, batch_size = BS, shuffle = True,\n                                            pin_memory = True, num_workers = self.n_worker))\n          sample_info = next(self.batch_iter)\n          sample_info = wrap_batch(sample_info, device = self.user_response_model.device)\n      self.current_observation = {\n          'user_profile': sample_info['user_profile'],  # (B, user_dim)\n          'history': sample_info['history'],  # (B, H)\n          'history_features': sample_info['history_features'], # (B, H, item_dim)\n          'cummulative_reward': torch.zeros(BS).to(self.user_response_model.device),\n          'min_reward': torch.full((BS,), float('inf'), device=self.user_response_model.device),\n          'temper': torch.ones(BS).to(self.user_response_model.device) * self.initial_temper,\n          'step': torch.zeros(BS).to(self.user_response_model.device),\n      }\n      self.reward_history = [0.]\n      self.step_history = [0.]\n      return copy.deepcopy(self.current_observation)\n\n\n  def sample_user(self, n_user, empty_history = False):\n    '''\n    Sample random users and their history\n    '''\n    random_rows = np.random.randint(0, len(self.reader.data['train']), n_user)\n    return self.pick_user(random_rows, empty_history)\n\n  def pick_user(self, rows, empty_history = False):\n    '''\n    Pick users and their history\n    '''\n    raw_portrait = [self.reader.user_meta[self.reader.data['train']['user_id'][rowid]]\n                    for rowid in rows]\n\n    portrait = np.array(raw_portrait)\n\n    history = []\n    history_features = []\n    for rowid in rows:\n      H = [] if empty_history else eval(f\"{self.reader.data['train']['user_mid_history'][rowid]}\")\n      H = padding_and_clip(H, self.reader.max_seq_len)\n      history.append(H)\n\n      history_features.append(self.reader.get_item_list_meta(H).astype(float))\n      return {'user_profile': portrait,\n              'history': history,\n              'history_features': np.array(history_features)}\n\n  def step(self, step_dict):\n    '''\n    @input:\n    - step_dict: {'action': (B, slate_size),\n                    'action_features': (B, slate_size, item_dim) }\n    '''\n    # actions (exposures)\n    action = step_dict['action'] # (B, slate_size), should be item ids only\n    action_features = step_dict['action_features']\n    batch_data = {\n        'user_profile': self.current_observation['user_profile'],\n        'history_features': self.current_observation['history_features'],\n        'exposure_features': action_features\n    }\n    # URM forward\n    with torch.no_grad():\n        output_dict = self.user_response_model(batch_data)\n        response = torch.bernoulli(output_dict['probs']) # (B, slate_size)\n        probs_under_temper = output_dict['probs'] # * prob_scale\n        response = torch.bernoulli(probs_under_temper).detach() # (B, slate_size)\n\n        # reward (B,)\n        immediate_reward = self.reward_func(response).detach()\n\n        self.current_observation['min_reward'] = torch.min(immediate_reward, self.current_observation['min_reward'])\n\n        # (B, H+slate_size)\n        H_prime = torch.cat((self.current_observation['history'], action), dim = 1)\n        # (B, H+slate_size, item_dim)\n        H_prime_features = torch.cat((self.current_observation['history_features'], action_features), dim = 1)\n        # (B, H+slate_size)\n        F_prime = torch.cat((torch.ones_like(self.current_observation['history']), response), dim = 1).to(torch.long)\n        # vector, vector\n        row_indices, col_indices = (F_prime == 1).nonzero(as_tuple=True)\n        # (B,), the number of positive iteraction as history length\n        L = F_prime.sum(dim = 1)\n\n        # user history update\n        offset = 0\n        newH = torch.zeros_like(self.current_observation['history'])\n        newH_features = torch.zeros_like(self.current_observation['history_features'])\n        for row_id in range(action.shape[0]):\n            right = offset + L[row_id]\n            left = right - self.reader.max_seq_len\n            newH[row_id] = H_prime[row_id, col_indices[left:right]]\n            newH_features[row_id] = H_prime_features[row_id,col_indices[left:right],:]\n            offset += L[row_id]\n        self.current_observation['history'] = newH\n        self.current_observation['history_features'] = newH_features\n        self.current_observation['cummulative_reward'] += immediate_reward\n\n        # temper update for leave model\n        temper_down = (-immediate_reward+1) * response.shape[1] + 1\n#             temper_down = -(torch.sum(response, dim = 1) - response.shape[1] - 1)\n#             temper_down = torch.abs(torch.sum(response, dim = 1) - response.shape[1] * self.temper_sweet_point) + 1\n        self.current_observation['temper'] -= temper_down\n        # leave signal\n        done_mask = self.current_observation['temper'] < 1\n        # step update\n        self.current_observation['step'] += 1\n\n        # update rows where user left\n#             refresh_rows = done_mask.nonzero().view(-1)\n#             print(f\"#refresh: {refresh_rows}\")\n        if done_mask.sum() > 0:\n            final_rewards = self.current_observation['cummulative_reward'][done_mask].detach().cpu().numpy()\n            final_steps = self.current_observation['step'][done_mask].detach().cpu().numpy()\n            self.reward_history.append(final_rewards[-1])\n            self.step_history.append(final_steps[-1])\n            # sample new users to fill in the blank\n            new_sample_flag = False\n            try:\n                sample_info = next(self.iter)\n                if sample_info['user_profile'].shape[0] != done_mask.shape[0]:\n                    new_sample_flag = True\n            except:\n                new_sample_flag = True\n            if new_sample_flag:\n                self.iter = iter(DataLoader(self.reader, batch_size = done_mask.shape[0], shuffle = True,\n                                            pin_memory = True, num_workers = params[\"n_worker\"]))\n                sample_info = next(self.iter)\n            sample_info = wrap_batch(sample_info, device = self.user_response_model.device)\n            for obs_key in ['user_profile', 'history', 'history_features']:\n                self.current_observation[obs_key][done_mask] = sample_info[obs_key][done_mask]\n            self.current_observation['cummulative_reward'][done_mask] *= 0\n            self.current_observation['min_reward'][done_mask] = float('inf')\n            self.current_observation['temper'][done_mask] *= 0\n            self.current_observation['temper'][done_mask] += self.initial_temper\n        self.current_observation['step'][done_mask] *= 0\n#         print(f\"step: {self.current_observation['step']}\")\n    return copy.deepcopy(self.current_observation), immediate_reward, done_mask, {'response': response}\n\n\n  def stop(self):\n    self.iter = None\n\n  def get_new_iterator(self, B):\n    return iter(DataLoader(self.reader, batch_size = B, shuffle = True,\n                              pin_memory = True, num_workers = params['n_worker']))\n","metadata":{"cellView":"form","id":"zBhnDCBm0ESQ","trusted":true,"execution":{"iopub.status.busy":"2025-07-04T11:45:08.720668Z","iopub.execute_input":"2025-07-04T11:45:08.720928Z","iopub.status.idle":"2025-07-04T11:45:08.742499Z","shell.execute_reply.started":"2025-07-04T11:45:08.720901Z","shell.execute_reply":"2025-07-04T11:45:08.741777Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# @title Self-Attentive Sequential Recommendation\n\nclass SASRec(nn.Module):\n  def __init__(self, environment, params):\n    super().__init__()\n    self.n_layer = params['sasrec_n_layer']\n    self.d_model = params['sasrec_d_model']\n    self.n_head = params['sasrec_n_head']\n    self.dropout_rate = params['sasrec_dropout']\n    self.d_forward = params['sasrec_d_forward']\n\n    # item space\n    self.item_space = environment.action_space['item_id'][1]\n    self.item_dim = environment.action_space['item_feature'][1]\n    self.maxlen = environment.observation_space['history'][1]\n    self.state_dim = self.d_model\n    self.action_dim = self.d_model\n\n    # policy network modules\n    self.item_map = nn.Linear(self.item_dim, self.d_model)\n    self.pos_emb = nn.Embedding(self.maxlen, self.d_model)\n    self.pos_emb_getter = torch.arange(self.maxlen, dtype = torch.long)\n    self.emb_dropout = nn.Dropout(self.dropout_rate)\n    self.emb_norm = nn.LayerNorm(self.d_model)\n    self.attn_mask = ~torch.tril(torch.ones((self.maxlen, self.maxlen), dtype=torch.bool))\n    encoder_layer = nn.TransformerEncoderLayer(d_model=self.d_model,\n                                               nhead=self.n_head,\n                                               dim_feedforward= self.d_forward,\n                                               dropout=self.dropout_rate,\n                                               batch_first = True\n                                               )\n    self.transformer = nn.TransformerEncoder(encoder_layer= encoder_layer,\n                                             num_layers = self.n_layer)\n\n  def score(self, action_emb, item_emb, do_softmax=True):\n    item_emb = self.item_map(item_emb)\n    output = dot_scorer(action_emb, item_emb, self.d_model)\n    if do_softmax:\n      return torch.softmax(output, dim=-1)\n    else:\n      return output\n\n  def get_scorer_parameters(self):\n    return self.item_map.parameters()\n\n  def encode_state(self, feed_dict):\n    user_history = feed_dict['history_features']\n    # (1, H, d_model)\n    # for item in feed_dict.items():\n    #   print(item)\n    # print(\"user_history device:\", user_history.device)\n    # print(\"self.pos_emb_getter device:\", self.pos_emb_getter.device)\n    # print(\"self.pos_emb device\", self.pos_emb.device)\n\n    pos_emb = self.pos_emb(self.pos_emb_getter.to(user_history.device)).view(1, self.maxlen, self.d_model)\n\n    # (B, H, d_model)\n    history_item_emb = self.item_map(user_history).view(-1, self.maxlen, self.d_model)\n    history_item_emb = self.emb_norm(self.emb_dropout(history_item_emb + pos_emb))\n\n    # (B, H, d_model)\n    output_seq = self.transformer(history_item_emb, mask = self.attn_mask.to(user_history.device))\n\n    return {'output_seq': output_seq, 'state_emb': output_seq[:, -1, :]}\n\n  def forward(self, feed_dict):\n    '''\n    @input\n    - feed_dict: {'user_profile': (B, user_dim),\n                  'history_features': (B, H, item_dim),\n                  'history_mask': (B),\n                  'candicate_features': (B, L, item_dim) or (1, L, item_dim)\n                  }\n    @model\n    - user_profile --> user_emb (B, 1, f_dim)\n    - hisotry_items --> history_item_emb (B, H, f_dim)\n    - (Q:user_emb, K&V: history_item_emb) --(multi-head attn) --> user_state(B, 1, feature_dim)\n    - user_state --> action_prob (B, n_item)\n    '''\n    hist_enc = self.encode_state(feed_dict)\n\n    # user embedding (B, 1, d_model)\n    user_state = hist_enc['state_emb'].view(-1, self.d_model)\n\n    # action embedding (B, d_model)\n    action_emb = user_state\n\n    # regularization\n    reg = get_regularization(self.item_map, self.transformer)\n\n    out_dict = {\n        'action_emb': action_emb,\n        'state_emb': user_state,\n        'seq_emb': hist_enc['output_seq'],\n        'reg': reg\n    }\n    return out_dict","metadata":{"cellView":"form","id":"3BvTCi5yZvxS","trusted":true,"execution":{"iopub.status.busy":"2025-07-04T11:45:08.743347Z","iopub.execute_input":"2025-07-04T11:45:08.743641Z","iopub.status.idle":"2025-07-04T11:45:08.761310Z","shell.execute_reply.started":"2025-07-04T11:45:08.743616Z","shell.execute_reply":"2025-07-04T11:45:08.760492Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# @title General Critic class\nclass GeneralCritic(nn.Module):\n  def __init__(self, policy, params):\n    super().__init__()\n    self.state_dim = policy.state_dim\n    self.action_dim = policy.action_dim\n    self.net = DNN(self.state_dim + self.action_dim, params['critic_hidden_dims'], 1,\n                   dropout_rate=params['critic_dropout_rate'], do_batch_norm=True)\n\n  def forward(self, feed_dict):\n    '''\n    @input:\n    - feed_dict: {'state_emb': (B, state_dim), 'action_emb': (B, action_dim)}\n    '''\n    state_emb = feed_dict['state_emb']\n    action_emb = feed_dict['action_emb'].view(-1, self.action_dim)\n\n    Q = self.net(torch.cat((state_emb, action_emb), dim = -1)).view(-1)\n\n    reg = get_regularization(self.net)\n    return {'q': Q, 'reg': reg}","metadata":{"cellView":"form","id":"Dz4XwAAtzx1d","trusted":true,"execution":{"iopub.status.busy":"2025-07-04T11:45:08.765278Z","iopub.execute_input":"2025-07-04T11:45:08.765862Z","iopub.status.idle":"2025-07-04T11:45:08.781505Z","shell.execute_reply.started":"2025-07-04T11:45:08.765837Z","shell.execute_reply":"2025-07-04T11:45:08.780700Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ValueCritic(nn.Module):\n    \n    def __init__(self, policy, params):\n        super().__init__()\n        self.state_dim = policy.state_dim\n        self.action_dim = policy.action_dim\n#         self.state_encoder = policy.state_encoder\n        self.net = DNN(self.state_dim, params['critic_hidden_dims'], 1, \n                       dropout_rate = params['critic_dropout_rate'], do_batch_norm = True)\n        \n    def forward(self, feed_dict):\n        '''\n        @input:\n        - feed_dict: {'state_emb': (B, state_dim), 'action_emb': (B, action_dim)}\n        '''\n        state_emb = feed_dict['state_emb']\n        V = self.net(state_emb).view(-1)\n        reg = get_regularization(self.net)\n        return {'v': V, 'reg': reg}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T11:45:08.782180Z","iopub.execute_input":"2025-07-04T11:45:08.782447Z","iopub.status.idle":"2025-07-04T11:45:08.798452Z","shell.execute_reply.started":"2025-07-04T11:45:08.782419Z","shell.execute_reply":"2025-07-04T11:45:08.797570Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# @title One Stage Facade\nclass OneStageFacade():\n  def __init__(self, environment, actor, critic, params):\n    super().__init__()\n    self.device = device\n    self.env = environment\n    self.actor = actor\n    self.critic = critic\n\n    self.slate_size = params['slate_size']\n    self.noise_var = params['noise_var']\n    self.noise_decay = params['noise_var'] / params['n_iter'][-1]\n    self.q_laplace_smoothness = params['q_laplace_smoothness']\n    self.topk_rate = params['topk_rate']\n    self.empty_start_rate = params['empty_start_rate']\n\n    self.n_item = self.env.action_space['item_id'][1]\n\n    # (N)\n    self.candidate_iids = np.arange(1, self.n_item + 1)\n\n    # (N, item_dim)\n    self.candidate_features = torch.FloatTensor(\n        self.env.reader.get_item_list_meta(self.candidate_iids)).to(self.device)\n    self.candidate_iids = torch.tensor(self.candidate_iids).to(self.device)\n\n    # replay buffer is initialized in initialize_train()\n    self.buffer_size = params['buffer_size']\n    self.start_timestamp = params['start_timestamp']\n\n  def initialize_train(self):\n    '''\n    Procedures before training\n    '''\n    self.buffer = {\n        \"user_profile\": torch.zeros(self.buffer_size, self.env.reader.portrait_len),\n        \"history\":torch.zeros(self.buffer_size, self.env.reader.max_seq_len).to(torch.long),\n        \"next_history\":torch.zeros(self.buffer_size, self.env.reader.max_seq_len).to(torch.long),\n        \"state_emb\": torch.zeros(self.buffer_size, self.actor.state_dim),\n        \"action_emb\":torch.zeros(self.buffer_size, self.actor.action_dim),\n        \"action\":torch.zeros(self.buffer_size, self.slate_size, dtype=torch.long),\n        \"reward\":torch.zeros(self.buffer_size),\n        \"min_reward\": torch.zeros(self.buffer_size),\n        \"feedback\": torch.zeros(self.buffer_size, self.slate_size),\n        \"done\": torch.zeros(self.buffer_size, dtype=torch.bool)\n    }\n\n    for k, v in self.buffer.items():\n      self.buffer[k] = v.to(self.device)\n    self.buffer_head = 0\n    self.current_buffer_size = 0\n    self.n_stream_record = 0\n    self.is_training_available = False\n\n  def reset_env(self, initial_params = {'batch_size': 1}):\n    '''\n    Reset user response environment\n    '''\n    initial_params['empty_history'] = True if np.random.rand() < self.empty_start_rate else False\n    initial_observation = self.env.reset(initial_params)\n    return initial_observation\n\n  def env_step(self, policy_output):\n    action_dict = {\n      'action': policy_output['action'],\n      'action_features': policy_output['action_features']\n    }\n    observation, reward, done, info = self.env.step(action_dict)\n    return observation, reward, done, info\n\n  def stop_env(self):\n    self.env.stop()\n\n  def get_episode_report(self, n_recent = 10):\n    recent_rewards = self.env.reward_history[-n_recent:]\n    recent_steps = self.env.step_history[-n_recent:]\n    epsiode_report = {\n        'average_total_reward': np.mean(recent_rewards),\n        'reward_variance': np.var(recent_rewards),\n        'max_total_reward': np.max(recent_rewards),\n        'min_total_reward': np.min(recent_rewards),\n        'average_n_step': np.mean(recent_steps),\n        'max_n_step': np.max(recent_steps),\n        'min_n_step': np.min(recent_steps),\n        'buffer_size': self.current_buffer_size\n    }\n    return epsiode_report\n\n  def apply_critic(self, observation, policy_output, critic_model):\n    feed_dict = {\n        'state_emb': policy_output['state_emb'],\n        'action_emb': policy_output['action_emb']\n    }\n    critic_output = critic_model(feed_dict)\n    return critic_output\n\n  def apply_policy(self, observation, policy_model, epsilon = 0, \n                 do_explore = False, do_softmax = True):\n    '''\n    @input:\n    - observation: input of policy model\n    - policy_model\n    - epsilon: greedy epsilon, effective only when do_explore == True\n    - do_explore: exploration flag, True if adding noise to action\n    - do_softmax: output softmax score\n    '''\n#         feed_dict = utils.wrap_batch(observation, device = self.device)\n    feed_dict = observation\n    out_dict = policy_model(feed_dict)\n    if do_explore:\n        action_emb = out_dict['action_emb']\n        # sampling noise of action embedding\n        if np.random.rand() < epsilon:\n            action_emb = torch.clamp(torch.rand_like(action_emb)*self.noise_var, -1, 1)\n        else:\n            action_emb = action_emb + torch.clamp(torch.rand_like(action_emb)*self.noise_var, -1, 1)\n#                 self.noise_var -= self.noise_decay\n        out_dict['action_emb'] = action_emb\n        \n    if 'candidate_ids' in feed_dict:\n        # (B, L, item_dim)\n        out_dict['candidate_features'] = feed_dict['candidate_features']\n        # (B, L)\n        out_dict['candidate_ids'] = feed_dict['candidate_ids']\n        batch_wise = True\n    else:\n        # (1,L,item_dim)\n        out_dict['candidate_features'] = self.candidate_features.unsqueeze(0)\n        # (L,)\n        out_dict['candidate_ids'] = self.candidate_iids\n        batch_wise = False\n        \n    # action prob (B,L)\n    action_prob = policy_model.score(out_dict['action_emb'], \n                                     out_dict['candidate_features'], \n                                     do_softmax = do_softmax)\n\n    # two types of greedy selection\n    if np.random.rand() >= self.topk_rate:\n        # greedy random: categorical sampling\n        action, indices = utils.sample_categorical_action(action_prob, out_dict['candidate_ids'], \n                                                          self.slate_size, with_replacement = False, \n                                                          batch_wise = batch_wise, return_idx = True)\n    else:\n        # indices on action_prob\n        _, indices = torch.topk(action_prob, k = self.slate_size, dim = 1)\n        # topk action\n        if batch_wise:\n            action = torch.gather(out_dict['candidate_ids'], 1, indices).detach() # (B, slate_size)\n        else:\n            action = out_dict['candidate_ids'][indices].detach() # (B, slate_size)\n    # (B,K)\n    out_dict['action'] = action \n    # (B,K,item_dim)\n    out_dict['action_features'] = self.candidate_features[action-1]\n    # (B,K)\n    out_dict['action_prob'] = torch.gather(action_prob, 1, indices) \n    # (B,L)\n    out_dict['candidate_prob'] = action_prob\n    return out_dict\n\n  def sample_buffer(self, batch_size):\n    '''\n    @output:\n    - observation\n    - policy output\n    - reward\n    - done_mask\n    - next_observation\n    '''\n    indices = np.random.randint(0, self.current_buffer_size, size = batch_size)\n    U, H, N, S, HA, A, R, F, D, MR = self.read_buffer(indices)\n    observation = {\n        'user_profile': U,\n        'history_features': H,\n        'min_reward': MR\n    }\n    policy_output = {\n        'state_emb': S,\n        'action_emb': HA,\n        'action': A\n    }\n    reward = R\n    done_mask = D\n    next_observation = {\n        'user_profile': U,\n        'history_features': N,\n        'min_reward': MR,\n        'previous_feedback': F\n    }\n    return observation, policy_output, reward, done_mask, next_observation\n\n  # def sample_raw_data(self, batch_size):\n  #   '''\n  #   Sample supervise data from raw training data\n  #   '''\n  #   batch = self.env.sample_user(batch_size)\n\n  def update_buffer(self, observation, policy_output, reward, done_mask,\n                    next_observation, info):\n    # Overwrite old entries in buffer\n    if self.buffer_head + reward.shape[0] >= self.buffer_size:\n      tail = self.buffer_size - self.buffer_head\n      indices = [self.buffer_head + i for i in range(tail)] + \\\n       [i for i in range(reward.shape[0] - tail)]\n    else:\n      indices = [self.buffer_head  + i for i in range(reward.shape[0])]\n\n    # update buffer\n    self.buffer[\"user_profile\"][indices] = observation['user_profile']\n    self.buffer[\"history\"][indices] = observation['history']\n    self.buffer[\"min_reward\"][indices] = observation['min_reward']\n    self.buffer[\"next_history\"][indices] = next_observation['history']\n    self.buffer[\"state_emb\"][indices] = policy_output['state_emb']\n    self.buffer[\"action\"][indices] = policy_output['action']\n    self.buffer[\"action_emb\"][indices] = policy_output['action_emb']\n    self.buffer[\"reward\"][indices] = reward\n    self.buffer[\"feedback\"][indices] = info['response']\n    self.buffer[\"done\"][indices] = done_mask\n\n    # update buffer pointer\n    self.buffer_head = (self.buffer_head + reward.shape[0]) % self.buffer_size\n    self.n_stream_record += reward.shape[0]\n    self.current_buffer_size = min(self.n_stream_record, self.buffer_size)\n\n    # available training when sufficient sample buffer\n    if self.n_stream_record >= self.start_timestamp:\n      self.is_training_available = True\n  def read_buffer(self, indices):\n    U = self.buffer['user_profile'][indices]\n    # (L, item_dim)\n    H = self.candidate_features[self.buffer[\"history\"][indices] - 1]\n    N = self.candidate_features[self.buffer[\"next_history\"][indices] - 1]\n    S = self.buffer[\"state_emb\"][indices]\n    HA = self.buffer[\"action_emb\"][indices]\n    A = self.buffer[\"action\"][indices]\n    R = self.buffer[\"reward\"][indices]\n    F = self.buffer[\"feedback\"][indices]\n    D = self.buffer[\"done\"][indices]\n    MR = self.buffer['min_reward'][indices]\n    return U, H, N, S, HA, A, R, F, D, MR\n\n  def extract_behavior_data(self, observation, policy_output, next_observation):\n    '''\n    Extract supervised data from RL samples\n    '''\n    observation = {\n        \"user_profile\": observation['user_profile'],\n        \"history_features\": observation['history_features']\n    }\n    exposed_items = policy_output['action']\n    exposure = {\n        \"ids\": exposed_items,\n        \"features\": self.candidate_features[exposed_items - 1]\n    }\n    user_feedback = next_observation[\"previous_feedback\"]\n    return observation, exposure, user_feedback","metadata":{"cellView":"form","id":"Jd56qLlk99ge","trusted":true,"execution":{"iopub.status.busy":"2025-07-04T11:45:08.799427Z","iopub.execute_input":"2025-07-04T11:45:08.799699Z","iopub.status.idle":"2025-07-04T11:45:08.825094Z","shell.execute_reply.started":"2025-07-04T11:45:08.799678Z","shell.execute_reply":"2025-07-04T11:45:08.824267Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# @title One Stage Facade with Hyper Action\n\nclass OneStageFacade_HyperAction(OneStageFacade):\n  def __init__(self, environment, actor, critic, params):\n    super().__init__(environment, actor, critic, params)\n\n  def apply_policy(self, observation, policy_model, epsilon = 0,\n                   do_explore = False, do_softmax = True):\n    feed_dict = wrap_batch(observation, device=device)\n    # print(feed_dict.device)\n    out_dict = policy_model(feed_dict)\n    if do_explore:\n      action_emb = out_dict['action_emb']\n      # explore and exploit + clamping\n      if np.random.rand() < epsilon:\n        action_emb = torch.clamp(torch.rand_like(action_emb) * self.noise_var, -1, 1)\n      else:\n        action_emb = action_emb + torch.clamp(torch.rand_like(action_emb) * self.noise_var, -1, 1)\n\n      out_dict['action_emb'] = action_emb\n\n    # Z latent space\n    out_dict['Z'] = out_dict['action_emb']\n\n    if 'candidate_ids' in feed_dict:\n      # (B, L, item_dim)\n      out_dict['candidate_features']  = feed_dict['candidate_features']\n      # (B, L)\n      out_dict['candidate_ids'] = feed_dict['candidate_ids']\n      batch_wise = True\n    else:\n      # (1, L, item_dim)\n      out_dict['candidate_features'] = self.candidate_features.unsqueeze(0)\n      #(L, )\n      out_dict['candidate_ids'] = self.candidate_iids\n      batch_wise = False\n\n    # action pron (B, L)\n    action_prob = policy_model.score(out_dict['action_emb'],\n                                      out_dict['candidate_features'],\n                                      do_softmax=do_softmax)\n\n    # two types of greedy selection\n    if np.random.rand() >= self.topk_rate:\n      # greedy random\n      action, indices = sample_categorical_action(action_prob, out_dict['candidate_ids'],\n                                                  self.slate_size, with_replacement=False,\n                                                  batch_wise=batch_wise,\n                                                  return_idx=True)\n    else:\n      # indices on action_prob\n      _, indices = torch.topk(action_prob, k = self.slate_size, dim = 1)\n      # print(indices.shape)\n      # print(self.candidate_features.shape)\n      # top k action:\n      # (B, slate_size)\n      if batch_wise:\n        action = torch.gather(out_dict['candidate_ids'], 1, indices).detach()\n      else:\n        action = out_dict['candidate_ids'][indices].detach()\n\n    # (B, K)\n    out_dict['action'] = action\n    # (B, K, item_dim)\n    out_dict['action_features'] = self.candidate_features[indices]\n    # (B, K)\n    out_dict['action_prob'] = torch.gather(action_prob, 1, indices)\n    # (B, L)\n    out_dict['candidate_prob'] = action_prob\n\n    return out_dict\n\n  def infer_hyper_action(self, observation, policy_output, actor):\n    '''\n    Inverse function A -> Z\n    '''\n    # (B, K)\n    A = policy_output['action']\n\n    # (B, K, item_dim)\n    item_embs = self.candidate_features[A - 1]\n\n    # (B, K, kernel_dim)\n    Z = torch.mean(actor.item_map(item_embs).view(A.shape[0], A.shape[1], -1), dim = 1)\n    return {\n        'Z': Z,\n        'action_emb': Z,\n        'state_emb': policy_output['state_emb']\n    }\n\n  def apply_critic(self, observation, policy_output, critic_model):\n    feed_dict = {\n        'state_emb': policy_output['state_emb'],\n        'action_emb': policy_output['action_emb']\n    }\n    critic_output = critic_model(feed_dict)\n    return critic_output","metadata":{"cellView":"form","id":"_H8nHNkrZdnk","trusted":true,"execution":{"iopub.status.busy":"2025-07-04T11:45:08.825982Z","iopub.execute_input":"2025-07-04T11:45:08.826282Z","iopub.status.idle":"2025-07-04T11:45:08.844503Z","shell.execute_reply.started":"2025-07-04T11:45:08.826252Z","shell.execute_reply":"2025-07-04T11:45:08.843783Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# @title Base RL Agent\n\n\n\nclass BaseRLAgent():\n  def __init__(self, facade, params):\n    self.device = params['device']\n    self.gamma = params['gamma']\n    self.n_iter = [0] + params['n_iter']\n    self.train_every_n_step = params['train_every_n_step']\n    self.check_episode = params['check_episode']\n    self.save_path = params['save_path']\n    self.facade = facade\n    self.check_episode = params['check_episode']\n    self.exploration_scheduler = LinearScheduler(int(sum(self.n_iter) * params['elbow_greedy']),\n                                                 params['final_greedy_epsilon'],\n                                                 params['initial_greedy_epsilon'])\n    # if len(self.n_iter) == 2:\n    #   with open(self.save_path + \".report\", 'w') as outfile:\n    #     outfile.write()\n\n  def train(self):\n    if len(self.n_iter) > 2:\n      self.load()\n\n    t = time()\n    start_time = t\n    print(\"Run procedure before training\")\n    self.action_before_train()\n\n    print(\"Start training\")\n    observation = self.facade.reset_env({\n        'batch_size': self.episode_batch_size,\n    })\n    step_offset = sum(self.n_iter[:-1])\n    for i in tqdm(range(step_offset, step_offset + self.n_iter[-1])):\n      observation = self.run_episode_step(i, self.exploration_scheduler.value(i),\n                                          observation, True)\n      if i % self.train_every_n_step == 0:\n        self.step_train()\n\n      if i % self.check_episode == 0:\n        t_ = time()\n        # print(f\"Episode step {i}, time diff {t_ - t}, total time dif {t - start_time})\")\n        self.log_iteration(i)\n        t = t_\n        if i % (3*self.check_episode) == 0:\n            self.save()\n\n    self.action_after_train()\n\n\n  def action_before_train(self):\n    pass\n\n  def action_after_train(self):\n    self.facade.stop_env()\n\n\n  def get_report(self):\n    episode_report = self.facade.get_episode_report(10)\n    train_report = {k: np.mean(v[-10:]) for k, v in self.training_history.items()}\n    return episode_report, train_report\n\n  def log_iteration(self, step):\n    episode_report, train_report = self.get_report()\n    run.log(episode_report | train_report)\n    log_str = f\"step: {step} @ episode report: {episode_report} @ step loss: {train_report}\\n\"\n    with open(self.save_path + \".report\", 'a') as outfile:\n        outfile.write(log_str)\n    return log_str\n\n  def test(self):\n    self.load()\n    self.facade.initialize_train()\n\n    t = time()\n    start_time = t\n\n    print(\"Start testing\")\n    observation = self.facade.reset_env({\n        'batch_size': self.episode_batch_size,\n    })\n    step_offset = sum(self.n_iter[:-1])\n    with torch.no_grad():\n        for i in tqdm(range(step_offset, step_offset + self.n_iter[-1])):\n          observation = self.run_episode_step(i, self.exploration_scheduler.value(i),\n                                              observation, True)\n          if i % self.check_episode == 0:\n            t_ = time()\n            episode_report = self.facade.get_episode_report(10)\n            log_str = f\"step: {i} @ episode report: {episode_report}\\n\"\n            run.log(episode_report)\n            with open(self.save_path + \"_eval.report\", 'a') as outfile:\n              outfile.write(log_str)\n            # print(f\"Episode step {i}, time diff {t_ - t}, total time dif {t - start_time})\")\n            # print(log_str)\n            t = t_\n    \n\n  #######################################\n  #           Abstract function         #\n  #######################################\n  def run_episode_step(self, *episode_args):\n    pass\n\n  def step_train(self):\n    pass\n\n  def save(self):\n    pass\n\n  def load(self):\n    pass","metadata":{"cellView":"form","id":"wJKb3K_326B5","trusted":true,"execution":{"iopub.status.busy":"2025-07-04T11:45:08.845440Z","iopub.execute_input":"2025-07-04T11:45:08.845669Z","iopub.status.idle":"2025-07-04T11:45:08.868101Z","shell.execute_reply.started":"2025-07-04T11:45:08.845648Z","shell.execute_reply":"2025-07-04T11:45:08.867345Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# @title Deep Deterministic Policy Gradient\n\n\nclass DDPG(BaseRLAgent):\n  def __init__(self, facade, params):\n    super().__init__(facade, params)\n    self.actor = facade.actor\n    self.actor_target = copy.deepcopy(self.actor)\n    self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr = params['actor_lr'], weight_decay = params['actor_decay'])\n\n    self.critic = facade.critic\n    self.critic_target = copy.deepcopy(self.critic)\n    self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr = params['critic_lr'], weight_decay = params['critic_decay'])\n\n    self.episode_batch_size = params['episode_batch_size']\n    self.tau = params['target_mitigate_coef']\n    self.actor_lr = params['actor_lr']\n    self.critic_lr = params['critic_lr']\n    self.actor_decay = params['actor_decay']\n    self.critic_decay = params['critic_decay']\n\n    self.batch_size = params['batch_size']\n\n    with open(self.save_path + \".report\", 'w') as outfile:\n      pass\n\n  def action_before_train(self):\n    '''\n    - facade setup\n      - buffer setup\n    - run random episodes to build-up the initial buffer\n    '''\n    self.facade.initialize_train()\n    # print(\"Facade Parameters:\")\n    # for param, value in vars(self.facade).items():\n    #     print(f\"{param}: {value}\")\n    prepare_step = 0\n    # random explore before training\n    initial_epsilon = 1.0\n    observation = self.facade.reset_env({\n        'batch_size': self.episode_batch_size,\n    })\n    while not self.facade.is_training_available:\n      observation = self.run_episode_step(0, initial_epsilon, observation, True)\n      # print(observation)\n      prepare_step += 1\n\n    # training records\n    self.training_history = {\"critic_loss\": [], \"actor_loss\": []}\n\n    print(f\"Total {prepare_step} prepare steps\")\n\n  def run_episode_step(self, *episode_args):\n    '''\n    One step of interaction\n    '''\n    episode_iter, epsilon, observation, do_buffer_update = episode_args\n    with torch.no_grad():\n      # sample action\n      policy_output = self.facade.apply_policy(observation, self.actor, epsilon,\n                                               do_explore=True)\n\n      # apply action on environment and update replay buffer\n      next_observation, reward, done, info = self.facade.env_step(policy_output)\n\n      # update replay buffer\n      if do_buffer_update:\n        self.facade.update_buffer(observation, policy_output, reward, done,\n                                  next_observation, info)\n    return next_observation\n\n  def step_train(self):\n    observation , policy_output, reward, done_mask, next_observation = self.facade.sample_buffer(params['batch_size'])\n\n    critic_loss, actor_loss = self.get_ddpg_loss(observation, policy_output, reward,\n                                                  done_mask, next_observation)\n    self.training_history[\"critic_loss\"].append(critic_loss.item())\n    self.training_history[\"actor_loss\"].append(actor_loss.item())\n\n    # Update the frozen target models\n    for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n      target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)\n\n    for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n      target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)\n\n    return {'step_loss': (self.training_history['actor_loss'][-1],\n                          self.training_history['critic_loss'][-1])}\n\n  def get_ddpg_loss(self, observation, policy_output, reward, done_mask, next_observation,\n                    do_actor_update = True, do_critic_update = True):\n    # Get current Q estimate\n    current_critic_output = self.facade.apply_critic(observation,\n                                                     wrap_batch(policy_output, device=self.device),\n                                                     self.critic)\n    current_Q = current_critic_output['q']\n\n    # Compute the target Q value\n    next_policy_output = self.facade.apply_policy(next_observation, self.actor_target)\n    target_critic_output = self.facade.apply_critic(next_observation, next_policy_output,\n                                                    self.critic_target)\n\n    target_Q = target_critic_output['q']\n    target_Q = reward + self.gamma * (done_mask * target_Q).detach()\n\n    # compute critic loss\n    # minimize current_Q predict and target_Q predict\n    critic_loss = F.mse_loss(current_Q, target_Q).mean()\n\n    if do_critic_update and self.critic_lr > 0:\n      # Optimize the critic\n      self.critic_optimizer.zero_grad()\n      critic_loss.backward()\n      self.critic_optimizer.step()\n\n    # compute actor loss\n    policy_output = self.facade.apply_policy(observation, self.actor)\n    critic_output = self.facade.apply_critic(observation, policy_output, self.critic)\n\n    # Maximize Q value\n    actor_loss = -critic_output['q'].mean()\n\n    if do_actor_update and self.actor_lr > 0:\n      # Optimize the actor\n      self.actor_optimizer.zero_grad()\n      actor_loss.backward()\n      self.actor_optimizer.step()\n    return critic_loss, actor_loss\n\n  def save(self):\n    torch.save(self.critic.state_dict(), self.save_path + \"_critic\")\n    torch.save(self.critic_optimizer.state_dict(), self.save_path + \"_critic_optimizer\")\n    torch.save(self.actor.state_dict(), self.save_path + \"_actor\")\n    torch.save(self.actor_optimizer.state_dict(), self.save_path + \"_actor_optimizer\")\n\n  def load(self):\n    self.critic.load_state_dict(torch.load(self.save_path + \"_critic\", map_location=self.device))\n    self.critic_optimizer.load_state_dict(torch.load(self.save_path + \"_critic_optimizer\", map_location=self.device))\n    self.critic_target = copy.deepcopy(self.critic)\n\n    self.actor.load_state_dict(torch.load(self.save_path + \"_actor\", map_location=self.device))\n    self.actor_optimizer.load_state_dict(torch.load(self.save_path + \"_actor_optimizer\", map_location=self.device))\n    self.actor_target = copy.deepcopy(self.actor)","metadata":{"cellView":"form","id":"VBbJXf-KfvDA","trusted":true,"execution":{"iopub.status.busy":"2025-07-04T11:45:08.869022Z","iopub.execute_input":"2025-07-04T11:45:08.869302Z","iopub.status.idle":"2025-07-04T11:45:08.892553Z","shell.execute_reply.started":"2025-07-04T11:45:08.869280Z","shell.execute_reply":"2025-07-04T11:45:08.891656Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# @title Hyper - Actor Critic\nclass HAC(DDPG):\n  def __init__(self, facade, params):\n    super().__init__(facade, params)\n    self.behavior_lr = params['behavior_lr']\n    self.behavior_decay = params['behavior_decay']\n    self.hyper_actor_coef = params['hyper_actor_coef']\n    self.actor_behavior_optimizer = torch.optim.Adam(self.actor.parameters(),\n                                                     lr=params['behavior_lr'],\n                                                     weight_decay=params['behavior_decay'])\n\n  def action_before_train(self):\n    super().action_before_train()\n    self.training_history['hyper_actor_loss'] = []\n    self.training_history['behavior_loss'] = []\n\n  def run_episode_step(self, *episode_args):\n    '''\n    One step of interaction\n    '''\n    episode_iter, epsilon, observation, do_buffer_update = episode_args\n    with torch.no_grad():\n      # sample action\n      policy_output = self.facade.apply_policy(observation, self.actor, epsilon,\n                                               do_explore=True)\n\n      # apply action on environment and update replay buffer\n      next_observation, reward, done, info = self.facade.env_step(policy_output)\n\n      # update replay buffer\n      if do_buffer_update:\n        self.facade.update_buffer(observation, policy_output, reward, done,\n                                  next_observation, info)\n    return next_observation\n\n  def step_train(self):\n    observation , policy_output, reward, done_mask, next_observation = self.facade.sample_buffer(params['batch_size'])\n    # reward  = torch.FloatTensor(reward)\n    # done_mask = torch.FloatTensor(done_mask)\n\n    critic_loss, actor_loss, hyper_actor_loss = self.get_hac_loss(observation, policy_output, reward,\n                                                  done_mask, next_observation)\n    behavior_loss = self.get_behavior_loss(observation, policy_output, next_observation)\n\n    self.training_history[\"critic_loss\"].append(critic_loss.item())\n    self.training_history[\"actor_loss\"].append(actor_loss.item())\n    self.training_history['hyper_actor_loss'].append(hyper_actor_loss.item())\n    self.training_history['behavior_loss'].append(behavior_loss.item())\n\n    # Update frozen target models\n    for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n      target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)\n\n    for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n      target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)\n\n    return {\"step_loss\": (self.training_history['actor_loss'][-1],\n                          self.training_history['critic_loss'][-1],\n                          self.training_history['hyper_actor_loss'][-1],\n                          self.training_history['behavior_loss'][-1])}\n\n  def get_hac_loss(self, observation, policy_output, reward, done_mask, next_observation,\n                    do_actor_update = True, do_critic_update = True):\n    \n    # Current Q estimate\n    hyper_output = self.facade.infer_hyper_action(observation, policy_output, self.actor)\n    current_critic_output = self.facade.apply_critic(observation, hyper_output, self.critic)\n    current_Q = current_critic_output['q']\n\n    # Compute target Q value\n    next_policy_output = self.facade.apply_policy(next_observation, self.actor_target)\n    target_critic_output = self.facade.apply_critic(next_observation, next_policy_output, self.critic_target)\n\n    target_Q = target_critic_output['q']\n    target_Q = reward + self.gamma * (done_mask * target_Q).detach()\n\n    assert not torch.isnan(current_Q).any(), \"NaN in current_Q!\"\n    assert not torch.isnan(target_Q).any(), \"NaN in target_Q!\"\n    assert not torch.isnan(reward).any(), \"NaN in reward!\"\n\n    critic_loss = F.mse_loss(current_Q, target_Q).mean()\n    if do_critic_update and self.critic_lr > 0:\n      self.critic_optimizer.zero_grad()\n      critic_loss.backward()\n      self.critic_optimizer.step()\n\n    # actor loss\n\n    if do_actor_update and self.actor_lr > 0:\n      self.actor_optimizer.zero_grad()\n      policy_output = self.facade.apply_policy(observation, self.actor)\n      critic_output = self.facade.apply_critic(observation, policy_output, self.critic)\n      actor_loss = -critic_output['q'].mean()\n      actor_loss.backward()\n      self.actor_optimizer.step()\n\n    # hyper actor loss\n\n    if do_actor_update and self.hyper_actor_coef > 0:\n      self.actor_optimizer.zero_grad()\n      policy_output = self.facade.apply_policy(observation, self.actor)\n      inferred_hyper_output = self.facade.infer_hyper_action(observation, policy_output, self.actor)\n      hyper_actor_loss = self.hyper_actor_coef * F.mse_loss(inferred_hyper_output['Z'],\n                                                            policy_output['Z']).mean()\n\n      hyper_actor_loss.backward()\n      self.actor_optimizer.step()\n\n    return critic_loss, actor_loss, hyper_actor_loss\n\n  def get_behavior_loss(self, observation, policy_output, next_observation, do_update = True):\n    observation, exposure, feedback = self.facade.extract_behavior_data(observation, policy_output, next_observation)\n    observation['candidate_ids'] = exposure['ids']\n    observation['candidate_features'] = exposure['features']\n    policy_output = self.facade.apply_policy(observation, self.actor, do_softmax=False)\n    action_prob = torch.sigmoid(policy_output['candidate_prob'])\n    behavior_loss = F.binary_cross_entropy(action_prob, feedback)\n\n    if do_update and self.behavior_lr > 0:\n      self.actor_behavior_optimizer.zero_grad()\n      behavior_loss.backward()\n      self.actor_behavior_optimizer.step()\n\n    return behavior_loss\n","metadata":{"cellView":"form","id":"HLBYcDpMSfuw","trusted":true,"execution":{"iopub.status.busy":"2025-07-04T11:45:08.893455Z","iopub.execute_input":"2025-07-04T11:45:08.893753Z","iopub.status.idle":"2025-07-04T11:45:08.912617Z","shell.execute_reply.started":"2025-07-04T11:45:08.893729Z","shell.execute_reply":"2025-07-04T11:45:08.911845Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class A2C(BaseRLAgent):\n    \n    def __init__(self, facade, params):\n        '''\n        self.gamma\n        self.n_iter\n        self.check_episode\n        self.with_eval\n        self.save_path\n        self.facade\n        self.exploration_scheduler\n        '''\n        super().__init__(facade, params)\n        self.episode_batch_size = params['episode_batch_size']\n        self.batch_size = params['batch_size']\n        \n        self.actor_lr = params['actor_lr']\n        self.critic_lr = params['critic_lr']\n        self.actor_decay = params['actor_decay']\n        self.critic_decay = params['critic_decay']\n        \n        self.actor = facade.actor\n        self.actor_target = copy.deepcopy(self.actor)\n        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=params['actor_lr'], \n                                                weight_decay=params['actor_decay'])\n\n        self.critic = facade.critic\n        self.critic_target = copy.deepcopy(self.critic)\n        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=params['critic_lr'], \n                                                 weight_decay=params['critic_decay'])\n\n        self.tau = params['target_mitigate_coef']\n        self.advantage_bias = params['advantage_bias']\n        self.entropy_coef = params['entropy_coef']\n        if len(self.n_iter) == 1:\n            with open(self.save_path + \".report\", 'w') as outfile:\n                outfile.write(f\"{args}\\n\")\n        \n        \n#     def action_after_train(self):\n#         self.facade.stop_env()\n        \n#     def get_report(self):\n#         episode_report = self.facade.get_episode_report(10)\n#         train_report = {k: np.mean(v[-10:]) for k,v in self.training_history.items()}\n#         return episode_report, train_report\n        \n    def action_before_train(self):\n        super().action_before_train()\n        self.facade.initialize_train()\n        self.training_history = {\"critic_loss\": [], \"actor_loss\": []}\n        self.training_history['entropy_loss'] = []\n        self.training_history['advantage'] = []\n        \n    def run_episode_step(self, *episode_args):\n        '''\n        One step of interaction\n        '''\n        episode_iter, epsilon, observation, do_buffer_update = episode_args\n        with torch.no_grad():\n            # sample action\n            policy_output = self.facade.apply_policy(observation, self.actor, epsilon, \n                                                     do_explore = True, do_softmax = True)\n            # apply action on environment and update replay buffer\n            next_observation, reward, done, info = self.facade.env_step(policy_output)\n            # update replay buffer\n            if do_buffer_update:\n                self.facade.update_buffer(observation, policy_output, reward, done, next_observation, info)\n        return next_observation\n            \n\n    def step_train(self):\n        observation, policy_output, reward, done_mask, next_observation = self.facade.sample_buffer(self.batch_size)\n#         reward = torch.FloatTensor(reward)\n#         done_mask = torch.FloatTensor(done_mask)\n        \n        critic_loss, actor_loss, entropy_loss, advantage = self.get_a2c_loss(observation, policy_output, reward, done_mask, next_observation)\n        self.training_history['actor_loss'].append(actor_loss.item())\n        self.training_history['critic_loss'].append(critic_loss.item())\n        self.training_history['entropy_loss'].append(entropy_loss.item())\n        self.training_history['advantage'].append(advantage.item())\n\n        # Update the frozen target models\n        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n\n        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n\n        return {\"step_loss\": (self.training_history['actor_loss'][-1], \n                              self.training_history['critic_loss'][-1], \n                              self.training_history['entropy_loss'][-1], \n                              self.training_history['advantage'][-1])}\n    \n    def get_a2c_loss(self, observation, policy_output, reward, done_mask, next_observation, \n                      do_actor_update = True, do_critic_update = True):\n        \n        # Get current Q estimate\n        current_policy_output = self.facade.apply_policy(observation, self.actor)\n        # print(current_policy_output)\n        S = current_policy_output['state_emb']\n        V_S = self.critic({'state_emb': S})['v']\n        \n        # Compute the target Q value\n        next_policy_output = self.facade.apply_policy(next_observation, self.actor_target)\n#         next_policy_output = self.facade.apply_p[olicy(next_observation, self.actor)\n        S_prime = next_policy_output['state_emb']\n        V_S_prime = self.critic_target({'state_emb': S_prime})['v'].detach()\n#         V_S_prime = self.critic({'state_emb': S_prime})['v'].detach()\n        Q_S = reward + self.gamma * (done_mask * V_S_prime)\n        advantage = torch.clamp((Q_S - V_S).detach(), -1, 1) # (B,)\n\n        # Compute critic loss\n        value_loss = F.mse_loss(V_S, Q_S).mean()\n        \n        # Regularization loss\n#         critic_reg = current_critic_output['reg']\n\n        if do_critic_update and self.critic_lr > 0:\n            # Optimize the critic\n            self.critic_optimizer.zero_grad()\n            value_loss.backward()\n            self.critic_optimizer.step()\n\n        # Compute actor loss\n        current_policy_output = self.facade.apply_policy(observation, self.actor)\n        A = policy_output['action']\n#         logp = -torch.log(current_policy_output['action_prob'] + 1e-6) # (B,K)\n        logp = -torch.log(torch.gather(current_policy_output['candidate_prob'],1,A-1) + 1e-6) # (B,K)\n        # use log(1-p), p is close to zero when there are large number of items\n#         logp = torch.log(-torch.gather(current_policy_output['candidate_prob'],1,A-1)+1) # (B,K)\n        actor_loss = torch.mean(torch.sum(logp * (advantage.view(-1,1) + self.advantage_bias), dim = 1))\n        entropy_loss = torch.sum(current_policy_output['candidate_prob'] \\\n                                  * torch.log(current_policy_output['candidate_prob']), dim = 1).mean()\n        \n        # Regularization loss\n#         actor_reg = policy_output['reg']\n\n        if do_actor_update and self.actor_lr > 0:\n            # Optimize the actor \n            self.actor_optimizer.zero_grad()\n            (actor_loss + self.entropy_coef * entropy_loss).backward()\n            self.actor_optimizer.step()\n            \n        return value_loss, actor_loss, entropy_loss, torch.mean(advantage)\n\n\n    def save(self):\n        torch.save(self.critic.state_dict(), self.save_path + \"_critic\")\n        torch.save(self.critic_optimizer.state_dict(), self.save_path + \"_critic_optimizer\")\n\n        torch.save(self.actor.state_dict(), self.save_path + \"_actor\")\n        torch.save(self.actor_optimizer.state_dict(), self.save_path + \"_actor_optimizer\")\n\n\n    def load(self):\n        self.critic.load_state_dict(torch.load(self.save_path + \"_critic\", map_location=self.device))\n        self.critic_optimizer.load_state_dict(torch.load(self.save_path + \"_critic_optimizer\", map_location=self.device))\n        self.critic_target = copy.deepcopy(self.critic)\n\n        self.actor.load_state_dict(torch.load(self.save_path + \"_actor\", map_location=self.device))\n        self.actor_optimizer.load_state_dict(torch.load(self.save_path + \"_actor_optimizer\", map_location=self.device))\n        self.actor_target = copy.deepcopy(self.actor)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T11:45:08.913363Z","iopub.execute_input":"2025-07-04T11:45:08.913616Z","iopub.status.idle":"2025-07-04T11:45:08.936763Z","shell.execute_reply.started":"2025-07-04T11:45:08.913599Z","shell.execute_reply":"2025-07-04T11:45:08.936007Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# @ ML1M setting\n\nparams['n_worker'] = 4\nparams['max_seq_len'] = 50\n\nparams['loss_type'] = 'bce'\nparams['device'] = device\nparams['l2_coef'] = 0.001\nparams['lr'] = 0.0003\nparams['feature_dim'] = 16\nparams['hidden_dims'] = [256]\nparams['attn_n_head'] = 2\nparams['batch_size'] = 128\nparams['epoch'] = 2\nparams['dropout_rate'] = 0.2\nparams['max_step'] = 20\nparams['initial_temper'] = 20\nparams['reward_function'] = mean_with_cost\nparams['sasrec_n_layer'] = 2\nparams['sasrec_d_model'] = 32\nparams['sasrec_n_head'] = 4\nparams['sasrec_dropout'] = 0.1\nparams['sasrec_d_forward'] = 64\nparams['critic_hidden_dims'] = [256, 64]\nparams['critic_dropout_rate'] = 0.2\nparams['n_iter']= [50000]\nparams['slate_size'] = 10\nparams['noise_var'] = 0.1\nparams['q_laplace_smoothness'] = 0.5\nparams['topk_rate'] = 1\nparams['empty_start_rate'] = 0\nparams['buffer_size'] = 100000\nparams['start_timestamp'] = 2000\nparams['gamma'] = 0.9\nparams['train_every_n_step']= 1\nparams['initial_greedy_epsilon'] = 0\nparams['final_greedy_epsilon'] = 0\nparams['elbow_greedy'] = 0.1\nparams['check_episode'] = 10\nparams['with_eval'] = False\n\nparams['episode_batch_size'] = 32\nparams['batch_size'] = 64\nparams['actor_lr'] = 0.00001\nparams['critic_lr'] = 0.001\nparams['actor_decay'] = 0.00001\nparams['critic_decay'] = 0.00001\nparams['target_mitigate_coef'] = 0.01\nparams['behavior_lr'] = 0.0003\nparams['behavior_decay'] = 0.00001\nparams['hyper_actor_coef'] = 0.1\nparams['advantage_bias'] = 0\nparams['entropy_coef'] = 0.0001\n\nconfig = params.copy()\nconfig.pop(\"train\", None)\nconfig.pop(\"val\", None)\nconfig.pop(\"item_meta\", None)\nconfig.pop(\"user_meta\", None)\n\n\nfor seed in [11]:\n    params['seed'] = seed\n    set_random_seed(params['seed'])\n    params['save_path'] = os.path.join(path_to_output, f\"agent/ml1m_model_seed{params['seed']}\")\n    run = wandb.init(\n    # Set the wandb entity where your project will be logged (generally your team name).\n    entity=\"23020082-uet\",\n    # Set the wandb project where this run will be logged.\n    project=\"HAC\",\n    # Track hyperparameters and run metadata.\n    config=config\n    )\n    os.makedirs(os.path.dirname(params['save_path']), exist_ok=True)\n    \n    env = ML1MEnvironment(params)\n    \n    policy = SASRec(env, params)\n    policy.to(device)\n    \n    \n    critic = GeneralCritic(policy, params)\n    critic.to(device)\n    \n    facade = OneStageFacade_HyperAction(env, policy, critic, params)\n    agent = HAC(facade, params)\n    \n    agent.train()\n    run.finish()","metadata":{"id":"q5dd23sM-gSN","outputId":"843e44a9-4951-4d2e-941e-074c5d06af00","trusted":true,"execution":{"iopub.status.busy":"2025-07-04T11:46:11.207334Z","iopub.execute_input":"2025-07-04T11:46:11.207652Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Test","metadata":{}},{"cell_type":"code","source":"# @title Load data\n\n# @title Load data for ml1m\nitem_info = np.load(os.path.join(path_to_data, \"item_info.npy\"))\nuser_info = np.load(os.path.join(path_to_data, \"user_info.npy\"))\ntrain = pd.read_csv(os.path.join(path_to_data, \"all.csv\"), sep=\"@\")\ntest = pd.read_csv(os.path.join(path_to_data, \"test.csv\"), sep=\"@\")\ndisplay(train.head())\ndisplay(test.head())\ndisplay(item_info.shape)\ndisplay(user_info.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T11:45:26.348547Z","iopub.status.idle":"2025-07-04T11:45:26.348811Z","shell.execute_reply.started":"2025-07-04T11:45:26.348694Z","shell.execute_reply":"2025-07-04T11:45:26.348706Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"params['train'] = train\nparams['val'] = test\nparams['item_meta'] = item_info\nparams['user_meta'] = user_info\nparams['seed'] = 26\nparams['model_path'] = os.path.join(path_to_output, \n                          f\"env/ml1m_user_env_lr{params['lr']}_reg{params['l2_coef']}_eval.model\")\nset_random_seed(params['seed'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T11:45:26.350399Z","iopub.status.idle":"2025-07-04T11:45:26.350711Z","shell.execute_reply.started":"2025-07-04T11:45:26.350533Z","shell.execute_reply":"2025-07-04T11:45:26.350543Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# @title Train user response\nreader = ML1MDataReader(params)\nmodel = ML1MUserResponse(reader, params).to(device)\n\n\n# reader = RL4RSDataReader(params)\n# model = RL4RSUserResponse(reader, params).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\nmodel.optimizer = optimizer\n\n\nepo = 0\nwhile epo < params['epoch']:\n  print(f\"epoch {epo} is training\")\n  epo += 1\n\n  model.train()\n  reader.set_phase(\"train\")\n  train_loader = DataLoader(reader, params['batch_size'], shuffle = True, pin_memory = True,\n                            num_workers= params['n_worker'])\n\n  t1 = time()\n  pbar = tqdm(total=len(train_loader.dataset))\n  step_loss = []\n  for i, batch_data in enumerate(train_loader):\n    optimizer.zero_grad()\n    wrapped_batch = wrap_batch(batch_data, device)\n\n    out_dict = model.do_forward_and_loss(wrapped_batch)\n    loss = out_dict['loss']\n    loss.backward()\n    step_loss.append(loss.item())\n    optimizer.step()\n    pbar.update(params['batch_size'])\n    # print(model.loss)\n    # if (i + 1) % 10 == 0:\n      # print(f\"Iteration {i + 1}, loss {np.mean(step_loss[-100:])}\")\n  pbar.close()\n    # print(\"Epoch {}; time {:.4f}\".format(epo, time() - t1))\n\n  # validation\n  t2 = time()\n  reader.set_phase(\"val\")\n  val_loader = DataLoader(reader, params['batch_size'], shuffle = False, pin_memory = False,\n                          num_workers= params['n_worker'])\n  valid_probs, valid_true =  [], []\n  pbar = tqdm(total = len(val_loader.dataset))\n  with torch.no_grad():\n    for i, batch_data in enumerate(val_loader):\n      wrapped_batch = wrap_batch(batch_data, device)\n      out_dict = model.forward(wrapped_batch)\n      valid_probs.append(out_dict['probs'].cpu().numpy())\n      valid_true.append(batch_data['feedback'].cpu().numpy())\n      pbar.update(params['batch_size'])\n  pbar.close()\n  auc = roc_auc_score(np.concatenate(valid_true), np.concatenate(valid_probs))\n  print(f\"epoch {epo} validating\" + \"; auc: {:.4f}\".format(np.mean(auc)))\n  model.save_checkpoint()\n  \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T11:45:26.351953Z","iopub.status.idle":"2025-07-04T11:45:26.352255Z","shell.execute_reply.started":"2025-07-04T11:45:26.352098Z","shell.execute_reply":"2025-07-04T11:45:26.352110Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# @title Load data\n\n# @title Load data for ml1m\ntrain = pd.read_csv(os.path.join(path_to_data, \"test.csv\"), sep=\"@\")\ntest = pd.read_csv(os.path.join(path_to_data, \"test.csv\"), sep=\"@\")\ndisplay(train.head())\ndisplay(test.head())\ndisplay(item_info.shape)\ndisplay(user_info.shape)","metadata":{"id":"JhQPZR_yXqHk","outputId":"cce3f497-89a7-415f-da30-173f195aa5ae","trusted":true,"execution":{"iopub.status.busy":"2025-07-04T11:45:26.353815Z","iopub.status.idle":"2025-07-04T11:45:26.354150Z","shell.execute_reply.started":"2025-07-04T11:45:26.353982Z","shell.execute_reply":"2025-07-04T11:45:26.353998Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"params['train'] = train\nparams['test'] = test\nconfig = params.copy()\nconfig.pop(\"train\", None)\nconfig.pop(\"val\", None)\nconfig.pop(\"item_meta\", None)\nconfig.pop(\"user_meta\", None)\n\nfor seed in [11]:\n    params['seed'] = seed\n    set_random_seed(params['seed'])\n    params['save_path'] = os.path.join(path_to_output, f\"agent/ml1m_model_seed{params['seed']}\")\n    run = wandb.init(\n    # Set the wandb entity where your project will be logged (generally your team name).\n    entity=\"23020082-uet\",\n    # Set the wandb project where this run will be logged.\n    project=\"HAC\",\n    # Track hyperparameters and run metadata.\n    config=config\n    )\n    os.makedirs(os.path.dirname(params['save_path']), exist_ok=True)\n    \n    env = ML1MEnvironment(params)\n    \n    policy = SASRec(env, params)\n    policy.to(device)\n    \n    \n    critic = GeneralCritic(policy, params)\n    critic.to(device)\n    \n    facade = OneStageFacade_HyperAction(env, policy, critic, params)\n    agent = HAC(facade, params)\n    \n    agent.test()\n    run.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T11:45:26.355186Z","iopub.status.idle":"2025-07-04T11:45:26.355531Z","shell.execute_reply.started":"2025-07-04T11:45:26.355382Z","shell.execute_reply":"2025-07-04T11:45:26.355399Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}