{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"gpuType":"T4","provenance":[]},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11683390,"sourceType":"datasetVersion","datasetId":7122510},{"sourceId":334090,"sourceType":"modelInstanceVersion","modelInstanceId":279738,"modelId":300659}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Setup\n","metadata":{"id":"36GFFPEquVlH"}},{"cell_type":"code","source":"!wandb login --relogin c2aabf528c3a17ca15b2306fdef1f0f0d24798bf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:09:53.930335Z","iopub.execute_input":"2025-07-10T10:09:53.930582Z","iopub.status.idle":"2025-07-10T10:09:57.241976Z","shell.execute_reply.started":"2025-07-10T10:09:53.930539Z","shell.execute_reply":"2025-07-10T10:09:57.241047Z"}},"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\nimport os\nimport random\nfrom tqdm import tqdm\nimport wandb\nimport torch.utils.data as data\nfrom torch.utils.data import Dataset\nimport torch.nn as nn\nfrom sklearn.metrics import roc_auc_score\nfrom torch.utils.data import DataLoader\nfrom time import time\nimport copy\nimport torch.nn.functional as F\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"id":"lJ3KHm7bdEpB","outputId":"e85fb76f-ac93-4310-ed56-b532d2b2cbc9","trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:09:57.244144Z","iopub.execute_input":"2025-07-10T10:09:57.244404Z","iopub.status.idle":"2025-07-10T10:10:02.994815Z","shell.execute_reply.started":"2025-07-10T10:09:57.244377Z","shell.execute_reply":"2025-07-10T10:10:02.994035Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/hac/pytorch/default/1/model_actor\n/kaggle/input/hac/pytorch/default/1/model_actor_optimizer\n/kaggle/input/hac/pytorch/default/1/model_critic_optimizer\n/kaggle/input/hac/pytorch/default/1/model_critic\n/kaggle/input/ml1m-dataset/user_info.npy\n/kaggle/input/ml1m-dataset/item_info.npy\n/kaggle/input/ml1m-dataset/all.csv\n/kaggle/input/ml1m-dataset/train.csv\n/kaggle/input/ml1m-dataset/test.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!mkdir -p /kaggle/working/ml1m/agent\n!mkdir -p /kaggle/working/ml1m/env","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:10:02.995684Z","iopub.execute_input":"2025-07-10T10:10:02.996107Z","iopub.status.idle":"2025-07-10T10:10:03.254962Z","shell.execute_reply.started":"2025-07-10T10:10:02.996088Z","shell.execute_reply":"2025-07-10T10:10:03.254083Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# @title Hyperparameter\n\npath_to_data = \"/kaggle/input/ml1m-dataset\"\npath_to_output = \"/kaggle/working/ml1m/\"\n\n\ncuda = 0\nif cuda >= 0 and torch.cuda.is_available():\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(cuda)\n    torch.cuda.set_device(cuda)\n    device = f\"cuda:{cuda}\"\nelse:\n    device = \"cpu\"","metadata":{"id":"batCUlrPrPpR","trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:10:03.255927Z","iopub.execute_input":"2025-07-10T10:10:03.256180Z","iopub.status.idle":"2025-07-10T10:10:03.340412Z","shell.execute_reply.started":"2025-07-10T10:10:03.256157Z","shell.execute_reply":"2025-07-10T10:10:03.339699Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# @title Load data\n\n# @title Load data for ml1m\nitem_info = np.load(os.path.join(path_to_data, \"item_info.npy\"))\nuser_info = np.load(os.path.join(path_to_data, \"user_info.npy\"))\ntrain = pd.read_csv(os.path.join(path_to_data, \"train.csv\"), sep=\"@\")\ntest = pd.read_csv(os.path.join(path_to_data, \"test.csv\"), sep=\"@\")\ndisplay(train.head())\ndisplay(test.head())\ndisplay(item_info.shape)\ndisplay(user_info.shape)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:10:03.341585Z","iopub.execute_input":"2025-07-10T10:10:03.341866Z","iopub.status.idle":"2025-07-10T10:10:04.909038Z","shell.execute_reply.started":"2025-07-10T10:10:03.341844Z","shell.execute_reply":"2025-07-10T10:10:04.908239Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"   user_id                                     slate_of_items  \\\n0        1  [3186, 1270, 1721, 1022, 2340, 1836, 3408, 280...   \n1        1  [720, 260, 919, 608, 2692, 1961, 2028, 3105, 9...   \n2        1  [1962, 2018, 150, 1028, 1097, 914, 1287, 2797,...   \n3        1  [661, 2918, 531, 3114, 2791, 2321, 1029, 1197,...   \n4        1  [1545, 527, 595, 2687, 745, 588, 1, 2355, 2294...   \n\n                         user_mid  \\\n0  [1, 1, 1, 1, 0, 1, 1, 1, 1, 1]   \n1  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n2  [1, 1, 1, 1, 1, 0, 1, 1, 1, 1]   \n3  [0, 1, 1, 1, 1, 0, 1, 0, 1, 1]   \n4  [1, 1, 1, 0, 0, 1, 1, 1, 1, 1]   \n\n                                    user_mid_history  sequence_id  \n0                                                 []            0  \n1  [3186, 1270, 1721, 1022, 1836, 3408, 2804, 120...            1  \n2  [3186, 1270, 1721, 1022, 1836, 3408, 2804, 120...            2  \n3  [3186, 1270, 1721, 1022, 1836, 3408, 2804, 120...            3  \n4  [3186, 1270, 1721, 1022, 1836, 3408, 2804, 120...            4  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>slate_of_items</th>\n      <th>user_mid</th>\n      <th>user_mid_history</th>\n      <th>sequence_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>[3186, 1270, 1721, 1022, 2340, 1836, 3408, 280...</td>\n      <td>[1, 1, 1, 1, 0, 1, 1, 1, 1, 1]</td>\n      <td>[]</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>[720, 260, 919, 608, 2692, 1961, 2028, 3105, 9...</td>\n      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n      <td>[3186, 1270, 1721, 1022, 1836, 3408, 2804, 120...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>[1962, 2018, 150, 1028, 1097, 914, 1287, 2797,...</td>\n      <td>[1, 1, 1, 1, 1, 0, 1, 1, 1, 1]</td>\n      <td>[3186, 1270, 1721, 1022, 1836, 3408, 2804, 120...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>[661, 2918, 531, 3114, 2791, 2321, 1029, 1197,...</td>\n      <td>[0, 1, 1, 1, 1, 0, 1, 0, 1, 1]</td>\n      <td>[3186, 1270, 1721, 1022, 1836, 3408, 2804, 120...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>[1545, 527, 595, 2687, 745, 588, 1, 2355, 2294...</td>\n      <td>[1, 1, 1, 0, 0, 1, 1, 1, 1, 1]</td>\n      <td>[3186, 1270, 1721, 1022, 1836, 3408, 2804, 120...</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"   user_id                                     slate_of_items  \\\n0     4794  [2087, 1073, 951, 1230, 1256, 3362, 1276, 1304...   \n1     4794  [1923, 3763, 3702, 3693, 2804, 1196, 541, 1197...   \n2     4794  [1299, 1394, 3342, 1231, 3683, 1199, 1270, 316...   \n3     4794  [1259, 3072, 3505, 1291, 1674, 2348, 3753, 969...   \n4     4794  [1200, 1201, 1240, 457, 589, 377, 2951, 2985, ...   \n\n                         user_mid  \\\n0  [1, 1, 1, 1, 1, 1, 1, 1, 0, 1]   \n1  [1, 1, 1, 0, 1, 1, 1, 1, 1, 1]   \n2  [1, 1, 1, 1, 1, 0, 1, 0, 1, 1]   \n3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n4  [1, 1, 1, 1, 0, 1, 1, 0, 1, 1]   \n\n                                    user_mid_history  sequence_id  \n0  [593, 1198, 2683, 2355, 1911, 3543, 3615, 2757...            4  \n1  [593, 1198, 2683, 2355, 1911, 3543, 3615, 2757...            5  \n2  [593, 1198, 2683, 2355, 1911, 3543, 3615, 2757...            6  \n3  [593, 1198, 2683, 2355, 1911, 3543, 3615, 2757...            7  \n4  [593, 1198, 2683, 2355, 1911, 3543, 3615, 2757...            8  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>slate_of_items</th>\n      <th>user_mid</th>\n      <th>user_mid_history</th>\n      <th>sequence_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4794</td>\n      <td>[2087, 1073, 951, 1230, 1256, 3362, 1276, 1304...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 0, 1]</td>\n      <td>[593, 1198, 2683, 2355, 1911, 3543, 3615, 2757...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4794</td>\n      <td>[1923, 3763, 3702, 3693, 2804, 1196, 541, 1197...</td>\n      <td>[1, 1, 1, 0, 1, 1, 1, 1, 1, 1]</td>\n      <td>[593, 1198, 2683, 2355, 1911, 3543, 3615, 2757...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4794</td>\n      <td>[1299, 1394, 3342, 1231, 3683, 1199, 1270, 316...</td>\n      <td>[1, 1, 1, 1, 1, 0, 1, 0, 1, 1]</td>\n      <td>[593, 1198, 2683, 2355, 1911, 3543, 3615, 2757...</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4794</td>\n      <td>[1259, 3072, 3505, 1291, 1674, 2348, 3753, 969...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n      <td>[593, 1198, 2683, 2355, 1911, 3543, 3615, 2757...</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4794</td>\n      <td>[1200, 1201, 1240, 457, 589, 377, 2951, 2985, ...</td>\n      <td>[1, 1, 1, 1, 0, 1, 1, 0, 1, 1]</td>\n      <td>[593, 1198, 2683, 2355, 1911, 3543, 3615, 2757...</td>\n      <td>8</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(3953, 18)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(6041, 30)"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"display(train.describe())\ndisplay(test.describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:10:04.909981Z","iopub.execute_input":"2025-07-10T10:10:04.910356Z","iopub.status.idle":"2025-07-10T10:10:04.939370Z","shell.execute_reply.started":"2025-07-10T10:10:04.910332Z","shell.execute_reply":"2025-07-10T10:10:04.938807Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"            user_id   sequence_id\ncount  77906.000000  77906.000000\nmean    2425.262830     19.629194\nstd     1377.860511     22.665141\nmin        1.000000      0.000000\n25%     1230.000000      4.000000\n50%     2383.000000     12.000000\n75%     3651.000000     27.000000\nmax     4794.000000    230.000000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>sequence_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>77906.000000</td>\n      <td>77906.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>2425.262830</td>\n      <td>19.629194</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1377.860511</td>\n      <td>22.665141</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>1230.000000</td>\n      <td>4.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>2383.000000</td>\n      <td>12.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>3651.000000</td>\n      <td>27.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>4794.000000</td>\n      <td>230.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"            user_id   sequence_id\ncount  19477.000000  19477.000000\nmean    5426.233917     16.850542\nstd      358.788743     18.521978\nmin     4794.000000      0.000000\n25%     5105.000000      4.000000\n50%     5443.000000     11.000000\n75%     5741.000000     23.000000\nmax     6040.000000    126.000000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>sequence_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>19477.000000</td>\n      <td>19477.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>5426.233917</td>\n      <td>16.850542</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>358.788743</td>\n      <td>18.521978</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>4794.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>5105.000000</td>\n      <td>4.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>5443.000000</td>\n      <td>11.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>5741.000000</td>\n      <td>23.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>6040.000000</td>\n      <td>126.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# @title Support function\n\ndef set_random_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\ndef padding_and_clip(sequence, max_len, padding_direction = 'left'):\n    if len(sequence) < max_len:\n        sequence = [0] * (max_len - len(sequence)) + sequence if padding_direction == 'left' else sequence + [0] * (max_len - len(sequence))\n    sequence = sequence[-max_len:] if padding_direction == 'left' else sequence[:max_len]\n    # print(f\"sequence{sequence}\")\n    return sequence\n\ndef get_regularization(*modules):\n  \"\"\"\n  Customized L2 regularization\n  \"\"\"\n  reg = 0\n  for m in modules:\n    for p in m.parameters():\n      reg = torch.mean(p * p) + reg\n  return reg\n\ndef wrap_batch(batch, device):\n  \"\"\"\n  Build feed_dict from batch data and move data to device\n  \"\"\"\n  for k,val in batch.items():\n    if type(val).__module__ == np.__name__:\n        batch[k] = torch.from_numpy(val)\n    elif torch.is_tensor(val):\n        batch[k] = val\n    elif type(val) is list:\n        batch[k] = torch.tensor(val)\n    else:\n        continue\n    if batch[k].type() == \"torch.DoubleTensor\":\n        batch[k] = batch[k].float()\n    batch[k] = batch[k].to(device)\n  return batch\n\ndef sample_categorical_action(action_prob, candidate_ids, slate_size,\n                              with_replacement=True, batch_wise=False,\n                              return_idx=False):\n  '''\n  @input:\n  - action_prob: (B, L)\n  - candidate_ids: (B, L) or (1, L)\n  - slate_size: K\n  - with_replacement: sample with replacement\n  - batch_wise: do batch wise candidate selection\n  '''\n  if with_replacement:\n    # (K, B)\n    indices = Categorical(action_prob).sample(sample_shape = (slate_size,))\n    # (B, K)\n    indices = torch.transpose(indices, 0, 1)\n  else:\n    indices = torch.cat([torch.multinomial(prob, slate_size, replacement=False).view(1, -1) \\\n                         for prob in action_prob], dim = 0)\n  action = torch.gather(candidate_ids, 1, indices) if batch_wise else candidate_ids[indices]\n  if return_idx:\n    return action.detach(), indices.detach()\n  else:\n    return action.detach()\n\n\n##################\n#   Learning     #\n##################\n\nclass LinearScheduler(object):\n  def __init__(self, schedule_timesteps, final_p, initial_p=1.0):\n    self.schedule_timesteps = schedule_timesteps\n    self.final_p = final_p\n    self.initial_p = initial_p\n\n  def value(self, t):\n    '''\n    see Schedule.value\n    '''\n    fraction = min(float(t) / self.schedule_timesteps, 1.0)\n    return self.initial_p + fraction * (self.final_p - self.initial_p)\n","metadata":{"cellView":"form","id":"6uAs4kp4whrk","trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:10:04.941737Z","iopub.execute_input":"2025-07-10T10:10:04.942266Z","iopub.status.idle":"2025-07-10T10:10:04.953341Z","shell.execute_reply.started":"2025-07-10T10:10:04.942241Z","shell.execute_reply":"2025-07-10T10:10:04.952585Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# @title Plot Function\n\ndef smooth(values, window = 3):\n  left = window // 2\n  new_values = [np.mean(values[max(0,idx-left):min(idx-left+window,len(values))]) for idx in range(len(values))]\n  return new_values\n\n\ndef get_rl_training_info(log_path, training_losses = ['actor_loss', 'critic_loss']):\n  episode = []\n  average_total_reward, reward_variance, max_total_reward, min_total_reward, average_n_step, max_n_step, min_n_step \\\n          = [], [], [], [], [], [], []\n  training_loss_records = {k: [] for k in training_losses}\n  with open(log_path, 'r') as infile:\n    for line in tqdm(infile):\n      split = line.split('@')\n      # episode\n      episode.append(eval(split[0].split(':')[1]))\n      # episode report\n      episode_report = eval(split[1].strip()[len(\"episode report:\"):])\n      average_total_reward.append(episode_report['average_total_reward'])\n      reward_variance.append(episode_report['reward_variance'])\n      max_total_reward.append(episode_report['max_total_reward'])\n      min_total_reward.append(episode_report['min_total_reward'])\n      average_n_step.append(episode_report['average_n_step'])\n      max_n_step.append(episode_report['max_n_step'])\n      min_n_step.append(episode_report['min_n_step'])\n      # loss report\n      if training_losses:\n          loss_report = eval(split[2].strip()[len(\"step loss:\"):])\n          for k in training_losses:\n              training_loss_records[k].append(loss_report[k])\n  info = {\n      \"episode\": episode,\n      \"average_total_reward\": average_total_reward,\n      \"reward_variance\": reward_variance,\n      \"max_total_reward\": max_total_reward,\n      \"min_total_reward\": min_total_reward,\n      \"average_depth_per_episode\": average_n_step,\n      \"max_depth_per_episode\": max_n_step,\n      \"min_depth_per_episode\": min_n_step\n  }\n  if training_losses:\n      for k in training_losses:\n        info[k] = training_loss_records[k]\n  return info\n\ndef plot_multiple_line(legend_names, list_of_stats, x_name, ncol = 2, row_height = 4, save_path=\"/kaggle/working/fig/rl.png\"):\n  '''\n  @input:\n  - legend_names: [legend]\n  - list_of_stats: [{field_name: [values]}]\n  - x_name: x-axis field_name\n  - ncol: number of subplots in each row\n  '''\n  plt.rcParams.update({'font.size': 14})\n  assert ncol > 0\n  features = list(list_of_stats[0].keys())\n  features.remove(x_name)\n  N = len(features)\n  fig_height = 12 // ncol if len(features) == 1 else row_height*((N-1)//ncol+1)\n  plt.figure(figsize = (16, fig_height))\n  for i,field in enumerate(features):\n      plt.subplot((N-1)//ncol+1,ncol,i+1)\n      minY,maxY = float('inf'),float('-inf')\n      for j,L in enumerate(legend_names):\n          X = list_of_stats[j][x_name]\n          value_list = list_of_stats[j][field]\n          minY,maxY = min(minY,min(value_list)),max(maxY,max(value_list))\n          plt.plot(X[:len(value_list)], value_list, label = L)\n      plt.ylabel(field)\n      plt.xlabel(x_name)\n      scale = 1e-4 + maxY - minY\n      plt.ylim(minY - scale * 0.05, maxY + scale * 0.05)\n      plt.legend()\n  plt.savefig(save_path)\n  plt.show()\n  ","metadata":{"cellView":"form","id":"I_-339N7XCzA","trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:10:04.954179Z","iopub.execute_input":"2025-07-10T10:10:04.954483Z","iopub.status.idle":"2025-07-10T10:10:04.971020Z","shell.execute_reply.started":"2025-07-10T10:10:04.954456Z","shell.execute_reply":"2025-07-10T10:10:04.970274Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# @title Socrer function\ndef dot_scorer(action_emb, item_emb, item_dim):\n  '''\n  score = item_emb * weight\n\n  @input:\n  - action_emb: (B, i_dim)\n  - item_emb: (B, L, i_dim) or (1, L, i_dim)\n  @output:\n  - score: (B, L)\n  '''\n  output = torch.sum(action_emb.view(-1, 1, item_dim) * item_emb, dim=-1)\n\n  return output","metadata":{"cellView":"form","id":"HVUXZjicg3kg","trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:10:04.971643Z","iopub.execute_input":"2025-07-10T10:10:04.971831Z","iopub.status.idle":"2025-07-10T10:10:04.985762Z","shell.execute_reply.started":"2025-07-10T10:10:04.971817Z","shell.execute_reply":"2025-07-10T10:10:04.985063Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# @title Dense Neural Network\n\nclass DNN(nn.Module):\n  def __init__(self, in_dim, hidden_dims, out_dim=1, dropout_rate= 0.,\n               do_batch_norm=True):\n    super(DNN, self).__init__()\n    self.in_dim = in_dim\n    layers = []\n\n    for hidden_dim in hidden_dims:\n      linear_layer = nn.Linear(in_dim, hidden_dim)\n\n      layers.append(linear_layer)\n      in_dim = hidden_dim\n      layers.append(nn.ReLU())\n      if dropout_rate > 0:\n        layers.append(nn.Dropout(dropout_rate))\n      if do_batch_norm:\n        layers.append(nn.LayerNorm(hidden_dim))\n\n    # Prediction layer\n    last_layer = nn.Linear(in_dim, out_dim)\n    layers.append(last_layer)\n\n    self.layers = nn.Sequential(*layers)\n\n  def forward(self, x):\n    x = x.view(-1, self.in_dim)\n    logit = self.layers(x)\n    return logit\n\n","metadata":{"cellView":"form","id":"t-f8lshQUlDz","trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:10:04.986446Z","iopub.execute_input":"2025-07-10T10:10:04.986680Z","iopub.status.idle":"2025-07-10T10:10:04.994585Z","shell.execute_reply.started":"2025-07-10T10:10:04.986665Z","shell.execute_reply":"2025-07-10T10:10:04.993886Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# @title # Data Reader class\n\nclass BaseDataReader(Dataset):\n  def __init__(self, params):\n    self.phase = 'train'\n    self.n_worker = params['n_worker']\n    self._read_data(params)\n\n  def _read_data(self, params):\n    self.data = dict()\n    self.data['train'] = params['train']\n    self.data['val'] = params['val']\n\n\n  def __getitem__(self, idx):\n    pass\n\n  def __len__(self):\n    return len(self.data[self.phase])\n\n  def get_statistics(self):\n    return {'length': len(self)}\n\n  def set_phase(self, phase):\n    assert phase in ['train', 'val', 'test']\n    self.phase = phase\n\n","metadata":{"cellView":"form","id":"EcNXI4e7tyGR","trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:10:04.995420Z","iopub.execute_input":"2025-07-10T10:10:04.995677Z","iopub.status.idle":"2025-07-10T10:10:05.008775Z","shell.execute_reply.started":"2025-07-10T10:10:04.995654Z","shell.execute_reply":"2025-07-10T10:10:05.008111Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# @title ML1M Data Reader\n\nclass ML1MDataReader(BaseDataReader):\n        \n    def __init__(self, params):\n        '''\n        - from BaseReader:\n            - phase\n            - data: will add Position column\n        '''\n        super().__init__(params)\n        self.max_seq_len = params['max_seq_len']\n        \n    def _read_data(self, params):\n        # read data_file\n        super()._read_data(params)\n        print(\"Load item meta data\")\n        self.item_meta = params['item_meta']\n        self.user_meta = params['user_meta']\n        self.item_vec_size = len(self.item_meta[0])\n        self.user_vec_size = len(self.user_meta[0])\n        self.portrait_len = len(self.user_meta[0])\n    \n    ###########################\n    #        Iterator         #\n    ###########################\n        \n    def __getitem__(self, idx):\n        user_ID, slate_of_items, user_feedback, user_history, sequence_id = self.data[self.phase].iloc[idx]\n        user_profile = self.user_meta[user_ID]\n    \n        exposure = eval(slate_of_items)\n    \n        history = eval(user_history)\n    \n        hist_length = len(history)\n        history = padding_and_clip(history, self.max_seq_len)\n        # print(f\"history{}\")\n        feedback = eval(user_feedback)\n    \n        record = {\n            'timestamp': int(1), # timestamp is irrelevant, just a hack temporal\n            'exposure': np.array(exposure).astype(int),\n            'exposure_features': self.get_item_list_meta(exposure).astype(float),\n            'feedback': np.array(feedback).astype(float),\n            'history': np.array(history).astype(int),\n            'history_features': self.get_item_list_meta(history).astype(float),\n            'history_length': int(min(hist_length, self.max_seq_len)),\n            'user_profile': np.array(user_profile)\n            }\n        return record\n        \n    def get_item_list_meta(self, item_list):\n        return np.array([self.item_meta[item] for item in item_list])\n    \n    def get_statistics(self):\n        '''\n        - n_user\n        - n_item\n        - s_parsity\n        - from BaseReader:\n            - length\n            - fields\n        '''\n        stats = super().get_statistics()\n        stats['length'] = len(self.data[self.phase])\n        stats['n_item'] = len(self.item_meta) - 1\n        stats['item_vec_size'] = self.item_vec_size\n        stats['user_portrait_len'] = self.user_vec_size\n        stats['max_seq_len'] = self.max_seq_len\n        return stats\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:10:05.009577Z","iopub.execute_input":"2025-07-10T10:10:05.010067Z","iopub.status.idle":"2025-07-10T10:10:05.024148Z","shell.execute_reply.started":"2025-07-10T10:10:05.010048Z","shell.execute_reply":"2025-07-10T10:10:05.023488Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## Model","metadata":{"id":"BZu5Gr-atrEE"}},{"cell_type":"code","source":"# @title Base Model\n\nclass BaseModel(nn.Module):\n  def __init__(self, reader, params):\n    super().__init__()\n    self.display_name = \"BaseModel\"\n    self.reader = reader\n    self.model_path = params['model_path']\n    self.loss_type = params['loss_type']\n    self.l2_coef = params['l2_coef']\n    self.device = params['device']\n    self.sigmoid = nn.Sigmoid()\n    self._define_params(reader, params)\n\n  def get_regularization(self, *modules):\n    return get_regularization(*modules)\n\n  def do_forward_and_loss(self, feed_dict: dict) -> dict:\n    '''\n    Used during training to compute predictions and the loss.\n    '''\n    out_dict = self.get_forward(feed_dict)\n    out_dict['loss'] = self.get_loss(feed_dict, out_dict)\n    return out_dict\n\n  def forward(self, feed_dict: dict, return_prob=True) -> dict:\n    '''\n      Used during evaluation/prediction to generate predictions and probabilities\n    '''\n    out_dict = self.get_forward(feed_dict)\n    if return_prob:\n      out_dict['probs'] = self.sigmoid(out_dict['preds'])\n    return out_dict\n\n  def wrap_batch (self, batch):\n    '''\n    Build feed_dict from batch data and move data to self.device\n    '''\n    for k, val in batch.items():\n      if type(val).__module__ == np.__name__:\n        batch[k] = torch.from_numpy(val)\n      elif torch.is_tensor(val):\n        batch[k] = val\n      elif type(val) is list:\n        batch[k] = torch.tensor(val)\n      else:\n        continue # No compatiable type\n      if batch[k].type() == 'torch.DoubleTensor':\n        batch[k] = batch[k].type(torch.FloatTensor)\n      batch[k] = batch[k].to(self.device)\n    return batch\n\n  def save_checkpoint(self):\n    torch.save({\n        \"model_state_dict\": self.state_dict(),\n        \"optimizer_state_dict\": self.optimizer.state_dict(),\n    }, self.model_path + \".checkpoint\")\n\n  def load_checkpoint(self, model_path, with_optimizer=True):\n    checkpoint = torch.load(model_path + \".checkpoint\",\n                            map_location=self.device)\n    self.load_state_dict(checkpoint[\"model_state_dict\"])\n    if with_optimizer:\n      self.optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n    self.model_path = model_path\n\n  def _define_params(self, reader, params):\n    pass\n\n  def get_forward(self, feed_dict: dict) -> dict:\n    pass\n\n  def get_loss(self, feed_dict: dict, out_dict: dict) -> dict:\n    pass","metadata":{"cellView":"form","id":"aE05j2QDr6lh","trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:10:05.024827Z","iopub.execute_input":"2025-07-10T10:10:05.025020Z","iopub.status.idle":"2025-07-10T10:10:05.040773Z","shell.execute_reply.started":"2025-07-10T10:10:05.025006Z","shell.execute_reply":"2025-07-10T10:10:05.039927Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# @title ML1M Response Model\n\nclass ML1MUserResponse(BaseModel):\n  def __init__(self, reader, params):\n    super().__init__(reader, params)\n    self.bce_loss = nn.BCEWithLogitsLoss(reduction= 'none')\n\n  def _define_params(self, reader, params):\n    stats = reader.get_statistics()\n    print(stats)\n    self.potrait_len = stats['user_portrait_len']\n    self.item_dim = stats['item_vec_size']\n    self.feature_dim = params['feature_dim']\n    self.hidden_dim = params['hidden_dims']\n    self.attn_n_head = params['attn_n_head']\n    self.dropout_rate = params['dropout_rate']\n    self.uEmb = nn.Embedding.from_pretrained(torch.FloatTensor(self.reader.user_meta), freeze=False)\n    self.iEmb = nn.Embedding.from_pretrained(torch.FloatTensor(self.reader.item_meta), freeze=False)\n\n    # fuse information\n    self.concat_layer = nn.Linear(self.feature_dim * 2, self.feature_dim)\n\n    # portrait embedding\n    self.portrait_encoding_layer = DNN(self.potrait_len, self.hidden_dim,\n                                        self.feature_dim, self.dropout_rate,\n                                        do_batch_norm= False)\n    # item embedding\n    self.item_emb_layer = nn.Linear(self.item_dim, self.feature_dim)\n\n    # user history encoder\n    self.seq_self_attn_layer = nn.MultiheadAttention(self.feature_dim, self.attn_n_head, batch_first= True)\n    self.seq_user_attn_layer = nn.MultiheadAttention(self.feature_dim, self.attn_n_head, batch_first= True)\n\n    self.loss = []\n\n  def get_forward(self, feed_dict: dict) -> dict:\n    user_emb = self.portrait_encoding_layer(feed_dict['user_profile']).view(-1, 1, self.feature_dim)\n    history_item_emb = self.item_emb_layer(feed_dict['history_features'])\n\n    seq_encoding, attn_weight = self.seq_self_attn_layer(history_item_emb, history_item_emb, history_item_emb)\n\n    user_interest, attn_weight = self.seq_user_attn_layer(user_emb, seq_encoding, seq_encoding)\n\n    user_interest = torch.concat([user_interest, user_emb], axis=-1)\n    user_interest = self.concat_layer(user_interest)\n\n    exposure_item_emb = self.item_emb_layer(feed_dict['exposure_features'])\n\n    score = torch.sum(exposure_item_emb * user_interest, dim=-1)\n\n    # regularization\n    reg = self.get_regularization(self.uEmb, self.iEmb, self.portrait_encoding_layer,\n                                  self.item_emb_layer, self.seq_user_attn_layer,\n                                  self.seq_self_attn_layer)\n    return {'preds': score, 'reg': reg}\n\n  def get_loss(self, feed_dict: dict, out_dict: dict):\n    preds, reg = out_dict[\"preds\"].view(-1), out_dict[\"reg\"]\n    target = feed_dict['feedback'].view(-1).to(torch.float)\n\n    # print(f\"preds: {self.sigmoid(preds)}\")\n    # print(f\"target: \", target)\n\n    loss = torch.mean(self.bce_loss(self.sigmoid(preds), target))\n    # print(f\"loss: {loss} l2: {reg} l2*coef: {self.l2_coef * reg}\")\n    self.loss.append(loss.item())\n    loss = loss + self.l2_coef * reg\n    return loss\n","metadata":{"cellView":"form","id":"EM4gIUpwL6Bu","trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:10:05.041516Z","iopub.execute_input":"2025-07-10T10:10:05.041790Z","iopub.status.idle":"2025-07-10T10:10:05.056434Z","shell.execute_reply.started":"2025-07-10T10:10:05.041770Z","shell.execute_reply":"2025-07-10T10:10:05.055790Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"## Train Environment","metadata":{"id":"EPzAu5L7uFO5"}},{"cell_type":"code","source":"params = dict()\nparams['train'] = train\nparams['val'] = test\nparams['item_meta'] = item_info\nparams['user_meta'] = user_info\nparams['n_worker'] = 4\nparams['max_seq_len'] = 50\n\nparams['loss_type'] = 'bce'\nparams['device'] = device\nparams['l2_coef'] = 0.001\nparams['lr'] = 0.0003\nparams['feature_dim'] = 16\nparams['hidden_dims'] = [256]\nparams['attn_n_head'] = 2\nparams['batch_size'] = 128\nparams['seed'] = 26\nparams['epoch'] = 2\nparams['dropout_rate'] = 0.2\nparams['model_path'] = os.path.join(path_to_output, \n                          f\"env/ml1m_user_env_lr{params['lr']}_reg{params['l2_coef']}.model\")\nset_random_seed(params['seed'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:10:05.057110Z","iopub.execute_input":"2025-07-10T10:10:05.057347Z","iopub.status.idle":"2025-07-10T10:10:05.074832Z","shell.execute_reply.started":"2025-07-10T10:10:05.057306Z","shell.execute_reply":"2025-07-10T10:10:05.074280Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# @title Train user response\nreader = ML1MDataReader(params)\nmodel = ML1MUserResponse(reader, params).to(device)\n\n\n# reader = RL4RSDataReader(params)\n# model = RL4RSUserResponse(reader, params).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\nmodel.optimizer = optimizer\n\n\nepo = 0\nwhile epo < params['epoch']:\n  print(f\"epoch {epo} is training\")\n  epo += 1\n\n  model.train()\n  reader.set_phase(\"train\")\n  train_loader = DataLoader(reader, params['batch_size'], shuffle = True, pin_memory = True,\n                            num_workers= params['n_worker'])\n\n  t1 = time()\n  pbar = tqdm(total=len(train_loader.dataset))\n  step_loss = []\n  for i, batch_data in enumerate(train_loader):\n    optimizer.zero_grad()\n    wrapped_batch = wrap_batch(batch_data, device)\n\n    out_dict = model.do_forward_and_loss(wrapped_batch)\n    loss = out_dict['loss']\n    loss.backward()\n    step_loss.append(loss.item())\n    optimizer.step()\n    pbar.update(params['batch_size'])\n    # print(model.loss)\n    # if (i + 1) % 10 == 0:\n      # print(f\"Iteration {i + 1}, loss {np.mean(step_loss[-100:])}\")\n  pbar.close()\n    # print(\"Epoch {}; time {:.4f}\".format(epo, time() - t1))\n\n  # validation\n  t2 = time()\n  reader.set_phase(\"val\")\n  val_loader = DataLoader(reader, params['batch_size'], shuffle = False, pin_memory = False,\n                          num_workers= params['n_worker'])\n  valid_probs, valid_true =  [], []\n  pbar = tqdm(total = len(val_loader.dataset))\n  with torch.no_grad():\n    for i, batch_data in enumerate(val_loader):\n      wrapped_batch = wrap_batch(batch_data, device)\n      out_dict = model.forward(wrapped_batch)\n      valid_probs.append(out_dict['probs'].cpu().numpy())\n      valid_true.append(batch_data['feedback'].cpu().numpy())\n      pbar.update(params['batch_size'])\n  pbar.close()\n  auc = roc_auc_score(np.concatenate(valid_true), np.concatenate(valid_probs))\n  print(f\"epoch {epo} validating\" + \"; auc: {:.4f}\".format(np.mean(auc)))\n  model.save_checkpoint()\n  \n","metadata":{"id":"pUiyHg58pQk3","outputId":"de7d8a66-a100-4cfe-b476-963ac617cda0","trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:10:05.075500Z","iopub.execute_input":"2025-07-10T10:10:05.075693Z","iopub.status.idle":"2025-07-10T10:10:48.501477Z","shell.execute_reply.started":"2025-07-10T10:10:05.075678Z","shell.execute_reply":"2025-07-10T10:10:48.500535Z"}},"outputs":[{"name":"stdout","text":"Load item meta data\n{'length': 77906, 'n_item': 3952, 'item_vec_size': 18, 'user_portrait_len': 30, 'max_seq_len': 50}\nepoch 0 is training\n","output_type":"stream"},{"name":"stderr","text":"77952it [00:16, 4845.85it/s]                           \n19584it [00:04, 4671.03it/s]                           \n","output_type":"stream"},{"name":"stdout","text":"epoch 1 validating; auc: 0.5956\nepoch 1 is training\n","output_type":"stream"},{"name":"stderr","text":"77952it [00:16, 4783.43it/s]                           \n19584it [00:04, 4802.57it/s]                           ","output_type":"stream"},{"name":"stdout","text":"epoch 2 validating; auc: 0.5986\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# @title Plot loss\n\ndef plot_line(labels, data_dicts, x_name='X-Axis', y_name='Y-Axis'):\n    plt.figure(figsize=(20, 6))\n    for label, data_dict in zip(labels, data_dicts):\n        for key, values in data_dict.items():\n            plt.plot(values, label=f\"{label} - {key}\")\n    plt.xlabel(x_name)\n    plt.ylabel(y_name)\n    plt.title('Loss Plot')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n\nloss_values = model.loss\nsmoothed_loss = smooth(np.array(loss_values), window=300)  # Adjust smoothness if needed\n\ninfo = {'Loss': smoothed_loss}\nplot_line(['Model'], [info], x_name='Epoch', y_name='Loss Value')\n\n","metadata":{"id":"eqgPbcAsID16","outputId":"ca1dea56-cb24-4412-b8bb-133c693198cd","trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:10:48.502552Z","iopub.execute_input":"2025-07-10T10:10:48.503416Z","iopub.status.idle":"2025-07-10T10:10:48.814044Z","shell.execute_reply.started":"2025-07-10T10:10:48.503390Z","shell.execute_reply":"2025-07-10T10:10:48.813307Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 2000x600 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":17},{"cell_type":"markdown","source":"## Train HAC","metadata":{}},{"cell_type":"code","source":"# @title Cost function\ndef mean_with_cost(feedback, zero_reward_cost=0.1):\n  B, L = feedback.shape\n  cost = torch.zeros_like(feedback)\n  cost[feedback == 0] = -zero_reward_cost\n  reward = torch.mean(feedback + cost, dim=-1)\n  return reward\n\ndef nsw(avg_r, min_r, lambda_nsw=1e-4, epsilon=1e-8):\n    r_vec = torch.stack([avg_r, min_r + lambda_nsw], dim=-1)\n    r_vec = torch.clamp(r_vec, min=epsilon)\n    return torch.sum(torch.log(r_vec), dim=-1)","metadata":{"cellView":"form","id":"Q7IfdzX4LFTk","trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:10:48.814936Z","iopub.execute_input":"2025-07-10T10:10:48.815185Z","iopub.status.idle":"2025-07-10T10:10:48.820506Z","shell.execute_reply.started":"2025-07-10T10:10:48.815169Z","shell.execute_reply":"2025-07-10T10:10:48.819668Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# @title BaseRL Environment\nclass BaseEnv():\n  def __init__(self, params):\n    super().__init__()\n    self.reward_func = params['reward_function']\n    self.max_step_per_episode = params['max_step']\n    self.initial_temper = params[\"initial_temper\"]\n\n  def reset(self, paras):\n    pass\n  def step(self, action):\n    pass","metadata":{"cellView":"form","id":"KP_0zmoEFwta","trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:10:48.821373Z","iopub.execute_input":"2025-07-10T10:10:48.821627Z","iopub.status.idle":"2025-07-10T10:10:48.834723Z","shell.execute_reply.started":"2025-07-10T10:10:48.821601Z","shell.execute_reply":"2025-07-10T10:10:48.834072Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"#### 1. Environment define","metadata":{}},{"cell_type":"code","source":"# @title ML1M Environment\n\n\nclass ML1MEnvironment(BaseEnv):\n  def __init__(self, params):\n    super().__init__(params)\n    self.reader = ML1MDataReader(params)\n    self.user_response_model = ML1MUserResponse(self.reader, params)\n    checkpoint = torch.load(params['model_path'] + \".checkpoint\", map_location=device)\n    self.user_response_model.load_state_dict(checkpoint[\"model_state_dict\"])\n    self.user_response_model.to(device)\n    self.n_worker = params['n_worker']\n\n    # spaces\n    stats = self.reader.get_statistics()\n    self.action_space = {'item_id': ('nomial', stats['n_item']),\n                         'item_feature': ('continuous', stats['item_vec_size'], 'normal')}\n    self.observation_space = {'user_profile': ('continuous', stats['user_portrait_len'], 'positive'),\n                              'history': ('sequence', stats['max_seq_len'], ('continuous', stats['item_vec_size']))}\n\n  def reset(self, params = {'batch_size': 1, 'empty_history': True}):\n      self.empty_history_flag = params['empty_history'] if 'empty_history' in params else True\n      BS = params['batch_size']\n      observation = {'batch_size': BS}\n      if 'sample' in params:\n          sample_info = params['sample']\n      else:\n          self.batch_iter = iter(DataLoader(self.reader, batch_size = BS, shuffle = True,\n                                            pin_memory = True, num_workers = self.n_worker))\n          sample_info = next(self.batch_iter)\n          sample_info = wrap_batch(sample_info, device = self.user_response_model.device)\n      self.current_observation = {\n          'user_profile': sample_info['user_profile'],  # (B, user_dim)\n          'history': sample_info['history'],  # (B, H)\n          'history_features': sample_info['history_features'], # (B, H, item_dim)\n          'cummulative_reward': torch.zeros(BS).to(self.user_response_model.device),\n          'min_reward': torch.full((BS,), float('inf'), device=self.user_response_model.device),\n          'temper': torch.ones(BS).to(self.user_response_model.device) * self.initial_temper,\n          'step': torch.zeros(BS).to(self.user_response_model.device),\n      }\n      self.reward_history = [0.]\n      self.step_history = [0.]\n      return copy.deepcopy(self.current_observation)\n\n\n  def sample_user(self, n_user, empty_history = False):\n    '''\n    Sample random users and their history\n    '''\n    random_rows = np.random.randint(0, len(self.reader.data['train']), n_user)\n    return self.pick_user(random_rows, empty_history)\n\n  def pick_user(self, rows, empty_history = False):\n    '''\n    Pick users and their history\n    '''\n    raw_portrait = [self.reader.user_meta[self.reader.data['train']['user_id'][rowid]]\n                    for rowid in rows]\n\n    portrait = np.array(raw_portrait)\n\n    history = []\n    history_features = []\n    for rowid in rows:\n      H = [] if empty_history else eval(f\"{self.reader.data['train']['user_mid_history'][rowid]}\")\n      H = padding_and_clip(H, self.reader.max_seq_len)\n      history.append(H)\n\n      history_features.append(self.reader.get_item_list_meta(H).astype(float))\n      return {'user_profile': portrait,\n              'history': history,\n              'history_features': np.array(history_features)}\n\n  def step(self, step_dict):\n    '''\n    @input:\n    - step_dict: {'action': (B, slate_size),\n                    'action_features': (B, slate_size, item_dim) }\n    '''\n    # actions (exposures)\n    action = step_dict['action'] # (B, slate_size), should be item ids only\n    action_features = step_dict['action_features']\n    batch_data = {\n        'user_profile': self.current_observation['user_profile'],\n        'history_features': self.current_observation['history_features'],\n        'exposure_features': action_features\n    }\n    # URM forward\n    with torch.no_grad():\n        output_dict = self.user_response_model(batch_data)\n        response = torch.bernoulli(output_dict['probs']) # (B, slate_size)\n        probs_under_temper = output_dict['probs'] # * prob_scale\n        response = torch.bernoulli(probs_under_temper).detach() # (B, slate_size)\n\n        # reward (B,)\n        immediate_reward = self.reward_func(response).detach()\n\n        self.current_observation['min_reward'] = torch.min(immediate_reward, self.current_observation['min_reward'])\n\n        # (B, H+slate_size)\n        H_prime = torch.cat((self.current_observation['history'], action), dim = 1)\n        # (B, H+slate_size, item_dim)\n        H_prime_features = torch.cat((self.current_observation['history_features'], action_features), dim = 1)\n        # (B, H+slate_size)\n        F_prime = torch.cat((torch.ones_like(self.current_observation['history']), response), dim = 1).to(torch.long)\n        # vector, vector\n        row_indices, col_indices = (F_prime == 1).nonzero(as_tuple=True)\n        # (B,), the number of positive iteraction as history length\n        L = F_prime.sum(dim = 1)\n\n        # user history update\n        offset = 0\n        newH = torch.zeros_like(self.current_observation['history'])\n        newH_features = torch.zeros_like(self.current_observation['history_features'])\n        for row_id in range(action.shape[0]):\n            right = offset + L[row_id]\n            left = right - self.reader.max_seq_len\n            newH[row_id] = H_prime[row_id, col_indices[left:right]]\n            newH_features[row_id] = H_prime_features[row_id,col_indices[left:right],:]\n            offset += L[row_id]\n        self.current_observation['history'] = newH\n        self.current_observation['history_features'] = newH_features\n        self.current_observation['cummulative_reward'] += immediate_reward\n\n        # temper update for leave model\n        temper_down = (-immediate_reward+1) * response.shape[1] + 1\n#             temper_down = -(torch.sum(response, dim = 1) - response.shape[1] - 1)\n#             temper_down = torch.abs(torch.sum(response, dim = 1) - response.shape[1] * self.temper_sweet_point) + 1\n        self.current_observation['temper'] -= temper_down\n        # leave signal\n        done_mask = self.current_observation['temper'] < 1\n        # step update\n        self.current_observation['step'] += 1\n\n        # update rows where user left\n#             refresh_rows = done_mask.nonzero().view(-1)\n#             print(f\"#refresh: {refresh_rows}\")\n        if done_mask.sum() > 0:\n            final_rewards = self.current_observation['cummulative_reward'][done_mask].detach().cpu().numpy()\n            final_steps = self.current_observation['step'][done_mask].detach().cpu().numpy()\n            self.reward_history.append(final_rewards[-1])\n            self.step_history.append(final_steps[-1])\n            # sample new users to fill in the blank\n            new_sample_flag = False\n            try:\n                sample_info = next(self.iter)\n                if sample_info['user_profile'].shape[0] != done_mask.shape[0]:\n                    new_sample_flag = True\n            except:\n                new_sample_flag = True\n            if new_sample_flag:\n                self.iter = iter(DataLoader(self.reader, batch_size = done_mask.shape[0], shuffle = True,\n                                            pin_memory = True, num_workers = params[\"n_worker\"]))\n                sample_info = next(self.iter)\n            sample_info = wrap_batch(sample_info, device = self.user_response_model.device)\n            for obs_key in ['user_profile', 'history', 'history_features']:\n                self.current_observation[obs_key][done_mask] = sample_info[obs_key][done_mask]\n            self.current_observation['cummulative_reward'][done_mask] *= 0\n            self.current_observation['min_reward'][done_mask] = float('inf')\n            self.current_observation['temper'][done_mask] *= 0\n            self.current_observation['temper'][done_mask] += self.initial_temper\n        self.current_observation['step'][done_mask] *= 0\n#         print(f\"step: {self.current_observation['step']}\")\n    return copy.deepcopy(self.current_observation), immediate_reward, done_mask, {'response': response}\n\n\n  def stop(self):\n    self.iter = None\n\n  def get_new_iterator(self, B):\n    return iter(DataLoader(self.reader, batch_size = B, shuffle = True,\n                              pin_memory = True, num_workers = params['n_worker']))\n","metadata":{"cellView":"form","id":"zBhnDCBm0ESQ","trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:10:48.835580Z","iopub.execute_input":"2025-07-10T10:10:48.835815Z","iopub.status.idle":"2025-07-10T10:10:48.858321Z","shell.execute_reply.started":"2025-07-10T10:10:48.835791Z","shell.execute_reply":"2025-07-10T10:10:48.857621Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# @title Self-Attentive Sequential Recommendation\n\nclass SASRec(nn.Module):\n  def __init__(self, environment, params):\n    super().__init__()\n    self.n_layer = params['sasrec_n_layer']\n    self.d_model = params['sasrec_d_model']\n    self.n_head = params['sasrec_n_head']\n    self.dropout_rate = params['sasrec_dropout']\n    self.d_forward = params['sasrec_d_forward']\n\n    # item space\n    self.item_space = environment.action_space['item_id'][1]\n    self.item_dim = environment.action_space['item_feature'][1]\n    self.maxlen = environment.observation_space['history'][1]\n    self.state_dim = self.d_model\n    self.action_dim = self.d_model\n\n    # policy network modules\n    self.item_map = nn.Linear(self.item_dim, self.d_model)\n    self.pos_emb = nn.Embedding(self.maxlen, self.d_model)\n    self.pos_emb_getter = torch.arange(self.maxlen, dtype = torch.long)\n    self.emb_dropout = nn.Dropout(self.dropout_rate)\n    self.emb_norm = nn.LayerNorm(self.d_model)\n    self.attn_mask = ~torch.tril(torch.ones((self.maxlen, self.maxlen), dtype=torch.bool))\n    encoder_layer = nn.TransformerEncoderLayer(d_model=self.d_model,\n                                               nhead=self.n_head,\n                                               dim_feedforward= self.d_forward,\n                                               dropout=self.dropout_rate,\n                                               batch_first = True\n                                               )\n    self.transformer = nn.TransformerEncoder(encoder_layer= encoder_layer,\n                                             num_layers = self.n_layer)\n\n  def score(self, action_emb, item_emb, do_softmax=True):\n    item_emb = self.item_map(item_emb)\n    output = dot_scorer(action_emb, item_emb, self.d_model)\n    if do_softmax:\n      return torch.softmax(output, dim=-1)\n    else:\n      return output\n\n  def get_scorer_parameters(self):\n    return self.item_map.parameters()\n\n  def encode_state(self, feed_dict):\n    user_history = feed_dict['history_features']\n    # (1, H, d_model)\n    # for item in feed_dict.items():\n    #   print(item)\n    # print(\"user_history device:\", user_history.device)\n    # print(\"self.pos_emb_getter device:\", self.pos_emb_getter.device)\n    # print(\"self.pos_emb device\", self.pos_emb.device)\n\n    pos_emb = self.pos_emb(self.pos_emb_getter.to(user_history.device)).view(1, self.maxlen, self.d_model)\n\n    # (B, H, d_model)\n    history_item_emb = self.item_map(user_history).view(-1, self.maxlen, self.d_model)\n    history_item_emb = self.emb_norm(self.emb_dropout(history_item_emb + pos_emb))\n\n    # (B, H, d_model)\n    output_seq = self.transformer(history_item_emb, mask = self.attn_mask.to(user_history.device))\n\n    return {'output_seq': output_seq, 'state_emb': output_seq[:, -1, :]}\n\n  def forward(self, feed_dict):\n    '''\n    @input\n    - feed_dict: {'user_profile': (B, user_dim),\n                  'history_features': (B, H, item_dim),\n                  'history_mask': (B),\n                  'candicate_features': (B, L, item_dim) or (1, L, item_dim)\n                  }\n    @model\n    - user_profile --> user_emb (B, 1, f_dim)\n    - hisotry_items --> history_item_emb (B, H, f_dim)\n    - (Q:user_emb, K&V: history_item_emb) --(multi-head attn) --> user_state(B, 1, feature_dim)\n    - user_state --> action_prob (B, n_item)\n    '''\n    hist_enc = self.encode_state(feed_dict)\n\n    # user embedding (B, 1, d_model)\n    user_state = hist_enc['state_emb'].view(-1, self.d_model)\n\n    # action embedding (B, d_model)\n    action_emb = user_state\n\n    # regularization\n    reg = get_regularization(self.item_map, self.transformer)\n\n    out_dict = {\n        'action_emb': action_emb,\n        'state_emb': user_state,\n        'seq_emb': hist_enc['output_seq'],\n        'reg': reg\n    }\n    return out_dict","metadata":{"cellView":"form","id":"3BvTCi5yZvxS","trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:10:48.859206Z","iopub.execute_input":"2025-07-10T10:10:48.859503Z","iopub.status.idle":"2025-07-10T10:10:48.880154Z","shell.execute_reply.started":"2025-07-10T10:10:48.859485Z","shell.execute_reply":"2025-07-10T10:10:48.879383Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"\n# @title General Critic class\nclass GeneralCritic(nn.Module):\n  def __init__(self, policy, params):\n    super().__init__()\n    self.state_dim = policy.state_dim\n    self.action_dim = policy.action_dim\n    self.net = DNN(self.state_dim + self.action_dim, params['critic_hidden_dims'], 1,\n                   dropout_rate=params['critic_dropout_rate'], do_batch_norm=True)\n\n  def forward(self, feed_dict):\n    '''\n    @input:\n    - feed_dict: {'state_emb': (B, state_dim), 'action_emb': (B, action_dim)}\n    '''\n    state_emb = feed_dict['state_emb']\n    action_emb = feed_dict['action_emb'].view(-1, self.action_dim)\n\n    Q = self.net(torch.cat((state_emb, action_emb), dim = -1)).view(-1)\n\n    reg = get_regularization(self.net)\n    return {'q': Q, 'reg': reg}","metadata":{"cellView":"form","id":"Dz4XwAAtzx1d","trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:10:48.883826Z","iopub.execute_input":"2025-07-10T10:10:48.884156Z","iopub.status.idle":"2025-07-10T10:10:48.897618Z","shell.execute_reply.started":"2025-07-10T10:10:48.884137Z","shell.execute_reply":"2025-07-10T10:10:48.896964Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# @title Vector-valued Critic class for MORL\nclass VectorCritic(nn.Module):\n  def __init__(self, policy, params, n_objectives=2):\n    super().__init__()\n    self.state_dim = policy.state_dim\n    self.action_dim = policy.action_dim\n    self.n_objectives = n_objectives\n\n    # Output now has shape (B, n_objectives)\n    self.net = DNN(\n        self.state_dim + self.action_dim,\n        params['critic_hidden_dims'],\n        output_dim=n_objectives,\n        dropout_rate=params['critic_dropout_rate'],\n        do_batch_norm=True\n    )\n\n  def forward(self, feed_dict):\n    '''\n    @input:\n    - feed_dict: {'state_emb': (B, state_dim), 'action_emb': (B, action_dim)}\n    @output:\n    - {'q_vector': (B, n_objectives), 'reg': scalar}\n    '''\n    state_emb = feed_dict['state_emb']\n    action_emb = feed_dict['action_emb'].view(-1, self.action_dim)\n\n    q_vector = self.net(torch.cat((state_emb, action_emb), dim=-1))  # shape: (B, n_objectives)\n    reg = get_regularization(self.net)\n    return {'q': q_vector, 'reg': reg}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:10:48.898324Z","iopub.execute_input":"2025-07-10T10:10:48.898594Z","iopub.status.idle":"2025-07-10T10:10:48.909110Z","shell.execute_reply.started":"2025-07-10T10:10:48.898553Z","shell.execute_reply":"2025-07-10T10:10:48.908521Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"class ValueCritic(nn.Module):\n    \n    def __init__(self, policy, params):\n        super().__init__()\n        self.state_dim = policy.state_dim\n        self.action_dim = policy.action_dim\n#         self.state_encoder = policy.state_encoder\n        self.net = DNN(self.state_dim, params['critic_hidden_dims'], 1, \n                       dropout_rate = params['critic_dropout_rate'], do_batch_norm = True)\n        \n    def forward(self, feed_dict):\n        '''\n        @input:\n        - feed_dict: {'state_emb': (B, state_dim), 'action_emb': (B, action_dim)}\n        '''\n        state_emb = feed_dict['state_emb']\n        V = self.net(state_emb).view(-1)\n        reg = get_regularization(self.net)\n        return {'v': V, 'reg': reg}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:10:48.909834Z","iopub.execute_input":"2025-07-10T10:10:48.910029Z","iopub.status.idle":"2025-07-10T10:10:48.921654Z","shell.execute_reply.started":"2025-07-10T10:10:48.910012Z","shell.execute_reply":"2025-07-10T10:10:48.920998Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# @title One Stage Facade\nclass OneStageFacade():\n  def __init__(self, environment, actor, critic, params):\n    super().__init__()\n    self.device = device\n    self.env = environment\n    self.actor = actor\n    self.critic = critic\n\n    self.slate_size = params['slate_size']\n    self.noise_var = params['noise_var']\n    self.noise_decay = params['noise_var'] / params['n_iter'][-1]\n    self.q_laplace_smoothness = params['q_laplace_smoothness']\n    self.topk_rate = params['topk_rate']\n    self.empty_start_rate = params['empty_start_rate']\n\n    self.n_item = self.env.action_space['item_id'][1]\n\n    # (N)\n    self.candidate_iids = np.arange(1, self.n_item + 1)\n\n    # (N, item_dim)\n    self.candidate_features = torch.FloatTensor(\n        self.env.reader.get_item_list_meta(self.candidate_iids)).to(self.device)\n    self.candidate_iids = torch.tensor(self.candidate_iids).to(self.device)\n\n    # replay buffer is initialized in initialize_train()\n    self.buffer_size = params['buffer_size']\n    self.start_timestamp = params['start_timestamp']\n\n  def initialize_train(self):\n    '''\n    Procedures before training\n    '''\n    self.buffer = {\n        \"user_profile\": torch.zeros(self.buffer_size, self.env.reader.portrait_len),\n        \"history\":torch.zeros(self.buffer_size, self.env.reader.max_seq_len).to(torch.long),\n        \"next_history\":torch.zeros(self.buffer_size, self.env.reader.max_seq_len).to(torch.long),\n        \"state_emb\": torch.zeros(self.buffer_size, self.actor.state_dim),\n        \"action_emb\":torch.zeros(self.buffer_size, self.actor.action_dim),\n        \"action\":torch.zeros(self.buffer_size, self.slate_size, dtype=torch.long),\n        \"reward\":torch.zeros(self.buffer_size),\n        \"min_reward\": torch.zeros(self.buffer_size),\n        \"feedback\": torch.zeros(self.buffer_size, self.slate_size),\n        \"done\": torch.zeros(self.buffer_size, dtype=torch.bool)\n    }\n\n    for k, v in self.buffer.items():\n      self.buffer[k] = v.to(self.device)\n    self.buffer_head = 0\n    self.current_buffer_size = 0\n    self.n_stream_record = 0\n    self.is_training_available = False\n\n  def reset_env(self, initial_params = {'batch_size': 1}):\n    '''\n    Reset user response environment\n    '''\n    initial_params['empty_history'] = True if np.random.rand() < self.empty_start_rate else False\n    initial_observation = self.env.reset(initial_params)\n    return initial_observation\n\n  def env_step(self, policy_output):\n    action_dict = {\n      'action': policy_output['action'],\n      'action_features': policy_output['action_features']\n    }\n    observation, reward, done, info = self.env.step(action_dict)\n    return observation, reward, done, info\n\n  def stop_env(self):\n    self.env.stop()\n\n  def get_episode_report(self, n_recent = 10):\n    recent_rewards = self.env.reward_history[-n_recent:]\n    recent_steps = self.env.step_history[-n_recent:]\n    epsiode_report = {\n        'average_total_reward': np.mean(recent_rewards),\n        'reward_variance': np.var(recent_rewards),\n        'max_total_reward': np.max(recent_rewards),\n        'min_total_reward': np.min(recent_rewards),\n        'average_n_step': np.mean(recent_steps),\n        'max_n_step': np.max(recent_steps),\n        'min_n_step': np.min(recent_steps),\n        'buffer_size': self.current_buffer_size\n    }\n    return epsiode_report\n\n  def apply_critic(self, observation, policy_output, critic_model):\n    feed_dict = {\n        'state_emb': policy_output['state_emb'],\n        'action_emb': policy_output['action_emb']\n    }\n    critic_output = critic_model(feed_dict)\n    return critic_output\n\n  def apply_policy(self, observation, policy_model, epsilon = 0, \n                 do_explore = False, do_softmax = True):\n    '''\n    @input:\n    - observation: input of policy model\n    - policy_model\n    - epsilon: greedy epsilon, effective only when do_explore == True\n    - do_explore: exploration flag, True if adding noise to action\n    - do_softmax: output softmax score\n    '''\n#         feed_dict = utils.wrap_batch(observation, device = self.device)\n    feed_dict = observation\n    out_dict = policy_model(feed_dict)\n    if do_explore:\n        action_emb = out_dict['action_emb']\n        # sampling noise of action embedding\n        if np.random.rand() < epsilon:\n            action_emb = torch.clamp(torch.rand_like(action_emb)*self.noise_var, -1, 1)\n        else:\n            action_emb = action_emb + torch.clamp(torch.rand_like(action_emb)*self.noise_var, -1, 1)\n#                 self.noise_var -= self.noise_decay\n        out_dict['action_emb'] = action_emb\n        \n    if 'candidate_ids' in feed_dict:\n        # (B, L, item_dim)\n        out_dict['candidate_features'] = feed_dict['candidate_features']\n        # (B, L)\n        out_dict['candidate_ids'] = feed_dict['candidate_ids']\n        batch_wise = True\n    else:\n        # (1,L,item_dim)\n        out_dict['candidate_features'] = self.candidate_features.unsqueeze(0)\n        # (L,)\n        out_dict['candidate_ids'] = self.candidate_iids\n        batch_wise = False\n        \n    # action prob (B,L)\n    action_prob = policy_model.score(out_dict['action_emb'], \n                                     out_dict['candidate_features'], \n                                     do_softmax = do_softmax)\n\n    # two types of greedy selection\n    if np.random.rand() >= self.topk_rate:\n        # greedy random: categorical sampling\n        action, indices = utils.sample_categorical_action(action_prob, out_dict['candidate_ids'], \n                                                          self.slate_size, with_replacement = False, \n                                                          batch_wise = batch_wise, return_idx = True)\n    else:\n        # indices on action_prob\n        _, indices = torch.topk(action_prob, k = self.slate_size, dim = 1)\n        # topk action\n        if batch_wise:\n            action = torch.gather(out_dict['candidate_ids'], 1, indices).detach() # (B, slate_size)\n        else:\n            action = out_dict['candidate_ids'][indices].detach() # (B, slate_size)\n    # (B,K)\n    out_dict['action'] = action \n    # (B,K,item_dim)\n    out_dict['action_features'] = self.candidate_features[action-1]\n    # (B,K)\n    out_dict['action_prob'] = torch.gather(action_prob, 1, indices) \n    # (B,L)\n    out_dict['candidate_prob'] = action_prob\n    return out_dict\n\n  def sample_buffer(self, batch_size):\n    '''\n    @output:\n    - observation\n    - policy output\n    - reward\n    - done_mask\n    - next_observation\n    '''\n    indices = np.random.randint(0, self.current_buffer_size, size = batch_size)\n    U, H, N, S, HA, A, R, F, D, MR = self.read_buffer(indices)\n    observation = {\n        'user_profile': U,\n        'history_features': H,\n        'min_reward': MR\n    }\n    policy_output = {\n        'state_emb': S,\n        'action_emb': HA,\n        'action': A\n    }\n    reward = R\n    done_mask = D\n    next_observation = {\n        'user_profile': U,\n        'history_features': N,\n        'min_reward': MR,\n        'previous_feedback': F\n    }\n    return observation, policy_output, reward, done_mask, next_observation\n\n  # def sample_raw_data(self, batch_size):\n  #   '''\n  #   Sample supervise data from raw training data\n  #   '''\n  #   batch = self.env.sample_user(batch_size)\n\n  def update_buffer(self, observation, policy_output, reward, done_mask,\n                    next_observation, info):\n    # Overwrite old entries in buffer\n    if self.buffer_head + reward.shape[0] >= self.buffer_size:\n      tail = self.buffer_size - self.buffer_head\n      indices = [self.buffer_head + i for i in range(tail)] + \\\n       [i for i in range(reward.shape[0] - tail)]\n    else:\n      indices = [self.buffer_head  + i for i in range(reward.shape[0])]\n\n    # update buffer\n    self.buffer[\"user_profile\"][indices] = observation['user_profile']\n    self.buffer[\"history\"][indices] = observation['history']\n    self.buffer[\"min_reward\"][indices] = observation['min_reward']\n    self.buffer[\"next_history\"][indices] = next_observation['history']\n    self.buffer[\"state_emb\"][indices] = policy_output['state_emb']\n    self.buffer[\"action\"][indices] = policy_output['action']\n    self.buffer[\"action_emb\"][indices] = policy_output['action_emb']\n    self.buffer[\"reward\"][indices] = reward\n    self.buffer[\"feedback\"][indices] = info['response']\n    self.buffer[\"done\"][indices] = done_mask\n\n    # update buffer pointer\n    self.buffer_head = (self.buffer_head + reward.shape[0]) % self.buffer_size\n    self.n_stream_record += reward.shape[0]\n    self.current_buffer_size = min(self.n_stream_record, self.buffer_size)\n\n    # available training when sufficient sample buffer\n    if self.n_stream_record >= self.start_timestamp:\n      self.is_training_available = True\n  def read_buffer(self, indices):\n    U = self.buffer['user_profile'][indices]\n    # (L, item_dim)\n    H = self.candidate_features[self.buffer[\"history\"][indices] - 1]\n    N = self.candidate_features[self.buffer[\"next_history\"][indices] - 1]\n    S = self.buffer[\"state_emb\"][indices]\n    HA = self.buffer[\"action_emb\"][indices]\n    A = self.buffer[\"action\"][indices]\n    R = self.buffer[\"reward\"][indices]\n    F = self.buffer[\"feedback\"][indices]\n    D = self.buffer[\"done\"][indices]\n    MR = self.buffer['min_reward'][indices]\n    return U, H, N, S, HA, A, R, F, D, MR\n\n  def extract_behavior_data(self, observation, policy_output, next_observation):\n    '''\n    Extract supervised data from RL samples\n    '''\n    observation = {\n        \"user_profile\": observation['user_profile'],\n        \"history_features\": observation['history_features']\n    }\n    exposed_items = policy_output['action']\n    exposure = {\n        \"ids\": exposed_items,\n        \"features\": self.candidate_features[exposed_items - 1]\n    }\n    user_feedback = next_observation[\"previous_feedback\"]\n    return observation, exposure, user_feedback","metadata":{"cellView":"form","id":"Jd56qLlk99ge","trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:10:48.922425Z","iopub.execute_input":"2025-07-10T10:10:48.922703Z","iopub.status.idle":"2025-07-10T10:10:48.946843Z","shell.execute_reply.started":"2025-07-10T10:10:48.922678Z","shell.execute_reply":"2025-07-10T10:10:48.946105Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# @title One Stage Facade with Hyper Action\n\nclass OneStageFacade_HyperAction(OneStageFacade):\n  def __init__(self, environment, actor, critic, params):\n    super().__init__(environment, actor, critic, params)\n\n  def apply_policy(self, observation, policy_model, epsilon = 0,\n                   do_explore = False, do_softmax = True):\n    feed_dict = wrap_batch(observation, device=device)\n    # print(feed_dict.device)\n    out_dict = policy_model(feed_dict)\n    if do_explore:\n      action_emb = out_dict['action_emb']\n      # explore and exploit + clamping\n      if np.random.rand() < epsilon:\n        action_emb = torch.clamp(torch.rand_like(action_emb) * self.noise_var, -1, 1)\n      else:\n        action_emb = action_emb + torch.clamp(torch.rand_like(action_emb) * self.noise_var, -1, 1)\n\n      out_dict['action_emb'] = action_emb\n\n    # Z latent space\n    out_dict['Z'] = out_dict['action_emb']\n\n    if 'candidate_ids' in feed_dict:\n      # (B, L, item_dim)\n      out_dict['candidate_features']  = feed_dict['candidate_features']\n      # (B, L)\n      out_dict['candidate_ids'] = feed_dict['candidate_ids']\n      batch_wise = True\n    else:\n      # (1, L, item_dim)\n      out_dict['candidate_features'] = self.candidate_features.unsqueeze(0)\n      #(L, )\n      out_dict['candidate_ids'] = self.candidate_iids\n      batch_wise = False\n\n    # action pron (B, L)\n    action_prob = policy_model.score(out_dict['action_emb'],\n                                      out_dict['candidate_features'],\n                                      do_softmax=do_softmax)\n\n    # two types of greedy selection\n    if np.random.rand() >= self.topk_rate:\n      # greedy random\n      action, indices = sample_categorical_action(action_prob, out_dict['candidate_ids'],\n                                                  self.slate_size, with_replacement=False,\n                                                  batch_wise=batch_wise,\n                                                  return_idx=True)\n    else:\n      # indices on action_prob\n      _, indices = torch.topk(action_prob, k = self.slate_size, dim = 1)\n      # print(indices.shape)\n      # print(self.candidate_features.shape)\n      # top k action:\n      # (B, slate_size)\n      if batch_wise:\n        action = torch.gather(out_dict['candidate_ids'], 1, indices).detach()\n      else:\n        action = out_dict['candidate_ids'][indices].detach()\n\n    # (B, K)\n    out_dict['action'] = action\n    # (B, K, item_dim)\n    out_dict['action_features'] = self.candidate_features[indices]\n    # (B, K)\n    out_dict['action_prob'] = torch.gather(action_prob, 1, indices)\n    # (B, L)\n    out_dict['candidate_prob'] = action_prob\n\n    return out_dict\n\n  def infer_hyper_action(self, observation, policy_output, actor):\n    '''\n    Inverse function A -> Z\n    '''\n    # (B, K)\n    A = policy_output['action']\n\n    # (B, K, item_dim)\n    item_embs = self.candidate_features[A - 1]\n\n    # (B, K, kernel_dim)\n    Z = torch.mean(actor.item_map(item_embs).view(A.shape[0], A.shape[1], -1), dim = 1)\n    return {\n        'Z': Z,\n        'action_emb': Z,\n        'state_emb': policy_output['state_emb']\n    }\n\n  def apply_critic(self, observation, policy_output, critic_model):\n    feed_dict = {\n        'state_emb': policy_output['state_emb'],\n        'action_emb': policy_output['action_emb']\n    }\n    critic_output = critic_model(feed_dict)\n    return critic_output","metadata":{"cellView":"form","id":"_H8nHNkrZdnk","trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:10:48.947766Z","iopub.execute_input":"2025-07-10T10:10:48.948013Z","iopub.status.idle":"2025-07-10T10:10:48.964392Z","shell.execute_reply.started":"2025-07-10T10:10:48.947997Z","shell.execute_reply":"2025-07-10T10:10:48.963786Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# @title Base RL Agent\n\n\n\nclass BaseRLAgent():\n  def __init__(self, facade, params):\n    self.device = params['device']\n    self.gamma = params['gamma']\n    self.n_iter = [0] + params['n_iter']\n    self.train_every_n_step = params['train_every_n_step']\n    self.check_episode = params['check_episode']\n    self.save_path = params['save_path']\n    self.facade = facade\n    self.check_episode = params['check_episode']\n    self.exploration_scheduler = LinearScheduler(int(sum(self.n_iter) * params['elbow_greedy']),\n                                                 params['final_greedy_epsilon'],\n                                                 params['initial_greedy_epsilon'])\n    # if len(self.n_iter) == 2:\n    #   with open(self.save_path + \".report\", 'w') as outfile:\n    #     outfile.write()\n\n  def train(self):\n    if len(self.n_iter) > 2:\n      self.load()\n\n    t = time()\n    start_time = t\n    print(\"Run procedure before training\")\n    self.action_before_train()\n\n    print(\"Start training\")\n    observation = self.facade.reset_env({\n        'batch_size': self.episode_batch_size,\n    })\n    step_offset = sum(self.n_iter[:-1])\n    for i in tqdm(range(step_offset, step_offset + self.n_iter[-1])):\n      observation = self.run_episode_step(i, self.exploration_scheduler.value(i),\n                                          observation, True)\n      if i % self.train_every_n_step == 0:\n        self.step_train()\n\n      if i % self.check_episode == 0:\n        t_ = time()\n        # print(f\"Episode step {i}, time diff {t_ - t}, total time dif {t - start_time})\")\n        self.log_iteration(i)\n        t = t_\n        if i % (3*self.check_episode) == 0:\n            self.save()\n\n    self.action_after_train()\n\n\n  def action_before_train(self):\n    pass\n\n  def action_after_train(self):\n    self.facade.stop_env()\n\n\n  def get_report(self):\n    episode_report = self.facade.get_episode_report(10)\n    train_report = {k: np.mean(v[-10:]) for k, v in self.training_history.items()}\n    return episode_report, train_report\n\n  def log_iteration(self, step):\n    episode_report, train_report = self.get_report()\n    run.log(episode_report | train_report)\n    log_str = f\"step: {step} @ episode report: {episode_report} @ step loss: {train_report}\\n\"\n    with open(self.save_path + \".report\", 'a') as outfile:\n        outfile.write(log_str)\n    return log_str\n\n  def test(self):\n    self.load()\n    self.facade.initialize_train()\n\n    t = time()\n    start_time = t\n\n    print(\"Start testing\")\n    observation = self.facade.reset_env({\n        'batch_size': self.episode_batch_size,\n    })\n    step_offset = sum(self.n_iter[:-1])\n    with torch.no_grad():\n        for i in tqdm(range(step_offset, step_offset + self.n_iter[-1])):\n          observation = self.run_episode_step(i, self.exploration_scheduler.value(i),\n                                              observation, True)\n          if i % self.check_episode == 0:\n            t_ = time()\n            episode_report = self.facade.get_episode_report(10)\n            log_str = f\"step: {i} @ episode report: {episode_report}\\n\"\n            run.log(episode_report)\n            with open(self.save_path + \"_eval.report\", 'a') as outfile:\n              outfile.write(log_str)\n            # print(f\"Episode step {i}, time diff {t_ - t}, total time dif {t - start_time})\")\n            # print(log_str)\n            t = t_\n    \n\n  #######################################\n  #           Abstract function         #\n  #######################################\n  def run_episode_step(self, *episode_args):\n    pass\n\n  def step_train(self):\n    pass\n\n  def save(self):\n    pass\n\n  def load(self):\n    pass","metadata":{"cellView":"form","id":"wJKb3K_326B5","trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:10:48.965229Z","iopub.execute_input":"2025-07-10T10:10:48.965473Z","iopub.status.idle":"2025-07-10T10:10:48.980958Z","shell.execute_reply.started":"2025-07-10T10:10:48.965447Z","shell.execute_reply":"2025-07-10T10:10:48.980200Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# @title Deep Deterministic Policy Gradient\n\n\nclass DDPG(BaseRLAgent):\n  def __init__(self, facade, params):\n    super().__init__(facade, params)\n    self.actor = facade.actor\n    self.actor_target = copy.deepcopy(self.actor)\n    self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr = params['actor_lr'], weight_decay = params['actor_decay'])\n\n    self.critic = facade.critic\n    self.critic_target = copy.deepcopy(self.critic)\n    self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr = params['critic_lr'], weight_decay = params['critic_decay'])\n\n    self.episode_batch_size = params['episode_batch_size']\n    self.tau = params['target_mitigate_coef']\n    self.actor_lr = params['actor_lr']\n    self.critic_lr = params['critic_lr']\n    self.actor_decay = params['actor_decay']\n    self.critic_decay = params['critic_decay']\n\n    self.batch_size = params['batch_size']\n\n    with open(self.save_path + \".report\", 'w') as outfile:\n      pass\n\n  def action_before_train(self):\n    '''\n    - facade setup\n      - buffer setup\n    - run random episodes to build-up the initial buffer\n    '''\n    self.facade.initialize_train()\n    # print(\"Facade Parameters:\")\n    # for param, value in vars(self.facade).items():\n    #     print(f\"{param}: {value}\")\n    prepare_step = 0\n    # random explore before training\n    initial_epsilon = 1.0\n    observation = self.facade.reset_env({\n        'batch_size': self.episode_batch_size,\n    })\n    while not self.facade.is_training_available:\n      observation = self.run_episode_step(0, initial_epsilon, observation, True)\n      # print(observation)\n      prepare_step += 1\n\n    # training records\n    self.training_history = {\"critic_loss\": [], \"actor_loss\": []}\n\n    print(f\"Total {prepare_step} prepare steps\")\n\n  def run_episode_step(self, *episode_args):\n    '''\n    One step of interaction\n    '''\n    episode_iter, epsilon, observation, do_buffer_update = episode_args\n    with torch.no_grad():\n      # sample action\n      policy_output = self.facade.apply_policy(observation, self.actor, epsilon,\n                                               do_explore=True)\n\n      # apply action on environment and update replay buffer\n      next_observation, reward, done, info = self.facade.env_step(policy_output)\n\n      # update replay buffer\n      if do_buffer_update:\n        self.facade.update_buffer(observation, policy_output, reward, done,\n                                  next_observation, info)\n    return next_observation\n\n  def step_train(self):\n    observation , policy_output, reward, done_mask, next_observation = self.facade.sample_buffer(params['batch_size'])\n\n    critic_loss, actor_loss = self.get_ddpg_loss(observation, policy_output, reward,\n                                                  done_mask, next_observation)\n    self.training_history[\"critic_loss\"].append(critic_loss.item())\n    self.training_history[\"actor_loss\"].append(actor_loss.item())\n\n    # Update the frozen target models\n    for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n      target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)\n\n    for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n      target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)\n\n    return {'step_loss': (self.training_history['actor_loss'][-1],\n                          self.training_history['critic_loss'][-1])}\n\n  def get_ddpg_loss(self, observation, policy_output, reward, done_mask, next_observation,\n                    do_actor_update = True, do_critic_update = True):\n    # Get current Q estimate\n    current_critic_output = self.facade.apply_critic(observation,\n                                                     wrap_batch(policy_output, device=self.device),\n                                                     self.critic)\n    current_Q = current_critic_output['q']\n\n    # Compute the target Q value\n    next_policy_output = self.facade.apply_policy(next_observation, self.actor_target)\n    target_critic_output = self.facade.apply_critic(next_observation, next_policy_output,\n                                                    self.critic_target)\n\n    target_Q = target_critic_output['q']\n    target_Q = reward + self.gamma * (done_mask * target_Q).detach()\n\n    # compute critic loss\n    # minimize current_Q predict and target_Q predict\n    critic_loss = F.mse_loss(current_Q, target_Q).mean()\n\n    if do_critic_update and self.critic_lr > 0:\n      # Optimize the critic\n      self.critic_optimizer.zero_grad()\n      critic_loss.backward()\n      self.critic_optimizer.step()\n\n    # compute actor loss\n    policy_output = self.facade.apply_policy(observation, self.actor)\n    critic_output = self.facade.apply_critic(observation, policy_output, self.critic)\n\n    # Maximize Q value\n    actor_loss = -critic_output['q'].mean()\n\n    if do_actor_update and self.actor_lr > 0:\n      # Optimize the actor\n      self.actor_optimizer.zero_grad()\n      actor_loss.backward()\n      self.actor_optimizer.step()\n    return critic_loss, actor_loss\n\n  def save(self):\n    torch.save(self.critic.state_dict(), self.save_path + \"_critic\")\n    torch.save(self.critic_optimizer.state_dict(), self.save_path + \"_critic_optimizer\")\n    torch.save(self.actor.state_dict(), self.save_path + \"_actor\")\n    torch.save(self.actor_optimizer.state_dict(), self.save_path + \"_actor_optimizer\")\n\n  def load(self):\n    self.critic.load_state_dict(torch.load(self.save_path + \"_critic\", map_location=self.device))\n    self.critic_optimizer.load_state_dict(torch.load(self.save_path + \"_critic_optimizer\", map_location=self.device))\n    self.critic_target = copy.deepcopy(self.critic)\n\n    self.actor.load_state_dict(torch.load(self.save_path + \"_actor\", map_location=self.device))\n    self.actor_optimizer.load_state_dict(torch.load(self.save_path + \"_actor_optimizer\", map_location=self.device))\n    self.actor_target = copy.deepcopy(self.actor)","metadata":{"cellView":"form","id":"VBbJXf-KfvDA","trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:10:48.981752Z","iopub.execute_input":"2025-07-10T10:10:48.981970Z","iopub.status.idle":"2025-07-10T10:10:48.999355Z","shell.execute_reply.started":"2025-07-10T10:10:48.981954Z","shell.execute_reply":"2025-07-10T10:10:48.998698Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"\n\n# @title Hyper - Actor Critic\nclass HAC(DDPG):\n  def __init__(self, facade, params):\n    super().__init__(facade, params)\n    self.behavior_lr = params['behavior_lr']\n    self.behavior_decay = params['behavior_decay']\n    self.hyper_actor_coef = params['hyper_actor_coef']\n    self.actor_behavior_optimizer = torch.optim.Adam(self.actor.parameters(),\n                                                     lr=params['behavior_lr'],\n                                                     weight_decay=params['behavior_decay'])\n\n  def action_before_train(self):\n    super().action_before_train()\n    self.training_history['hyper_actor_loss'] = []\n    self.training_history['behavior_loss'] = []\n\n  def run_episode_step(self, *episode_args):\n    '''\n    One step of interaction\n    '''\n    episode_iter, epsilon, observation, do_buffer_update = episode_args\n    with torch.no_grad():\n      # sample action\n      policy_output = self.facade.apply_policy(observation, self.actor, epsilon,\n                                               do_explore=True)\n\n      # apply action on environment and update replay buffer\n      next_observation, reward, done, info = self.facade.env_step(policy_output)\n\n      # update replay buffer\n      if do_buffer_update:\n        self.facade.update_buffer(observation, policy_output, reward, done,\n                                  next_observation, info)\n    return next_observation\n\n  def step_train(self):\n    observation , policy_output, reward, done_mask, next_observation = self.facade.sample_buffer(params['batch_size'])\n    # reward  = torch.FloatTensor(reward)\n    # done_mask = torch.FloatTensor(done_mask)\n\n    critic_loss, actor_loss, hyper_actor_loss = self.get_hac_loss(observation, policy_output, reward,\n                                                  done_mask, next_observation)\n    behavior_loss = self.get_behavior_loss(observation, policy_output, next_observation)\n\n    self.training_history[\"critic_loss\"].append(critic_loss.item())\n    self.training_history[\"actor_loss\"].append(actor_loss.item())\n    self.training_history['hyper_actor_loss'].append(hyper_actor_loss.item())\n    self.training_history['behavior_loss'].append(behavior_loss.item())\n\n    # Update frozen target models\n    for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n      target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)\n\n    for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n      target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)\n\n    return {\"step_loss\": (self.training_history['actor_loss'][-1],\n                          self.training_history['critic_loss'][-1],\n                          self.training_history['hyper_actor_loss'][-1],\n                          self.training_history['behavior_loss'][-1])}\n\n  def get_hac_loss(self, observation, policy_output, reward, done_mask, next_observation,\n                    do_actor_update = True, do_critic_update = True):\n    # nsw reward\n    cummulative_r = reward\n    min_r = observation['min_reward']\n    min_r = torch.where(torch.isinf(min_r), torch.zeros_like(min_r), min_r)\n    \n    # Current Q estimate\n    hyper_output = self.facade.infer_hyper_action(observation, policy_output, self.actor)\n    current_critic_output = self.facade.apply_critic(observation, hyper_output, self.critic)\n    current_Q = current_critic_output['q']\n\n    # Compute target Q value\n    next_policy_output = self.facade.apply_policy(next_observation, self.actor_target)\n    target_critic_output = self.facade.apply_critic(next_observation, next_policy_output, self.critic_target)\n\n    target_Q = target_critic_output['q']\n    target_Q = cummulative_r + self.gamma * (done_mask * target_Q).detach()\n\n\n    critic_loss = F.mse_loss(current_Q, target_Q).mean()\n    if do_critic_update and self.critic_lr > 0:\n      self.critic_optimizer.zero_grad()\n      critic_loss.backward()\n      self.critic_optimizer.step()\n\n    # actor loss\n\n    if do_actor_update and self.actor_lr > 0:\n      self.actor_optimizer.zero_grad()\n      policy_output = self.facade.apply_policy(observation, self.actor)\n      critic_output = self.facade.apply_critic(observation, policy_output, self.critic)\n      actor_loss = -nsw(critic_output['q'], min_r).mean()\n      actor_loss.backward()\n      self.actor_optimizer.step()\n\n    # hyper actor loss\n\n    if do_actor_update and self.hyper_actor_coef > 0:\n      self.actor_optimizer.zero_grad()\n      policy_output = self.facade.apply_policy(observation, self.actor)\n      inferred_hyper_output = self.facade.infer_hyper_action(observation, policy_output, self.actor)\n      hyper_actor_loss = self.hyper_actor_coef * F.mse_loss(inferred_hyper_output['Z'],\n                                                            policy_output['Z']).mean()\n\n      hyper_actor_loss.backward()\n      self.actor_optimizer.step()\n\n    return critic_loss, actor_loss, hyper_actor_loss\n\n  def get_behavior_loss(self, observation, policy_output, next_observation, do_update = True):\n    observation, exposure, feedback = self.facade.extract_behavior_data(observation, policy_output, next_observation)\n    observation['candidate_ids'] = exposure['ids']\n    observation['candidate_features'] = exposure['features']\n    policy_output = self.facade.apply_policy(observation, self.actor, do_softmax=False)\n    action_prob = torch.sigmoid(policy_output['candidate_prob'])\n    behavior_loss = F.binary_cross_entropy(action_prob, feedback)\n\n    if do_update and self.behavior_lr > 0:\n      self.actor_behavior_optimizer.zero_grad()\n      behavior_loss.backward()\n      self.actor_behavior_optimizer.step()\n\n    return behavior_loss\n","metadata":{"cellView":"form","id":"HLBYcDpMSfuw","trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:21:45.499449Z","iopub.execute_input":"2025-07-10T10:21:45.500145Z","iopub.status.idle":"2025-07-10T10:21:45.517982Z","shell.execute_reply.started":"2025-07-10T10:21:45.500122Z","shell.execute_reply":"2025-07-10T10:21:45.517181Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"class A2C(BaseRLAgent):\n    \n    def __init__(self, facade, params):\n        '''\n        self.gamma\n        self.n_iter\n        self.check_episode\n        self.with_eval\n        self.save_path\n        self.facade\n        self.exploration_scheduler\n        '''\n        super().__init__(facade, params)\n        self.episode_batch_size = params['episode_batch_size']\n        self.batch_size = params['batch_size']\n        \n        self.actor_lr = params['actor_lr']\n        self.critic_lr = params['critic_lr']\n        self.actor_decay = params['actor_decay']\n        self.critic_decay = params['critic_decay']\n        \n        self.actor = facade.actor\n        self.actor_target = copy.deepcopy(self.actor)\n        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=params['actor_lr'], \n                                                weight_decay=params['actor_decay'])\n\n        self.critic = facade.critic\n        self.critic_target = copy.deepcopy(self.critic)\n        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=params['critic_lr'], \n                                                 weight_decay=params['critic_decay'])\n\n        self.tau = params['target_mitigate_coef']\n        self.advantage_bias = params['advantage_bias']\n        self.entropy_coef = params['entropy_coef']\n        if len(self.n_iter) == 1:\n            with open(self.save_path + \".report\", 'w') as outfile:\n                outfile.write(f\"{args}\\n\")\n        \n        \n#     def action_after_train(self):\n#         self.facade.stop_env()\n        \n#     def get_report(self):\n#         episode_report = self.facade.get_episode_report(10)\n#         train_report = {k: np.mean(v[-10:]) for k,v in self.training_history.items()}\n#         return episode_report, train_report\n        \n    def action_before_train(self):\n        super().action_before_train()\n        self.facade.initialize_train()\n        self.training_history = {\"critic_loss\": [], \"actor_loss\": []}\n        self.training_history['entropy_loss'] = []\n        self.training_history['advantage'] = []\n        \n    def run_episode_step(self, *episode_args):\n        '''\n        One step of interaction\n        '''\n        episode_iter, epsilon, observation, do_buffer_update = episode_args\n        with torch.no_grad():\n            # sample action\n            policy_output = self.facade.apply_policy(observation, self.actor, epsilon, \n                                                     do_explore = True, do_softmax = True)\n            # apply action on environment and update replay buffer\n            next_observation, reward, done, info = self.facade.env_step(policy_output)\n            # update replay buffer\n            if do_buffer_update:\n                self.facade.update_buffer(observation, policy_output, reward, done, next_observation, info)\n        return next_observation\n            \n\n    def step_train(self):\n        observation, policy_output, reward, done_mask, next_observation = self.facade.sample_buffer(self.batch_size)\n#         reward = torch.FloatTensor(reward)\n#         done_mask = torch.FloatTensor(done_mask)\n        \n        critic_loss, actor_loss, entropy_loss, advantage = self.get_a2c_loss(observation, policy_output, reward, done_mask, next_observation)\n        self.training_history['actor_loss'].append(actor_loss.item())\n        self.training_history['critic_loss'].append(critic_loss.item())\n        self.training_history['entropy_loss'].append(entropy_loss.item())\n        self.training_history['advantage'].append(advantage.item())\n\n        # Update the frozen target models\n        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n\n        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n\n        return {\"step_loss\": (self.training_history['actor_loss'][-1], \n                              self.training_history['critic_loss'][-1], \n                              self.training_history['entropy_loss'][-1], \n                              self.training_history['advantage'][-1])}\n    \n    def get_a2c_loss(self, observation, policy_output, reward, done_mask, next_observation, \n                      do_actor_update = True, do_critic_update = True):\n        \n        # Get current Q estimate\n        current_policy_output = self.facade.apply_policy(observation, self.actor)\n        # print(current_policy_output)\n        S = current_policy_output['state_emb']\n        V_S = self.critic({'state_emb': S})['v']\n        \n        # Compute the target Q value\n        next_policy_output = self.facade.apply_policy(next_observation, self.actor_target)\n#         next_policy_output = self.facade.apply_p[olicy(next_observation, self.actor)\n        S_prime = next_policy_output['state_emb']\n        V_S_prime = self.critic_target({'state_emb': S_prime})['v'].detach()\n#         V_S_prime = self.critic({'state_emb': S_prime})['v'].detach()\n        Q_S = reward + self.gamma * (done_mask * V_S_prime)\n        advantage = torch.clamp((Q_S - V_S).detach(), -1, 1) # (B,)\n\n        # Compute critic loss\n        value_loss = F.mse_loss(V_S, Q_S).mean()\n        \n        # Regularization loss\n#         critic_reg = current_critic_output['reg']\n\n        if do_critic_update and self.critic_lr > 0:\n            # Optimize the critic\n            self.critic_optimizer.zero_grad()\n            value_loss.backward()\n            self.critic_optimizer.step()\n\n        # Compute actor loss\n        current_policy_output = self.facade.apply_policy(observation, self.actor)\n        A = policy_output['action']\n#         logp = -torch.log(current_policy_output['action_prob'] + 1e-6) # (B,K)\n        logp = -torch.log(torch.gather(current_policy_output['candidate_prob'],1,A-1) + 1e-6) # (B,K)\n        # use log(1-p), p is close to zero when there are large number of items\n#         logp = torch.log(-torch.gather(current_policy_output['candidate_prob'],1,A-1)+1) # (B,K)\n        actor_loss = torch.mean(torch.sum(logp * (advantage.view(-1,1) + self.advantage_bias), dim = 1))\n        entropy_loss = torch.sum(current_policy_output['candidate_prob'] \\\n                                  * torch.log(current_policy_output['candidate_prob']), dim = 1).mean()\n        \n        # Regularization loss\n#         actor_reg = policy_output['reg']\n\n        if do_actor_update and self.actor_lr > 0:\n            # Optimize the actor \n            self.actor_optimizer.zero_grad()\n            (actor_loss + self.entropy_coef * entropy_loss).backward()\n            self.actor_optimizer.step()\n            \n        return value_loss, actor_loss, entropy_loss, torch.mean(advantage)\n\n\n    def save(self):\n        torch.save(self.critic.state_dict(), self.save_path + \"_critic\")\n        torch.save(self.critic_optimizer.state_dict(), self.save_path + \"_critic_optimizer\")\n\n        torch.save(self.actor.state_dict(), self.save_path + \"_actor\")\n        torch.save(self.actor_optimizer.state_dict(), self.save_path + \"_actor_optimizer\")\n\n\n    def load(self):\n        self.critic.load_state_dict(torch.load(self.save_path + \"_critic\", map_location=self.device))\n        self.critic_optimizer.load_state_dict(torch.load(self.save_path + \"_critic_optimizer\", map_location=self.device))\n        self.critic_target = copy.deepcopy(self.critic)\n\n        self.actor.load_state_dict(torch.load(self.save_path + \"_actor\", map_location=self.device))\n        self.actor_optimizer.load_state_dict(torch.load(self.save_path + \"_actor_optimizer\", map_location=self.device))\n        self.actor_target = copy.deepcopy(self.actor)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:21:45.631417Z","iopub.execute_input":"2025-07-10T10:21:45.631717Z","iopub.status.idle":"2025-07-10T10:21:45.649551Z","shell.execute_reply.started":"2025-07-10T10:21:45.631696Z","shell.execute_reply":"2025-07-10T10:21:45.648761Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"# @ ML1M setting\n\nparams['n_worker'] = 4\nparams['max_seq_len'] = 50\n\nparams['loss_type'] = 'bce'\nparams['device'] = device\nparams['l2_coef'] = 0.001\nparams['lr'] = 0.0003\nparams['feature_dim'] = 16\nparams['hidden_dims'] = [256]\nparams['attn_n_head'] = 2\nparams['batch_size'] = 128\nparams['epoch'] = 2\nparams['dropout_rate'] = 0.2\nparams['max_step'] = 20\nparams['initial_temper'] = 20\nparams['reward_function'] = mean_with_cost\nparams['sasrec_n_layer'] = 2\nparams['sasrec_d_model'] = 32\nparams['sasrec_n_head'] = 4\nparams['sasrec_dropout'] = 0.1\nparams['sasrec_d_forward'] = 64\nparams['critic_hidden_dims'] = [256, 64]\nparams['critic_dropout_rate'] = 0.2\nparams['n_iter']= [50000]\nparams['slate_size'] = 10\nparams['noise_var'] = 0.1\nparams['q_laplace_smoothness'] = 0.5\nparams['topk_rate'] = 1\nparams['empty_start_rate'] = 0\nparams['buffer_size'] = 100000\nparams['start_timestamp'] = 2000\nparams['gamma'] = 0.9\nparams['train_every_n_step']= 1\nparams['initial_greedy_epsilon'] = 0\nparams['final_greedy_epsilon'] = 0\nparams['elbow_greedy'] = 0.1\nparams['check_episode'] = 10\nparams['with_eval'] = False\n\nparams['episode_batch_size'] = 32\nparams['batch_size'] = 64\nparams['actor_lr'] = 0.00001\nparams['critic_lr'] = 0.001\nparams['actor_decay'] = 0.00001\nparams['critic_decay'] = 0.00001\nparams['target_mitigate_coef'] = 0.01\nparams['behavior_lr'] = 0.0003\nparams['behavior_decay'] = 0.00001\nparams['hyper_actor_coef'] = 0.1\nparams['advantage_bias'] = 0\nparams['entropy_coef'] = 0.0001\n\nconfig = params.copy()\nconfig.pop(\"train\", None)\nconfig.pop(\"val\", None)\nconfig.pop(\"item_meta\", None)\nconfig.pop(\"user_meta\", None)\n\n\nfor seed in [11]:\n    params['seed'] = seed\n    set_random_seed(params['seed'])\n    params['save_path'] = os.path.join(path_to_output, f\"agent/ml1m_model_seed{params['seed']}\")\n    run = wandb.init(\n    # Set the wandb entity where your project will be logged (generally your team name).\n    entity=\"23020082-uet\",\n    # Set the wandb project where this run will be logged.\n    project=\"HAC\",\n    # Track hyperparameters and run metadata.\n    config=config\n    )\n    os.makedirs(os.path.dirname(params['save_path']), exist_ok=True)\n    \n    env = ML1MEnvironment(params)\n    \n    policy = SASRec(env, params)\n    policy.to(device)\n    \n    \n    critic = GeneralCritic(policy, params)\n    critic.to(device)\n    \n    facade = OneStageFacade_HyperAction(env, policy, critic, params)\n    agent = HAC(facade, params)\n    \n    agent.train()\n    run.finish()","metadata":{"id":"q5dd23sM-gSN","outputId":"843e44a9-4951-4d2e-941e-074c5d06af00","trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:21:45.651168Z","iopub.execute_input":"2025-07-10T10:21:45.651752Z","iopub.status.idle":"2025-07-10T11:02:02.334888Z","shell.execute_reply.started":"2025-07-10T10:21:45.651732Z","shell.execute_reply":"2025-07-10T11:02:02.333343Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250710_102145-fz6klbtb</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/23020082-uet/HAC/runs/fz6klbtb' target=\"_blank\">sage-dream-76</a></strong> to <a href='https://wandb.ai/23020082-uet/HAC' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/23020082-uet/HAC' target=\"_blank\">https://wandb.ai/23020082-uet/HAC</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/23020082-uet/HAC/runs/fz6klbtb' target=\"_blank\">https://wandb.ai/23020082-uet/HAC/runs/fz6klbtb</a>"},"metadata":{}},{"name":"stdout","text":"Load item meta data\n{'length': 77906, 'n_item': 3952, 'item_vec_size': 18, 'user_portrait_len': 30, 'max_seq_len': 50}\nRun procedure before training\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/2270135661.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(params['model_path'] + \".checkpoint\", map_location=device)\n","output_type":"stream"},{"name":"stdout","text":"Total 63 prepare steps\nStart training\n","output_type":"stream"},{"name":"stderr","text":" 87%|████████▋ | 43326/50000 [40:06<06:10, 18.00it/s] \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/1058994429.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHAC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfacade\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/1134968338.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mstep_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_offset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m       observation = self.run_episode_step(i, self.exploration_scheduler.value(i),\n\u001b[0m\u001b[1;32m     38\u001b[0m                                           observation, True)\n\u001b[1;32m     39\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_every_n_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/69291990.py\u001b[0m in \u001b[0;36mrun_episode_step\u001b[0;34m(self, *episode_args)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m       \u001b[0;31m# apply action on environment and update replay buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m       \u001b[0mnext_observation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfacade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m       \u001b[0;31m# update replay buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/2569368723.py\u001b[0m in \u001b[0;36menv_step\u001b[0;34m(self, policy_output)\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0;34m'action_features'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpolicy_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action_features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     }\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/2270135661.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, step_dict)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mnewH\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mH_prime\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mnewH_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mH_prime_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0moffset\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_observation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'history'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_observation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'history_features'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewH_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":39},{"cell_type":"code","source":"run.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T11:09:27.447030Z","iopub.execute_input":"2025-07-10T11:09:27.447395Z","iopub.status.idle":"2025-07-10T11:09:30.796173Z","shell.execute_reply.started":"2025-07-10T11:09:27.447367Z","shell.execute_reply":"2025-07-10T11:09:30.795625Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>actor_loss</td><td>█▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>average_n_step</td><td>▄▅▄▇▃▄▄▇▄▅▁█▂▆▄▇▅▅▅▅▅▆▄▆▅▃▇▃▅▆▆▆▆▄▅▄▆▇▇▇</td></tr><tr><td>average_total_reward</td><td>▁▁▆▆█▄▃▇▃▅▄▄▃▂▃▃▁▃▅▃▅▅▂▃▂▃▄▃▅▄▅▄▆▂▅▅▅▅▂▂</td></tr><tr><td>behavior_loss</td><td>█▄▃▂▁▂▃▂▂▃▂▂▂▃▂▂▂▂▂▂▂▃▂▁▃▂▂▂▁▂▁▂▂▂▂▂▂▂▂▂</td></tr><tr><td>buffer_size</td><td>▁▁██████████████████████████████████████</td></tr><tr><td>critic_loss</td><td>██▇█▅▅▄▄▅▆▇▄▅▄▅▅▆▄▄▁▆█▆▇▆▇▅▅▅▆▆▄▇▆▆▇▇▄▇▂</td></tr><tr><td>hyper_actor_loss</td><td>█▇▆▅▅▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>max_n_step</td><td>▃▅█▅▁▃▃▅▁▆▃▅▅▅▃▅▃▃█▁█▃▅▅▅▅▅▃▅▃▅█▁▃▃▅▃▃▅▃</td></tr><tr><td>max_total_reward</td><td>▁▇▇▇▇▇█▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▆▆▆██▇▇▇</td></tr><tr><td>min_n_step</td><td>▁▇▇▆█▆▇▆▇▇▆▆▇▆▇▇█▇▆▇▇▇█▇▇▇█▇▇▇▇▇█▆▇▇▇▇▇▇</td></tr><tr><td>min_total_reward</td><td>▅▆▃▆▅▅▅▅▆▃▆▄▅▆▁▆▆▅▅▆▄▃▅▅▆▃▃▅▆▅▆▅▆▃▆▃▁█▃▄</td></tr><tr><td>reward_variance</td><td>▂▃▂▂▁▂▂▂▂▁▁▄▂▃▂▂▂▂▃▂▅▁▂▁▃█▃▃▃▂▃▃▃▂▃▂▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>actor_loss</td><td>0.73999</td></tr><tr><td>average_n_step</td><td>15.2</td></tr><tr><td>average_total_reward</td><td>14.749</td></tr><tr><td>behavior_loss</td><td>0.12891</td></tr><tr><td>buffer_size</td><td>100000</td></tr><tr><td>critic_loss</td><td>0.05749</td></tr><tr><td>hyper_actor_loss</td><td>0.00031</td></tr><tr><td>max_n_step</td><td>18</td></tr><tr><td>max_total_reward</td><td>17.89</td></tr><tr><td>min_n_step</td><td>14</td></tr><tr><td>min_total_reward</td><td>13.23</td></tr><tr><td>reward_variance</td><td>2.50779</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">sage-dream-76</strong> at: <a href='https://wandb.ai/23020082-uet/HAC/runs/fz6klbtb' target=\"_blank\">https://wandb.ai/23020082-uet/HAC/runs/fz6klbtb</a><br> View project at: <a href='https://wandb.ai/23020082-uet/HAC' target=\"_blank\">https://wandb.ai/23020082-uet/HAC</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250710_102145-fz6klbtb/logs</code>"},"metadata":{}}],"execution_count":40},{"cell_type":"markdown","source":"## Test","metadata":{}},{"cell_type":"code","source":"# @title Load data\n\n# @title Load data for ml1m\nitem_info = np.load(os.path.join(path_to_data, \"item_info.npy\"))\nuser_info = np.load(os.path.join(path_to_data, \"user_info.npy\"))\ntrain = pd.read_csv(os.path.join(path_to_data, \"all.csv\"), sep=\"@\")\ntest = pd.read_csv(os.path.join(path_to_data, \"test.csv\"), sep=\"@\")\ndisplay(train.head())\ndisplay(test.head())\ndisplay(item_info.shape)\ndisplay(user_info.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T11:10:24.844661Z","iopub.execute_input":"2025-07-10T11:10:24.844995Z","iopub.status.idle":"2025-07-10T11:10:26.653223Z","shell.execute_reply.started":"2025-07-10T11:10:24.844963Z","shell.execute_reply":"2025-07-10T11:10:26.652620Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"   user_id                                     slate_of_items  \\\n0        1  [3186, 1270, 1721, 1022, 2340, 1836, 3408, 280...   \n1        1  [720, 260, 919, 608, 2692, 1961, 2028, 3105, 9...   \n2        1  [1962, 2018, 150, 1028, 1097, 914, 1287, 2797,...   \n3        1  [661, 2918, 531, 3114, 2791, 2321, 1029, 1197,...   \n4        1  [1545, 527, 595, 2687, 745, 588, 1, 2355, 2294...   \n\n                         user_mid  \\\n0  [1, 1, 1, 1, 0, 1, 1, 1, 1, 1]   \n1  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n2  [1, 1, 1, 1, 1, 0, 1, 1, 1, 1]   \n3  [0, 1, 1, 1, 1, 0, 1, 0, 1, 1]   \n4  [1, 1, 1, 0, 0, 1, 1, 1, 1, 1]   \n\n                                    user_mid_history  sequence_id  \n0                                                 []            0  \n1  [3186, 1270, 1721, 1022, 1836, 3408, 2804, 120...            1  \n2  [3186, 1270, 1721, 1022, 1836, 3408, 2804, 120...            2  \n3  [3186, 1270, 1721, 1022, 1836, 3408, 2804, 120...            3  \n4  [3186, 1270, 1721, 1022, 1836, 3408, 2804, 120...            4  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>slate_of_items</th>\n      <th>user_mid</th>\n      <th>user_mid_history</th>\n      <th>sequence_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>[3186, 1270, 1721, 1022, 2340, 1836, 3408, 280...</td>\n      <td>[1, 1, 1, 1, 0, 1, 1, 1, 1, 1]</td>\n      <td>[]</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>[720, 260, 919, 608, 2692, 1961, 2028, 3105, 9...</td>\n      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n      <td>[3186, 1270, 1721, 1022, 1836, 3408, 2804, 120...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>[1962, 2018, 150, 1028, 1097, 914, 1287, 2797,...</td>\n      <td>[1, 1, 1, 1, 1, 0, 1, 1, 1, 1]</td>\n      <td>[3186, 1270, 1721, 1022, 1836, 3408, 2804, 120...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>[661, 2918, 531, 3114, 2791, 2321, 1029, 1197,...</td>\n      <td>[0, 1, 1, 1, 1, 0, 1, 0, 1, 1]</td>\n      <td>[3186, 1270, 1721, 1022, 1836, 3408, 2804, 120...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>[1545, 527, 595, 2687, 745, 588, 1, 2355, 2294...</td>\n      <td>[1, 1, 1, 0, 0, 1, 1, 1, 1, 1]</td>\n      <td>[3186, 1270, 1721, 1022, 1836, 3408, 2804, 120...</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"   user_id                                     slate_of_items  \\\n0     4794  [2087, 1073, 951, 1230, 1256, 3362, 1276, 1304...   \n1     4794  [1923, 3763, 3702, 3693, 2804, 1196, 541, 1197...   \n2     4794  [1299, 1394, 3342, 1231, 3683, 1199, 1270, 316...   \n3     4794  [1259, 3072, 3505, 1291, 1674, 2348, 3753, 969...   \n4     4794  [1200, 1201, 1240, 457, 589, 377, 2951, 2985, ...   \n\n                         user_mid  \\\n0  [1, 1, 1, 1, 1, 1, 1, 1, 0, 1]   \n1  [1, 1, 1, 0, 1, 1, 1, 1, 1, 1]   \n2  [1, 1, 1, 1, 1, 0, 1, 0, 1, 1]   \n3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n4  [1, 1, 1, 1, 0, 1, 1, 0, 1, 1]   \n\n                                    user_mid_history  sequence_id  \n0  [593, 1198, 2683, 2355, 1911, 3543, 3615, 2757...            4  \n1  [593, 1198, 2683, 2355, 1911, 3543, 3615, 2757...            5  \n2  [593, 1198, 2683, 2355, 1911, 3543, 3615, 2757...            6  \n3  [593, 1198, 2683, 2355, 1911, 3543, 3615, 2757...            7  \n4  [593, 1198, 2683, 2355, 1911, 3543, 3615, 2757...            8  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>slate_of_items</th>\n      <th>user_mid</th>\n      <th>user_mid_history</th>\n      <th>sequence_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4794</td>\n      <td>[2087, 1073, 951, 1230, 1256, 3362, 1276, 1304...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 0, 1]</td>\n      <td>[593, 1198, 2683, 2355, 1911, 3543, 3615, 2757...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4794</td>\n      <td>[1923, 3763, 3702, 3693, 2804, 1196, 541, 1197...</td>\n      <td>[1, 1, 1, 0, 1, 1, 1, 1, 1, 1]</td>\n      <td>[593, 1198, 2683, 2355, 1911, 3543, 3615, 2757...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4794</td>\n      <td>[1299, 1394, 3342, 1231, 3683, 1199, 1270, 316...</td>\n      <td>[1, 1, 1, 1, 1, 0, 1, 0, 1, 1]</td>\n      <td>[593, 1198, 2683, 2355, 1911, 3543, 3615, 2757...</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4794</td>\n      <td>[1259, 3072, 3505, 1291, 1674, 2348, 3753, 969...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n      <td>[593, 1198, 2683, 2355, 1911, 3543, 3615, 2757...</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4794</td>\n      <td>[1200, 1201, 1240, 457, 589, 377, 2951, 2985, ...</td>\n      <td>[1, 1, 1, 1, 0, 1, 1, 0, 1, 1]</td>\n      <td>[593, 1198, 2683, 2355, 1911, 3543, 3615, 2757...</td>\n      <td>8</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(3953, 18)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(6041, 30)"},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"params['train'] = train\nparams['val'] = test\nparams['item_meta'] = item_info\nparams['user_meta'] = user_info\nparams['seed'] = 26\nparams['model_path'] = os.path.join(path_to_output, \n                          f\"env/ml1m_user_env_lr{params['lr']}_reg{params['l2_coef']}_eval.model\")\nset_random_seed(params['seed'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T11:10:26.654401Z","iopub.execute_input":"2025-07-10T11:10:26.654686Z","iopub.status.idle":"2025-07-10T11:10:26.660439Z","shell.execute_reply.started":"2025-07-10T11:10:26.654668Z","shell.execute_reply":"2025-07-10T11:10:26.659782Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"# @title Train user response\nreader = ML1MDataReader(params)\nmodel = ML1MUserResponse(reader, params).to(device)\n\n\n# reader = RL4RSDataReader(params)\n# model = RL4RSUserResponse(reader, params).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\nmodel.optimizer = optimizer\n\n\nepo = 0\nwhile epo < params['epoch']:\n  print(f\"epoch {epo} is training\")\n  epo += 1\n\n  model.train()\n  reader.set_phase(\"train\")\n  train_loader = DataLoader(reader, params['batch_size'], shuffle = True, pin_memory = True,\n                            num_workers= params['n_worker'])\n\n  t1 = time()\n  pbar = tqdm(total=len(train_loader.dataset))\n  step_loss = []\n  for i, batch_data in enumerate(train_loader):\n    optimizer.zero_grad()\n    wrapped_batch = wrap_batch(batch_data, device)\n\n    out_dict = model.do_forward_and_loss(wrapped_batch)\n    loss = out_dict['loss']\n    loss.backward()\n    step_loss.append(loss.item())\n    optimizer.step()\n    pbar.update(params['batch_size'])\n    # print(model.loss)\n    # if (i + 1) % 10 == 0:\n      # print(f\"Iteration {i + 1}, loss {np.mean(step_loss[-100:])}\")\n  pbar.close()\n    # print(\"Epoch {}; time {:.4f}\".format(epo, time() - t1))\n\n  # validation\n  t2 = time()\n  reader.set_phase(\"val\")\n  val_loader = DataLoader(reader, params['batch_size'], shuffle = False, pin_memory = False,\n                          num_workers= params['n_worker'])\n  valid_probs, valid_true =  [], []\n  pbar = tqdm(total = len(val_loader.dataset))\n  with torch.no_grad():\n    for i, batch_data in enumerate(val_loader):\n      wrapped_batch = wrap_batch(batch_data, device)\n      out_dict = model.forward(wrapped_batch)\n      valid_probs.append(out_dict['probs'].cpu().numpy())\n      valid_true.append(batch_data['feedback'].cpu().numpy())\n      pbar.update(params['batch_size'])\n  pbar.close()\n  auc = roc_auc_score(np.concatenate(valid_true), np.concatenate(valid_probs))\n  print(f\"epoch {epo} validating\" + \"; auc: {:.4f}\".format(np.mean(auc)))\n  model.save_checkpoint()\n  \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T11:10:26.661243Z","iopub.execute_input":"2025-07-10T11:10:26.661971Z","iopub.status.idle":"2025-07-10T11:11:22.835360Z","shell.execute_reply.started":"2025-07-10T11:10:26.661938Z","shell.execute_reply":"2025-07-10T11:11:22.834636Z"}},"outputs":[{"name":"stdout","text":"Load item meta data\n{'length': 97383, 'n_item': 3952, 'item_vec_size': 18, 'user_portrait_len': 30, 'max_seq_len': 50}\nepoch 0 is training\n","output_type":"stream"},{"name":"stderr","text":"97408it [00:23, 4090.78it/s]                           \n19520it [00:04, 4366.74it/s]                           \n","output_type":"stream"},{"name":"stdout","text":"epoch 1 validating; auc: 0.6019\nepoch 1 is training\n","output_type":"stream"},{"name":"stderr","text":"97408it [00:23, 4115.04it/s]                           \n19520it [00:04, 4777.30it/s]                           \n","output_type":"stream"},{"name":"stdout","text":"epoch 2 validating; auc: 0.6076\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"# @title Load data\n\n# @title Load data for ml1m\ntrain = pd.read_csv(os.path.join(path_to_data, \"test.csv\"), sep=\"@\")\ntest = pd.read_csv(os.path.join(path_to_data, \"test.csv\"), sep=\"@\")\ndisplay(train.head())\ndisplay(test.head())\ndisplay(item_info.shape)\ndisplay(user_info.shape)","metadata":{"id":"JhQPZR_yXqHk","outputId":"cce3f497-89a7-415f-da30-173f195aa5ae","trusted":true,"execution":{"iopub.status.busy":"2025-07-10T11:11:22.837330Z","iopub.execute_input":"2025-07-10T11:11:22.838017Z","iopub.status.idle":"2025-07-10T11:11:23.135325Z","shell.execute_reply.started":"2025-07-10T11:11:22.837991Z","shell.execute_reply":"2025-07-10T11:11:23.134597Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"   user_id                                     slate_of_items  \\\n0     4794  [2087, 1073, 951, 1230, 1256, 3362, 1276, 1304...   \n1     4794  [1923, 3763, 3702, 3693, 2804, 1196, 541, 1197...   \n2     4794  [1299, 1394, 3342, 1231, 3683, 1199, 1270, 316...   \n3     4794  [1259, 3072, 3505, 1291, 1674, 2348, 3753, 969...   \n4     4794  [1200, 1201, 1240, 457, 589, 377, 2951, 2985, ...   \n\n                         user_mid  \\\n0  [1, 1, 1, 1, 1, 1, 1, 1, 0, 1]   \n1  [1, 1, 1, 0, 1, 1, 1, 1, 1, 1]   \n2  [1, 1, 1, 1, 1, 0, 1, 0, 1, 1]   \n3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n4  [1, 1, 1, 1, 0, 1, 1, 0, 1, 1]   \n\n                                    user_mid_history  sequence_id  \n0  [593, 1198, 2683, 2355, 1911, 3543, 3615, 2757...            4  \n1  [593, 1198, 2683, 2355, 1911, 3543, 3615, 2757...            5  \n2  [593, 1198, 2683, 2355, 1911, 3543, 3615, 2757...            6  \n3  [593, 1198, 2683, 2355, 1911, 3543, 3615, 2757...            7  \n4  [593, 1198, 2683, 2355, 1911, 3543, 3615, 2757...            8  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>slate_of_items</th>\n      <th>user_mid</th>\n      <th>user_mid_history</th>\n      <th>sequence_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4794</td>\n      <td>[2087, 1073, 951, 1230, 1256, 3362, 1276, 1304...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 0, 1]</td>\n      <td>[593, 1198, 2683, 2355, 1911, 3543, 3615, 2757...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4794</td>\n      <td>[1923, 3763, 3702, 3693, 2804, 1196, 541, 1197...</td>\n      <td>[1, 1, 1, 0, 1, 1, 1, 1, 1, 1]</td>\n      <td>[593, 1198, 2683, 2355, 1911, 3543, 3615, 2757...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4794</td>\n      <td>[1299, 1394, 3342, 1231, 3683, 1199, 1270, 316...</td>\n      <td>[1, 1, 1, 1, 1, 0, 1, 0, 1, 1]</td>\n      <td>[593, 1198, 2683, 2355, 1911, 3543, 3615, 2757...</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4794</td>\n      <td>[1259, 3072, 3505, 1291, 1674, 2348, 3753, 969...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n      <td>[593, 1198, 2683, 2355, 1911, 3543, 3615, 2757...</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4794</td>\n      <td>[1200, 1201, 1240, 457, 589, 377, 2951, 2985, ...</td>\n      <td>[1, 1, 1, 1, 0, 1, 1, 0, 1, 1]</td>\n      <td>[593, 1198, 2683, 2355, 1911, 3543, 3615, 2757...</td>\n      <td>8</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"   user_id                                     slate_of_items  \\\n0     4794  [2087, 1073, 951, 1230, 1256, 3362, 1276, 1304...   \n1     4794  [1923, 3763, 3702, 3693, 2804, 1196, 541, 1197...   \n2     4794  [1299, 1394, 3342, 1231, 3683, 1199, 1270, 316...   \n3     4794  [1259, 3072, 3505, 1291, 1674, 2348, 3753, 969...   \n4     4794  [1200, 1201, 1240, 457, 589, 377, 2951, 2985, ...   \n\n                         user_mid  \\\n0  [1, 1, 1, 1, 1, 1, 1, 1, 0, 1]   \n1  [1, 1, 1, 0, 1, 1, 1, 1, 1, 1]   \n2  [1, 1, 1, 1, 1, 0, 1, 0, 1, 1]   \n3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n4  [1, 1, 1, 1, 0, 1, 1, 0, 1, 1]   \n\n                                    user_mid_history  sequence_id  \n0  [593, 1198, 2683, 2355, 1911, 3543, 3615, 2757...            4  \n1  [593, 1198, 2683, 2355, 1911, 3543, 3615, 2757...            5  \n2  [593, 1198, 2683, 2355, 1911, 3543, 3615, 2757...            6  \n3  [593, 1198, 2683, 2355, 1911, 3543, 3615, 2757...            7  \n4  [593, 1198, 2683, 2355, 1911, 3543, 3615, 2757...            8  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>slate_of_items</th>\n      <th>user_mid</th>\n      <th>user_mid_history</th>\n      <th>sequence_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4794</td>\n      <td>[2087, 1073, 951, 1230, 1256, 3362, 1276, 1304...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 0, 1]</td>\n      <td>[593, 1198, 2683, 2355, 1911, 3543, 3615, 2757...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4794</td>\n      <td>[1923, 3763, 3702, 3693, 2804, 1196, 541, 1197...</td>\n      <td>[1, 1, 1, 0, 1, 1, 1, 1, 1, 1]</td>\n      <td>[593, 1198, 2683, 2355, 1911, 3543, 3615, 2757...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4794</td>\n      <td>[1299, 1394, 3342, 1231, 3683, 1199, 1270, 316...</td>\n      <td>[1, 1, 1, 1, 1, 0, 1, 0, 1, 1]</td>\n      <td>[593, 1198, 2683, 2355, 1911, 3543, 3615, 2757...</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4794</td>\n      <td>[1259, 3072, 3505, 1291, 1674, 2348, 3753, 969...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n      <td>[593, 1198, 2683, 2355, 1911, 3543, 3615, 2757...</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4794</td>\n      <td>[1200, 1201, 1240, 457, 589, 377, 2951, 2985, ...</td>\n      <td>[1, 1, 1, 1, 0, 1, 1, 0, 1, 1]</td>\n      <td>[593, 1198, 2683, 2355, 1911, 3543, 3615, 2757...</td>\n      <td>8</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(3953, 18)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(6041, 30)"},"metadata":{}}],"execution_count":44},{"cell_type":"code","source":"params['train'] = train\nparams['test'] = test\nconfig = params.copy()\nconfig.pop(\"train\", None)\nconfig.pop(\"val\", None)\nconfig.pop(\"item_meta\", None)\nconfig.pop(\"user_meta\", None)\n\nfor seed in [11]:\n    params['seed'] = seed\n    set_random_seed(params['seed'])\n    params['save_path'] = os.path.join(path_to_output, f\"agent/ml1m_model_seed{params['seed']}\")\n    run = wandb.init(\n    # Set the wandb entity where your project will be logged (generally your team name).\n    entity=\"23020082-uet\",\n    # Set the wandb project where this run will be logged.\n    project=\"HAC\",\n    # Track hyperparameters and run metadata.\n    config=config\n    )\n    os.makedirs(os.path.dirname(params['save_path']), exist_ok=True)\n    \n    env = ML1MEnvironment(params)\n    \n    policy = SASRec(env, params)\n    policy.to(device)\n    \n    \n    critic = GeneralCritic(policy, params)\n    critic.to(device)\n    \n    facade = OneStageFacade_HyperAction(env, policy, critic, params)\n    agent = HAC(facade, params)\n    \n    agent.test()\n    run.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T11:11:23.136136Z","iopub.execute_input":"2025-07-10T11:11:23.136354Z","iopub.status.idle":"2025-07-10T11:28:07.427674Z","shell.execute_reply.started":"2025-07-10T11:11:23.136339Z","shell.execute_reply":"2025-07-10T11:28:07.426795Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Serializing object of type DataFrame that is 16093252 bytes\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250710_111123-p5iy728h</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/23020082-uet/HAC/runs/p5iy728h' target=\"_blank\">genial-snow-77</a></strong> to <a href='https://wandb.ai/23020082-uet/HAC' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/23020082-uet/HAC' target=\"_blank\">https://wandb.ai/23020082-uet/HAC</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/23020082-uet/HAC/runs/p5iy728h' target=\"_blank\">https://wandb.ai/23020082-uet/HAC/runs/p5iy728h</a>"},"metadata":{}},{"name":"stdout","text":"Load item meta data\n{'length': 19477, 'n_item': 3952, 'item_vec_size': 18, 'user_portrait_len': 30, 'max_seq_len': 50}\nStart testing\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/2270135661.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(params['model_path'] + \".checkpoint\", map_location=device)\n/tmp/ipykernel_31/928354164.py:137: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  self.critic.load_state_dict(torch.load(self.save_path + \"_critic\", map_location=self.device))\n/tmp/ipykernel_31/928354164.py:138: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  self.critic_optimizer.load_state_dict(torch.load(self.save_path + \"_critic_optimizer\", map_location=self.device))\n/tmp/ipykernel_31/928354164.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  self.actor.load_state_dict(torch.load(self.save_path + \"_actor\", map_location=self.device))\n/tmp/ipykernel_31/928354164.py:142: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  self.actor_optimizer.load_state_dict(torch.load(self.save_path + \"_actor_optimizer\", map_location=self.device))\n100%|██████████| 50000/50000 [16:33<00:00, 50.34it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_n_step</td><td>▃▄▃▅▅▃▃▆█▃▂▃▅▄▅▃▄▅▄▃▅▄▅▃▅▆▁▇▄▆▄▅▄▄▆▄▂▃▅▃</td></tr><tr><td>average_total_reward</td><td>█▆▃▄▅█▃▅▁▆▃▄▅▇▃▃▄▂▁▅▇▃▅▃▅▄▃▇▅▁▇▃▃▄▃▃▃▆▅▅</td></tr><tr><td>buffer_size</td><td>▁▂▂▆▇███████████████████████████████████</td></tr><tr><td>max_n_step</td><td>▅▅▄▄▄▅▅▂▅▄▄▅▄▄▄▂▅▇▅█▄▅█▅▄▇▅▅▄▇▄▄█▂▄▁▅▄▂▂</td></tr><tr><td>max_total_reward</td><td>▄▄▄█▅▄▅▅▄▄▅▁▄▅▄▇▅▄▄▂▄▅▄▄▅▄▅▂▄▂▅▅▅▂▅▄▅▇▇▄</td></tr><tr><td>min_n_step</td><td>▃▅▅▆▆▃▅▅▆▁▁▅▅▆▃▅▆█▅▆▆▅▆▆▅▅▅▃▅▆█▅▆▃▃▆▅▃▅▅</td></tr><tr><td>min_total_reward</td><td>▁▄▇▅▁▄▁▅▄▅▄▇█▄█▇▁▇▄▇█▅▇▁▄▅▇█▅█▅█▄▄█▄▁▅▄▄</td></tr><tr><td>reward_variance</td><td>▁▄▁▅▃▁▃▂▇▃▄▂▃▄▇▃▃▅▅▁▂▃▂▇▂▅█▄▄▄▁▄▅▅▁▂▆▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_n_step</td><td>8.1</td></tr><tr><td>average_total_reward</td><td>6.901</td></tr><tr><td>buffer_size</td><td>100000</td></tr><tr><td>max_n_step</td><td>11</td></tr><tr><td>max_total_reward</td><td>9.9</td></tr><tr><td>min_n_step</td><td>7</td></tr><tr><td>min_total_reward</td><td>5.57</td></tr><tr><td>reward_variance</td><td>2.34299</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">genial-snow-77</strong> at: <a href='https://wandb.ai/23020082-uet/HAC/runs/p5iy728h' target=\"_blank\">https://wandb.ai/23020082-uet/HAC/runs/p5iy728h</a><br> View project at: <a href='https://wandb.ai/23020082-uet/HAC' target=\"_blank\">https://wandb.ai/23020082-uet/HAC</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250710_111123-p5iy728h/logs</code>"},"metadata":{}}],"execution_count":45},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}